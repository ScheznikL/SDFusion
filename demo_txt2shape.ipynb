{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a46PIolBovog"
      },
      "source": [
        "# SDFusion: Text-guided Generation (txt2shape)\n",
        "\n",
        "### TODO: add sample results or teaser images"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/ScheznikL/SDFusion.git\n",
        "%cd SDFusion\n",
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YML3ilOsqZZO",
        "outputId": "1b97ee4c-1568-4cb4-fd75-602f358d1743"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'SDFusion'...\n",
            "remote: Enumerating objects: 2027, done.\u001b[K\n",
            "remote: Counting objects: 100% (85/85), done.\u001b[K\n",
            "remote: Compressing objects: 100% (49/49), done.\u001b[K\n",
            "remote: Total 2027 (delta 63), reused 36 (delta 36), pack-reused 1942 (from 2)\u001b[K\n",
            "Receiving objects: 100% (2027/2027), 36.09 MiB | 10.31 MiB/s, done.\n",
            "Resolving deltas: 100% (847/847), done.\n",
            "Updating files: 100% (2464/2464), done.\n",
            "/content/SDFusion\n",
            "configs\t\t      demo_mm2shape.ipynb\t    LICENSE\tsetup_env.sh\n",
            "dataset_info_files    demo_txt2shape.ipynb\t    models\ttrain.py\n",
            "datasets\t      demo_uncond_shape_comp.ipynb  options\tutils\n",
            "demo_data\t      external\t\t\t    preprocess\n",
            "demo_img2shape.ipynb  launchers\t\t\t    README.md\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "ngTOHKKoovoj"
      },
      "outputs": [],
      "source": [
        "# first set up which gpu to use\n",
        "import os\n",
        "gpu_ids = 0\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = f\"{gpu_ids}\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#exit()\n",
        "#How to run a Python script in a '.py' file from a Google Colab notebook?\n",
        "from models.base_model import create_model"
      ],
      "metadata": {
        "id": "RH49NE8R1det"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install h5py joblib trimesh scipy PyMCubes\n",
        "#pip install --upgrade PyMCubes"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Whf2UAAO4Ic7",
        "outputId": "a8c79433-4fbb-44f6-9067-04e416f318c8"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: h5py in /usr/local/lib/python3.11/dist-packages (3.13.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (1.4.2)\n",
            "Collecting trimesh\n",
            "  Downloading trimesh-4.6.8-py3-none-any.whl.metadata (18 kB)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (1.15.2)\n",
            "Collecting PyMCubes\n",
            "  Downloading PyMCubes-0.1.6-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (868 bytes)\n",
            "Requirement already satisfied: numpy>=1.19.3 in /usr/local/lib/python3.11/dist-packages (from h5py) (2.0.2)\n",
            "Downloading trimesh-4.6.8-py3-none-any.whl (709 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m709.3/709.3 kB\u001b[0m \u001b[31m19.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading PyMCubes-0.1.6-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (336 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m336.8/336.8 kB\u001b[0m \u001b[31m31.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: trimesh, PyMCubes\n",
            "Successfully installed PyMCubes-0.1.6 trimesh-4.6.8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title NEW way\n",
        "import sys\n",
        "import torch\n",
        "pyt_version_str=torch.__version__.split(\"+\")[0].replace(\".\", \"\")\n",
        "version_str=\"\".join([\n",
        "    f\"py3{sys.version_info.minor}_cu\",\n",
        "    torch.version.cuda.replace(\".\",\"\"),\n",
        "    f\"_pyt{pyt_version_str}\"\n",
        "])\n",
        "!pip install iopath\n",
        "!pip install --no-index --no-cache-dir pytorch3d -f https://dl.fbaipublicfiles.com/pytorch3d/packaging/wheels/{version_str}/download.html"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cLPR-DlQTeCv",
        "outputId": "3c661b71-d34c-4e65-d579-f4ecffb98f5c"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting iopath\n",
            "  Downloading iopath-0.1.10.tar.gz (42 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/42.2 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.2/42.2 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from iopath) (4.67.1)\n",
            "Requirement already satisfied: typing_extensions in /usr/local/lib/python3.11/dist-packages (from iopath) (4.13.2)\n",
            "Collecting portalocker (from iopath)\n",
            "  Downloading portalocker-3.1.1-py3-none-any.whl.metadata (8.6 kB)\n",
            "Downloading portalocker-3.1.1-py3-none-any.whl (19 kB)\n",
            "Building wheels for collected packages: iopath\n",
            "  Building wheel for iopath (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for iopath: filename=iopath-0.1.10-py3-none-any.whl size=31527 sha256=f1de7f163b1c7a260dba7a9f193b3068b92dfee926b475f5141d92baef1509f4\n",
            "  Stored in directory: /root/.cache/pip/wheels/ba/5e/16/6117f8fe7e9c0c161a795e10d94645ebcf301ccbd01f66d8ec\n",
            "Successfully built iopath\n",
            "Installing collected packages: portalocker, iopath\n",
            "Successfully installed iopath-0.1.10 portalocker-3.1.1\n",
            "Looking in links: https://dl.fbaipublicfiles.com/pytorch3d/packaging/wheels/py311_cu124_pyt260/download.html\n",
            "\u001b[31mERROR: Could not find a version that satisfies the requirement pytorch3d (from versions: none)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for pytorch3d\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import sys\n",
        "import torch\n",
        "need_pytorch3d=False\n",
        "try:\n",
        "    import pytorch3d\n",
        "except ModuleNotFoundError:\n",
        "    need_pytorch3d=True\n",
        "if need_pytorch3d:\n",
        "    if torch.__version__.startswith(\"2.1.\") and sys.platform.startswith(\"linux\"):\n",
        "        # We try to install PyTorch3D via a released wheel.\n",
        "        pyt_version_str=torch.__version__.split(\"+\")[0].replace(\".\", \"\")\n",
        "        version_str=\"\".join([\n",
        "            f\"py3{sys.version_info.minor}_cu\",\n",
        "            torch.version.cuda.replace(\".\",\"\"),\n",
        "            f\"_pyt{pyt_version_str}\"\n",
        "        ])\n",
        "        !pip install fvcore iopath\n",
        "        !pip install --no-index --no-cache-dir pytorch3d -f https://dl.fbaipublicfiles.com/pytorch3d/packaging/wheels/{version_str}/download.html\n",
        "    else:\n",
        "        # We try to install PyTorch3D from source.\n",
        "        !pip install 'git+https://github.com/facebookresearch/pytorch3d.git@stable'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RiMDwTPe5ajn",
        "outputId": "8e5c1cd9-ab94-4dc8-98d1-538c4380b37c"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/facebookresearch/pytorch3d.git@stable\n",
            "  Cloning https://github.com/facebookresearch/pytorch3d.git (to revision stable) to /tmp/pip-req-build-85f_25_2\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/facebookresearch/pytorch3d.git /tmp/pip-req-build-85f_25_2\n",
            "  Running command git checkout -q 75ebeeaea0908c5527e7b1e305fbc7681382db47\n",
            "  Resolved https://github.com/facebookresearch/pytorch3d.git to commit 75ebeeaea0908c5527e7b1e305fbc7681382db47\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: iopath in /usr/local/lib/python3.11/dist-packages (from pytorch3d==0.7.8) (0.1.10)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from iopath->pytorch3d==0.7.8) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from iopath->pytorch3d==0.7.8) (4.13.2)\n",
            "Requirement already satisfied: portalocker in /usr/local/lib/python3.11/dist-packages (from iopath->pytorch3d==0.7.8) (3.1.1)\n",
            "Building wheels for collected packages: pytorch3d\n",
            "  Building wheel for pytorch3d (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pytorch3d: filename=pytorch3d-0.7.8-cp311-cp311-linux_x86_64.whl size=60242813 sha256=630ac105cbf28b5a8084671508e011b789284d945caa052fbb08ad179e134558\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-9r4j78q1/wheels/08/90/1b/df18c3e3634f86278e793b87f37ea4c58d0c36731196122518\n",
            "Successfully built pytorch3d\n",
            "Installing collected packages: pytorch3d\n",
            "Successfully installed pytorch3d-0.7.8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "H4rc3dvRovok"
      },
      "outputs": [],
      "source": [
        "# import libraries\n",
        "import numpy as np\n",
        "from IPython.display import Image as ipy_image\n",
        "from IPython.display import display\n",
        "from termcolor import colored, cprint\n",
        "\n",
        "import torch\n",
        "import torch.backends.cudnn as cudnn\n",
        "cudnn.benchmark = True\n",
        "import torchvision.utils as vutils\n",
        "\n",
        "from models.base_model import create_model\n",
        "from utils.util_3d import render_sdf, render_mesh, sdf_to_mesh, save_mesh_as_gif\n",
        "\n",
        "%load_ext autoreload\n",
        "%autoreload 2"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# New section"
      ],
      "metadata": {
        "id": "hiAWSzGrym7E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install omegaconf"
      ],
      "metadata": {
        "id": "nnCrwizmJOFc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 411
        },
        "outputId": "dad14b99-9fde-4974-921f-cbc88fe66bbd"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting omegaconf\n",
            "  Downloading omegaconf-2.3.0-py3-none-any.whl.metadata (3.9 kB)\n",
            "Collecting antlr4-python3-runtime==4.9.* (from omegaconf)\n",
            "  Downloading antlr4-python3-runtime-4.9.3.tar.gz (117 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m117.0/117.0 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: PyYAML>=5.1.0 in /usr/local/lib/python3.11/dist-packages (from omegaconf) (6.0.2)\n",
            "Downloading omegaconf-2.3.0-py3-none-any.whl (79 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.5/79.5 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: antlr4-python3-runtime\n",
            "  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.9.3-py3-none-any.whl size=144554 sha256=d074608249c48059779e88013f8f070a0e8edc87dd2b1bdda71ea5c8adb85058\n",
            "  Stored in directory: /root/.cache/pip/wheels/1a/97/32/461f837398029ad76911109f07047fde1d7b661a147c7c56d1\n",
            "Successfully built antlr4-python3-runtime\n",
            "Installing collected packages: antlr4-python3-runtime, omegaconf\n",
            "Successfully installed antlr4-python3-runtime-4.9.3 omegaconf-2.3.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "pydevd_plugins"
                ]
              },
              "id": "f5703a63b1d249589e7640b879bfe9a5"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "Q5F5K37covom",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4d98986f-3a7b-48f9-8fad-fb5bbc77abfd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[*] SDFusionText2ShapeOption initialized.\n"
          ]
        }
      ],
      "source": [
        "# @title Seed 2024\n",
        "# options for the model. please check `utils/demo_util.py` for more details\n",
        "from utils.demo_util import SDFusionText2ShapeOpt\n",
        "\n",
        "seed = 2025\n",
        "opt = SDFusionText2ShapeOpt(gpu_ids=gpu_ids, seed=seed)\n",
        "device = opt.device"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir saved_ckpt"
      ],
      "metadata": {
        "id": "FO-FyCz4J6mO"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# SDFusion: text-guided shape generation (txt2shape)\n",
        "!wget https://uofi.box.com/shared/static/vyqs6aex3rwbgxweyl3qh21c8p6vu33f.pth -O saved_ckpt/sdfusion-txt2shape.pth\n",
        "# VQVAE's checkpoint\n",
        "!wget https://uofi.box.com/shared/static/zdb9pm9wmxaupzclc7m8gzluj20ja0b6.pth -O saved_ckpt/vqvae-snet-all.pth"
      ],
      "metadata": {
        "id": "RB_TIaXOJw7S",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dc5122b1-1321-4296-d0d4-9d4fcb278d57"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-05-11 17:07:30--  https://uofi.box.com/shared/static/vyqs6aex3rwbgxweyl3qh21c8p6vu33f.pth\n",
            "Resolving uofi.box.com (uofi.box.com)... 74.112.186.157, 2620:117:bff0:12d::\n",
            "Connecting to uofi.box.com (uofi.box.com)|74.112.186.157|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: /public/static/vyqs6aex3rwbgxweyl3qh21c8p6vu33f.pth [following]\n",
            "--2025-05-11 17:07:30--  https://uofi.box.com/public/static/vyqs6aex3rwbgxweyl3qh21c8p6vu33f.pth\n",
            "Reusing existing connection to uofi.box.com:443.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: https://uofi.app.box.com/public/static/vyqs6aex3rwbgxweyl3qh21c8p6vu33f.pth [following]\n",
            "--2025-05-11 17:07:30--  https://uofi.app.box.com/public/static/vyqs6aex3rwbgxweyl3qh21c8p6vu33f.pth\n",
            "Resolving uofi.app.box.com (uofi.app.box.com)... 74.112.186.157, 2620:117:bff0:12d::\n",
            "Connecting to uofi.app.box.com (uofi.app.box.com)|74.112.186.157|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://public.boxcloud.com/d/1/b1!rZJguAGvI-WHfk_g1LXQaTKS3Nzkkhodz3GXhRJmDTv0Y814CLTJwwAMW48U3Y5TT2wedoCE6I4lsjduVXuP06Zq04qBHGp3vCMQMwDzfSuAr4gx2VMMMcljBz1ceYmst0weC3Jun_xPXdnH8tHrfpcjnFqxO8Km16yOT6gbZ_E8xj3jY0LgW2ac_A80jhQfpp2AothVOa7Y6Y7WHVB2fTTElk8BYJyXnYpNm_Q_5z-_hbrXAbE4mzdk0HFNxrOMBvy_m_ZxAX4NjqLLf6q1Q9qFa2xx8pnEP6OHUAP4p_QfmVEK34MckDEZY6hIVYWxVFr7O5ysIoG0AE0o60lgRVRRiTDtbULmqnpvSSuqe7rxMilujnWrPnzbBtZz64MDd5lmHq5yvLcADGfn6g9VVUqHVNxyFhNXTtnboKe8eciTiluwYMMLhGYucEDPNiPCgVl2kt-0vTTsB_1M-wIxnkoG1fTrdDSnEshQJGPRsuGEqWsbxP7wunrmTfpmMwysUeC5ozmucDxB2KzIyBTZHfjJPf-KL_2eNasmwnnL5c6uMglkOysLyu-Fz4xCK-yUAbg8OefldzXpk018hzJQ56Qvi1aFkuUYHQTHsB3TUdJMNTpVVH1C0mblO1IvqKZs7kuImBA09umdZqS0AJr7wocZWfJ0_nNALZ1vuh-GiSpKTMY-XwlwUM9_A0toIukojeaqrjtc8VIyxV98PNN2N-yWRM9eQdtuti-jAl_wZVysDkExqCuLBP6FCtbBIZeattr9GfC5dZCcMTztF0RU_fUlQ3-48qWZAI2s-5zdMs-P1i4pvaYitYtWtMuwx9mZqRJ3vPVniTXEGNCCGIYfu9fFlOP8EU7wxHR6FfNXqXswbV_maEnGlRyX9RFfDGiboByiAb7hZuML1xE7DPp_6q8jZgP-78BTZcnZb-NLzSP7ffVilA9xdK3g4CrIXEp-1Pko-Ie-7NTDRmAi3VEZDd9uYokw9YWg5qTd2MqpXuWWxccF8wcfqNRCXlbVVDcmzb9PdXj6sJ-XtUDW9yAQAqNHbx3ZzEnkHqX-Dspx41vvCoO-HA9-M6P_IK8NUyCisnV3lIzVP0wZsHyvzKVx2mb-L7TjOuWw8uFA8F4f6AMHEIrtWq-JU6NF5JtApGoEy8uWArDlozsh7c3wDlVNgUuVVhtXkObLpuw97jtKsYWzRlozBFJ_Q-kLS0b3AIfZuI3g7oGa8q18zuAokW92PaSMwF80va8LenE2LrEJK6SSM-Sm6S-7gKckF-xAzjxeZKGZRXpNe-X4XEB1U51G8dV5vbk8XfRDbpxWV7vc9RmquLQ0Kn56zXklZAUpi3lsw5eXVNraZMo_iwpiRlEwAfeRsGXE8lk8FmaimZElT2HNqw3QjM5Hguu7xuisMBBDKn5ePrZKDw5-uWF8Ac44UZbfxCzerQ3ry5va3hKS8wO0cgGcG00lQzIEdwFtMokbtZ4Yg2Ea0NYm6GqpJmL9nuxbIZYXpig48XWkEWOnHpRiADlzd2ZkHD_9QAh8NMi_Sw../download [following]\n",
            "--2025-05-11 17:07:30--  https://public.boxcloud.com/d/1/b1!rZJguAGvI-WHfk_g1LXQaTKS3Nzkkhodz3GXhRJmDTv0Y814CLTJwwAMW48U3Y5TT2wedoCE6I4lsjduVXuP06Zq04qBHGp3vCMQMwDzfSuAr4gx2VMMMcljBz1ceYmst0weC3Jun_xPXdnH8tHrfpcjnFqxO8Km16yOT6gbZ_E8xj3jY0LgW2ac_A80jhQfpp2AothVOa7Y6Y7WHVB2fTTElk8BYJyXnYpNm_Q_5z-_hbrXAbE4mzdk0HFNxrOMBvy_m_ZxAX4NjqLLf6q1Q9qFa2xx8pnEP6OHUAP4p_QfmVEK34MckDEZY6hIVYWxVFr7O5ysIoG0AE0o60lgRVRRiTDtbULmqnpvSSuqe7rxMilujnWrPnzbBtZz64MDd5lmHq5yvLcADGfn6g9VVUqHVNxyFhNXTtnboKe8eciTiluwYMMLhGYucEDPNiPCgVl2kt-0vTTsB_1M-wIxnkoG1fTrdDSnEshQJGPRsuGEqWsbxP7wunrmTfpmMwysUeC5ozmucDxB2KzIyBTZHfjJPf-KL_2eNasmwnnL5c6uMglkOysLyu-Fz4xCK-yUAbg8OefldzXpk018hzJQ56Qvi1aFkuUYHQTHsB3TUdJMNTpVVH1C0mblO1IvqKZs7kuImBA09umdZqS0AJr7wocZWfJ0_nNALZ1vuh-GiSpKTMY-XwlwUM9_A0toIukojeaqrjtc8VIyxV98PNN2N-yWRM9eQdtuti-jAl_wZVysDkExqCuLBP6FCtbBIZeattr9GfC5dZCcMTztF0RU_fUlQ3-48qWZAI2s-5zdMs-P1i4pvaYitYtWtMuwx9mZqRJ3vPVniTXEGNCCGIYfu9fFlOP8EU7wxHR6FfNXqXswbV_maEnGlRyX9RFfDGiboByiAb7hZuML1xE7DPp_6q8jZgP-78BTZcnZb-NLzSP7ffVilA9xdK3g4CrIXEp-1Pko-Ie-7NTDRmAi3VEZDd9uYokw9YWg5qTd2MqpXuWWxccF8wcfqNRCXlbVVDcmzb9PdXj6sJ-XtUDW9yAQAqNHbx3ZzEnkHqX-Dspx41vvCoO-HA9-M6P_IK8NUyCisnV3lIzVP0wZsHyvzKVx2mb-L7TjOuWw8uFA8F4f6AMHEIrtWq-JU6NF5JtApGoEy8uWArDlozsh7c3wDlVNgUuVVhtXkObLpuw97jtKsYWzRlozBFJ_Q-kLS0b3AIfZuI3g7oGa8q18zuAokW92PaSMwF80va8LenE2LrEJK6SSM-Sm6S-7gKckF-xAzjxeZKGZRXpNe-X4XEB1U51G8dV5vbk8XfRDbpxWV7vc9RmquLQ0Kn56zXklZAUpi3lsw5eXVNraZMo_iwpiRlEwAfeRsGXE8lk8FmaimZElT2HNqw3QjM5Hguu7xuisMBBDKn5ePrZKDw5-uWF8Ac44UZbfxCzerQ3ry5va3hKS8wO0cgGcG00lQzIEdwFtMokbtZ4Yg2Ea0NYm6GqpJmL9nuxbIZYXpig48XWkEWOnHpRiADlzd2ZkHD_9QAh8NMi_Sw../download\n",
            "Resolving public.boxcloud.com (public.boxcloud.com)... 74.112.186.165, 2620:117:bff0:69::\n",
            "Connecting to public.boxcloud.com (public.boxcloud.com)|74.112.186.165|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 4087970878 (3.8G) [application/octet-stream]\n",
            "Saving to: ‘saved_ckpt/sdfusion-txt2shape.pth’\n",
            "\n",
            "saved_ckpt/sdfusion 100%[===================>]   3.81G  89.6MB/s    in 53s     \n",
            "\n",
            "2025-05-11 17:08:24 (73.3 MB/s) - ‘saved_ckpt/sdfusion-txt2shape.pth’ saved [4087970878/4087970878]\n",
            "\n",
            "--2025-05-11 17:08:24--  https://uofi.box.com/shared/static/zdb9pm9wmxaupzclc7m8gzluj20ja0b6.pth\n",
            "Resolving uofi.box.com (uofi.box.com)... 74.112.186.157, 2620:117:bff0:12d::\n",
            "Connecting to uofi.box.com (uofi.box.com)|74.112.186.157|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: /public/static/zdb9pm9wmxaupzclc7m8gzluj20ja0b6.pth [following]\n",
            "--2025-05-11 17:08:24--  https://uofi.box.com/public/static/zdb9pm9wmxaupzclc7m8gzluj20ja0b6.pth\n",
            "Reusing existing connection to uofi.box.com:443.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: https://uofi.app.box.com/public/static/zdb9pm9wmxaupzclc7m8gzluj20ja0b6.pth [following]\n",
            "--2025-05-11 17:08:24--  https://uofi.app.box.com/public/static/zdb9pm9wmxaupzclc7m8gzluj20ja0b6.pth\n",
            "Resolving uofi.app.box.com (uofi.app.box.com)... 74.112.186.157, 2620:117:bff0:12d::\n",
            "Connecting to uofi.app.box.com (uofi.app.box.com)|74.112.186.157|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://public.boxcloud.com/d/1/b1!MDvZ14yJ6shkA3gnZWkmrEMIlw_D4yx0jtfAbuyPlaAilRmwQiegH1EREZT7M4Ho4AQiTLUN31hqrYaS8S8nQEZUTWx9iKFzjXHHUsoVSiriZv5gbtvfd7JGFNWXs91StRbR1Ld89oO0gBHxZyLwm4_nTdOaY_x-2X52atNYP62In-6IlLSkok-NJnY_nTFYWJ-dLuehYDySH2T0AsjwpX3d_UU77StyIgLgb3XKrK5leP-SLvJipAEEMy30um3PMcQu7lpBFAhDUftgRe3Jy04GQR_ApjA_Lmx-nVYkqLp3zu8AZhSyFuB4d_5nZy6cRYBwhoZK4l1sg3_nL7q9mojL3qHNiRVUwsUmTxUfoSFJKwJr5dEJX3qfuBg-mg1AtQF86mtVdKVCa8d0HihOXNpgsogaY48oRVYxwhm83qQZ8uAIhG25T5pvkidMCovsIVOYcsvcCmK6N7zSuhaH2PZmTwBki7bqeOgv3SAHZkpvq5AM-SoFh3UkFGgY5SaMy1oSzu-RiXZNh5bvkQE1fLYGlG4uDA16tois8gNZRRv7alWmQBVWy_oKez8vhYUcmj-4_jP2dS2sirIJ9Z1UKx0nXK-Mz9aNcuTupV1eXGHFSibrMBbG5Y5xU7yqqIMw_S49gmnWw1ZKNrOhyMweRxdJjso4XH7VR3q76TcJfZsevROPHsqJuQS6Fd2hAmDo-0X5Wq6BnkGAWR8FjMZ-rWLRMF_I3JokiCpDGzOf0MtKv6yIbZQZDBBr5hJywwBKlIkbAd8I9zYZmv5bl9WU2wn_StbdVcXBVwFWCvVc618VAEuumiX6uFgRJVttTxhZhzeobFRGiDsdvdxD62qM77Dw2AduGr_GplgKTOsF7uokxWp_ecx4m5rjlq_IX-4h8anvplJZgy8w4fLob6CW7F-eSrxSN6fDLBJl895mlqSUWRe8ba8qYFFuSaAJmFHoVIANvJmahnp-NAxHSG_QQPeayM-yTUtEOJqZyGWgeomgxupMWW0ddUwYl4Z7BasNEjlMSAFzgBDEf1vzj525GEnO4WYVB0VmxEY6YkUxk1YNlcgr4zWz2y8zTcggw5oF5DP6qOOg-UmGS3_EYr_-MRYFi7P9xtNLXWLJ8Ng1l1YnzTwztvjAANtuo252dBF4pWNK-Brm2KohzvLXMbmIn_22xiOg4Sr5s43i-pUXcSTRtxcjP4UwyRbeSQaDqNMqZpaDXTZinO2V0GquvD9QZb0vmGwck79xcvc1xB7-_YgwCrntqxIHkpwISjQ9kYL9MlNxcR_QRNZm76P898_3sxHsVqDcp5XpVZjfTmN2IQLg8XatI5SFEjtDmrQT6w898ibN30LQVMj-o_f0elfVCU72npSLUTEZvPxk4UBCNGDNIfC72dqgD5dU5xqDY0qlPaAe6_KWltcQWEenR8qjIy8Sdz159PGYoAXAW3WNdCqLBh698ctRmh-545xEMkIjPJmddwQ81FnjN1BY2syf3zY9VbpsN4hGp7XmGWN-kJtPNYWdkYcQv-NhnyIE-NWgtGPHPJ_Ai6IUyUtZ9QeJpMuET9DuFfjgaskXdQoH-T4./download [following]\n",
            "--2025-05-11 17:08:25--  https://public.boxcloud.com/d/1/b1!MDvZ14yJ6shkA3gnZWkmrEMIlw_D4yx0jtfAbuyPlaAilRmwQiegH1EREZT7M4Ho4AQiTLUN31hqrYaS8S8nQEZUTWx9iKFzjXHHUsoVSiriZv5gbtvfd7JGFNWXs91StRbR1Ld89oO0gBHxZyLwm4_nTdOaY_x-2X52atNYP62In-6IlLSkok-NJnY_nTFYWJ-dLuehYDySH2T0AsjwpX3d_UU77StyIgLgb3XKrK5leP-SLvJipAEEMy30um3PMcQu7lpBFAhDUftgRe3Jy04GQR_ApjA_Lmx-nVYkqLp3zu8AZhSyFuB4d_5nZy6cRYBwhoZK4l1sg3_nL7q9mojL3qHNiRVUwsUmTxUfoSFJKwJr5dEJX3qfuBg-mg1AtQF86mtVdKVCa8d0HihOXNpgsogaY48oRVYxwhm83qQZ8uAIhG25T5pvkidMCovsIVOYcsvcCmK6N7zSuhaH2PZmTwBki7bqeOgv3SAHZkpvq5AM-SoFh3UkFGgY5SaMy1oSzu-RiXZNh5bvkQE1fLYGlG4uDA16tois8gNZRRv7alWmQBVWy_oKez8vhYUcmj-4_jP2dS2sirIJ9Z1UKx0nXK-Mz9aNcuTupV1eXGHFSibrMBbG5Y5xU7yqqIMw_S49gmnWw1ZKNrOhyMweRxdJjso4XH7VR3q76TcJfZsevROPHsqJuQS6Fd2hAmDo-0X5Wq6BnkGAWR8FjMZ-rWLRMF_I3JokiCpDGzOf0MtKv6yIbZQZDBBr5hJywwBKlIkbAd8I9zYZmv5bl9WU2wn_StbdVcXBVwFWCvVc618VAEuumiX6uFgRJVttTxhZhzeobFRGiDsdvdxD62qM77Dw2AduGr_GplgKTOsF7uokxWp_ecx4m5rjlq_IX-4h8anvplJZgy8w4fLob6CW7F-eSrxSN6fDLBJl895mlqSUWRe8ba8qYFFuSaAJmFHoVIANvJmahnp-NAxHSG_QQPeayM-yTUtEOJqZyGWgeomgxupMWW0ddUwYl4Z7BasNEjlMSAFzgBDEf1vzj525GEnO4WYVB0VmxEY6YkUxk1YNlcgr4zWz2y8zTcggw5oF5DP6qOOg-UmGS3_EYr_-MRYFi7P9xtNLXWLJ8Ng1l1YnzTwztvjAANtuo252dBF4pWNK-Brm2KohzvLXMbmIn_22xiOg4Sr5s43i-pUXcSTRtxcjP4UwyRbeSQaDqNMqZpaDXTZinO2V0GquvD9QZb0vmGwck79xcvc1xB7-_YgwCrntqxIHkpwISjQ9kYL9MlNxcR_QRNZm76P898_3sxHsVqDcp5XpVZjfTmN2IQLg8XatI5SFEjtDmrQT6w898ibN30LQVMj-o_f0elfVCU72npSLUTEZvPxk4UBCNGDNIfC72dqgD5dU5xqDY0qlPaAe6_KWltcQWEenR8qjIy8Sdz159PGYoAXAW3WNdCqLBh698ctRmh-545xEMkIjPJmddwQ81FnjN1BY2syf3zY9VbpsN4hGp7XmGWN-kJtPNYWdkYcQv-NhnyIE-NWgtGPHPJ_Ai6IUyUtZ9QeJpMuET9DuFfjgaskXdQoH-T4./download\n",
            "Resolving public.boxcloud.com (public.boxcloud.com)... 74.112.186.165, 2620:117:bff0:69::\n",
            "Connecting to public.boxcloud.com (public.boxcloud.com)|74.112.186.165|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 105444855 (101M) [application/octet-stream]\n",
            "Saving to: ‘saved_ckpt/vqvae-snet-all.pth’\n",
            "\n",
            "saved_ckpt/vqvae-sn 100%[===================>] 100.56M  79.2MB/s    in 1.3s    \n",
            "\n",
            "2025-05-11 17:08:27 (79.2 MB/s) - ‘saved_ckpt/vqvae-snet-all.pth’ saved [105444855/105444855]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://uofi.box.com/shared/static/ueo01ctnlzobp2dmvd8iexy1bdsquuc1.pth -O saved_ckpt/sdfusion-snet-all.pth"
      ],
      "metadata": {
        "id": "YtEN5wRZN0Vj",
        "outputId": "8b9b8c71-f2dd-4bd9-94b8-079141b991ff",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-05-11 17:08:27--  https://uofi.box.com/shared/static/ueo01ctnlzobp2dmvd8iexy1bdsquuc1.pth\n",
            "Resolving uofi.box.com (uofi.box.com)... 74.112.186.157, 2620:117:bff0:12d::\n",
            "Connecting to uofi.box.com (uofi.box.com)|74.112.186.157|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: /public/static/ueo01ctnlzobp2dmvd8iexy1bdsquuc1.pth [following]\n",
            "--2025-05-11 17:08:27--  https://uofi.box.com/public/static/ueo01ctnlzobp2dmvd8iexy1bdsquuc1.pth\n",
            "Reusing existing connection to uofi.box.com:443.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: https://uofi.app.box.com/public/static/ueo01ctnlzobp2dmvd8iexy1bdsquuc1.pth [following]\n",
            "--2025-05-11 17:08:27--  https://uofi.app.box.com/public/static/ueo01ctnlzobp2dmvd8iexy1bdsquuc1.pth\n",
            "Resolving uofi.app.box.com (uofi.app.box.com)... 74.112.186.157, 2620:117:bff0:12d::\n",
            "Connecting to uofi.app.box.com (uofi.app.box.com)|74.112.186.157|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://public.boxcloud.com/d/1/b1!AAdYs3shu0QpZ1bm0xImCFpuxY59vnyM8unm0I4Z-bZavO8LqNLwMyuIsIQyrdPHHytce8FluHoQ8rxxXYqJ0BrBDZx9EdMO9Z54zcSLzioytHWqOIdZdGxfl3sERyU0hqv6_xSRhs3MA7EX8RTXdNBdOnuoLZVCssJvVYCbRMKYJJTVfji0h96GritR42eSpRsVmln9zWNi1tae05E0ggVnH1uYH7aJAzhI4ITYuVBKEPvFXgRtqzq20PYRaQUvxLFUzmDF8k__n86Iq1Eez-RTCe2IUf4Oq-dv6rKsfJbbuHa4GfLQf8w4EN_13V3dUTF9hAdeerXOJVYRcy-nkmy1ut9ftCjbWmzLr52N3Z66tbbrkyTkZWQzhWkGdsqX0yIOGoK-wAQC9WdRx1j7WzpMPeTz2MDlzXP_7Zz6hNNwiHsPl0sB-ay3WH6nSCTpG2kUuBO_OUDr7Ti1U9vLEK5xgNC9wYNls1e9CipR7HFkejeL9nq6YjyCpnBCTGb9rqHh0n_ouy0Zgki1TOEYO_zrt_4CGeJZ54-9TRE9-Xu5tZ5X0KP5DLs1FfK4LGx4f0NXuLWp2qLfcQG0YdMK9pPtXwBF7hdWibS7wLX20ECRKhYACXifinhF6KSvuvN7C-uMRnKnq42q9feRQ3t34TvnQTQe-o5BdTUZ11hFRedH3uPfUul9lgo3xzWg5P_L3vICNXJ8KLNx4A1rvbGp6wb3AYGcN5Ymy51qSywFZgsB9PkcYuK7GVJpylvf1Khq5dfAg-yBlBfyk65SFsV8468zDYjbnPHsVrnDcF0awr7XKuHoIRKTz3JI--_sESztLxkHSDda_WaowvVy_0kPOLW_RE-L7vV4gMEKeJOPes0acWYPtt_UKGWfs06U5AYh8t0ZxSWcPC31OK2G4bNy-458phNjk5Ts8R5MuMadhc_AiPe1gHLv0XgFKKcXVW8Yl0OOiU-8oBJBfXwyDewEybKMXA1z23V8iiXvDv7CmIHS9PjKExj6FMOYHKoi0kvyxEw3nYt0QmQevGY-NyJhirNDLrvdBDS3gmxtXlk1JuNHIWmdGktmfY8F3rev1gaRu9cjxW_QJ40_ZEu197BKRzZO0r59Fp-ecmX3AUoImOH-ohh9DLKmYAo3IdvmZyoANyLQLh1PMquwRlF9eTXEkrAsy5q_N_uhAtBflnh0giWwLChbD0NjsO7wEYTWXckLtzFxlwgC0hloMoDr_5BArIyAzY9mAbaEsSj3rpPkNyvVXjKELE808stAvQgun7M7hlY7uvinfrX7PweXq55ju2f1MtagZPgTx_CyfYl2Ftjtt60FXThSTjk3Z3zlP1XaXhum2II9x9Xux1NVdz4-LnoPA-W3wtrJsQ4nareE53SQGJN6FWxdRxnyEJUvKK2UHugpg5aMtCz-3UfzIWt0MRK91B5Fu29G3k0WNvlIHfaUgrI6R1CHp8-pyi2nkEI30u-9N4muWA_v4wRgB0AM2DZokZfoKdBrhZgKfFOHX73W93bg7ZN9HrbdV9YXqMnL/download [following]\n",
            "--2025-05-11 17:08:28--  https://public.boxcloud.com/d/1/b1!AAdYs3shu0QpZ1bm0xImCFpuxY59vnyM8unm0I4Z-bZavO8LqNLwMyuIsIQyrdPHHytce8FluHoQ8rxxXYqJ0BrBDZx9EdMO9Z54zcSLzioytHWqOIdZdGxfl3sERyU0hqv6_xSRhs3MA7EX8RTXdNBdOnuoLZVCssJvVYCbRMKYJJTVfji0h96GritR42eSpRsVmln9zWNi1tae05E0ggVnH1uYH7aJAzhI4ITYuVBKEPvFXgRtqzq20PYRaQUvxLFUzmDF8k__n86Iq1Eez-RTCe2IUf4Oq-dv6rKsfJbbuHa4GfLQf8w4EN_13V3dUTF9hAdeerXOJVYRcy-nkmy1ut9ftCjbWmzLr52N3Z66tbbrkyTkZWQzhWkGdsqX0yIOGoK-wAQC9WdRx1j7WzpMPeTz2MDlzXP_7Zz6hNNwiHsPl0sB-ay3WH6nSCTpG2kUuBO_OUDr7Ti1U9vLEK5xgNC9wYNls1e9CipR7HFkejeL9nq6YjyCpnBCTGb9rqHh0n_ouy0Zgki1TOEYO_zrt_4CGeJZ54-9TRE9-Xu5tZ5X0KP5DLs1FfK4LGx4f0NXuLWp2qLfcQG0YdMK9pPtXwBF7hdWibS7wLX20ECRKhYACXifinhF6KSvuvN7C-uMRnKnq42q9feRQ3t34TvnQTQe-o5BdTUZ11hFRedH3uPfUul9lgo3xzWg5P_L3vICNXJ8KLNx4A1rvbGp6wb3AYGcN5Ymy51qSywFZgsB9PkcYuK7GVJpylvf1Khq5dfAg-yBlBfyk65SFsV8468zDYjbnPHsVrnDcF0awr7XKuHoIRKTz3JI--_sESztLxkHSDda_WaowvVy_0kPOLW_RE-L7vV4gMEKeJOPes0acWYPtt_UKGWfs06U5AYh8t0ZxSWcPC31OK2G4bNy-458phNjk5Ts8R5MuMadhc_AiPe1gHLv0XgFKKcXVW8Yl0OOiU-8oBJBfXwyDewEybKMXA1z23V8iiXvDv7CmIHS9PjKExj6FMOYHKoi0kvyxEw3nYt0QmQevGY-NyJhirNDLrvdBDS3gmxtXlk1JuNHIWmdGktmfY8F3rev1gaRu9cjxW_QJ40_ZEu197BKRzZO0r59Fp-ecmX3AUoImOH-ohh9DLKmYAo3IdvmZyoANyLQLh1PMquwRlF9eTXEkrAsy5q_N_uhAtBflnh0giWwLChbD0NjsO7wEYTWXckLtzFxlwgC0hloMoDr_5BArIyAzY9mAbaEsSj3rpPkNyvVXjKELE808stAvQgun7M7hlY7uvinfrX7PweXq55ju2f1MtagZPgTx_CyfYl2Ftjtt60FXThSTjk3Z3zlP1XaXhum2II9x9Xux1NVdz4-LnoPA-W3wtrJsQ4nareE53SQGJN6FWxdRxnyEJUvKK2UHugpg5aMtCz-3UfzIWt0MRK91B5Fu29G3k0WNvlIHfaUgrI6R1CHp8-pyi2nkEI30u-9N4muWA_v4wRgB0AM2DZokZfoKdBrhZgKfFOHX73W93bg7ZN9HrbdV9YXqMnL/download\n",
            "Resolving public.boxcloud.com (public.boxcloud.com)... 74.112.186.165, 2620:117:bff0:69::\n",
            "Connecting to public.boxcloud.com (public.boxcloud.com)|74.112.186.165|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2588314523 (2.4G) [application/octet-stream]\n",
            "Saving to: ‘saved_ckpt/sdfusion-snet-all.pth’\n",
            "\n",
            "saved_ckpt/sdfusion 100%[===================>]   2.41G  45.1MB/s    in 32s     \n",
            "\n",
            "2025-05-11 17:09:00 (76.9 MB/s) - ‘saved_ckpt/sdfusion-snet-all.pth’ saved [2588314523/2588314523]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "watercraft"
      ],
      "metadata": {
        "id": "zhCA-w0ghOwx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title initialize SDFusion model\n",
        "ckpt_path = 'saved_ckpt/sdfusion-txt2shape.pth'\n",
        "opt.init_model_args(ckpt_path=ckpt_path)\n",
        "\n",
        "SDFusion = create_model(opt)\n",
        "cprint(f'[*] \"{SDFusion.name()}\" loaded.', 'cyan')"
      ],
      "metadata": {
        "id": "Nu1cNW1dkW8C",
        "outputId": "4d967e9a-14ed-47b1-cc72-b8816dc4f7c8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373,
          "referenced_widgets": [
            "c77ca4f16aa849fcbd51cb16bdb22cdc",
            "6fe1dd115b7e4adb832fdc7c4ded7366",
            "d46ca147dc854b0da91404ed124eceba",
            "68a7fcf81a0d4695b714759880ce5238",
            "c4e4179930ec415da854864f18628c81",
            "e924f10f7ff64140a56973aa67726565",
            "c9ff1818e20347fdbea65541bb5e1611",
            "03b67c457158420f91390fb3e6893d29",
            "25e053d1684b4498853228d2b72a975f",
            "9a8bebb433034f47813a97d101f10cc0",
            "8e5bee842e1447aeb48f981aeadf894c",
            "e4520810fe8a44ec8bd91be918751f43",
            "1b19a32e33f74fa686a5eb18767b1761",
            "81add2e3a3f24ab2ac84d84c250c9de0",
            "14460c3f95624fb5808d1bb73fb6207c",
            "744e1c4f60ff4a579ff566b74eb3ef46",
            "b0a0c53350c24e729f74f0fdea9aafeb",
            "ed58b87b65874c41a590eebac46fa8f7",
            "a98ec91c561c40f9b59ebace49579397",
            "cef908833e434399bd8df323bdb137d0",
            "96f804c959df485cb52f3e461dbb58d2",
            "1a5c09bd882b4fce850280ee457fafcd",
            "a0b69cbceda644d8b6a10d5e38de5c0d",
            "5913ab7923134cad8dc81781b10e6005",
            "4042a235a4774c2f8098d6f032bf533c",
            "4eab10aa9fa6413b871a7bfcd0393059",
            "3464c9848be540e582822a99019b3c41",
            "39f15ccba796449e92126dbb4e93d8be",
            "8f2409654fc4442e86de723d059b6a03",
            "f868c4f7b80e4bd58b23901bee9e4f48",
            "e50a7d504ea346648c617a4d5779a872",
            "a5abf288fcb148acb682fe94620ed839",
            "a6e81d376bda4c989da7c54c84764919",
            "efe6ac01a4c04bb9a0c3e68221b23728",
            "8cc36c44ea7c4a27b76b8e678f903c7e",
            "f4c84c148975496493c6012cdc8a6d87",
            "c1717fb5d549414fba48009ecf2d7449",
            "52328a2b12d34f00a6b8542b0e8da026",
            "3b29f79c2b154f37b0d34bf36619237a",
            "3d9be72e92074c058f8a45db025c6aa4",
            "94aced33138c4b6b991279e688610002",
            "39434dd0ae1b4646a1f1709661d42bcf",
            "ec9fd65c5a5b44e99e94929256d9df0f",
            "9cf99614c73b40dd9453b73515354276"
          ]
        }
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Working with z of shape (1, 3, 16, 16, 16) = 12288 dimensions.\n",
            "[*] VQVAE: weight successfully load from: saved_ckpt/vqvae-snet-all.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c77ca4f16aa849fcbd51cb16bdb22cdc"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e4520810fe8a44ec8bd91be918751f43"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a0b69cbceda644d8b6a10d5e38de5c0d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "efe6ac01a4c04bb9a0c3e68221b23728"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[*] weight successfully load from: saved_ckpt/sdfusion-txt2shape.pth\n",
            "[*] setting ddim_steps=100\n",
            "[*] Model has been created: SDFusion-Text2Shape-Model\n",
            "[*] \"SDFusion-Text2Shape-Model\" loaded.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IeqFgJDoovon"
      },
      "source": [
        "## SDFusion: text-guided generation (txt2shape)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title initialize SDFusion model\n",
        "ckpt_path = 'saved_ckpt/sdfusion-snet-all.pth'\n",
        "opt.init_model_args(ckpt_path=ckpt_path)\n",
        "\n",
        "SDFusion = create_model(opt)\n",
        "cprint(f'[*] \"{SDFusion.name()}\" loaded.', 'cyan')"
      ],
      "metadata": {
        "id": "ZNjdSeL_OMT3",
        "outputId": "eb0dcc69-fae8-4420-de3e-67c59dae184a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Working with z of shape (1, 3, 16, 16, 16) = 12288 dimensions.\n",
            "[*] VQVAE: weight successfully load from: saved_ckpt/vqvae-snet-all.pth\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "Error(s) in loading state_dict for DiffusionUNet:\n\tMissing key(s) in state_dict: \"diffusion_net.input_blocks.4.1.proj_in.weight\", \"diffusion_net.input_blocks.4.1.proj_in.bias\", \"diffusion_net.input_blocks.4.1.transformer_blocks.0.attn1.to_q.weight\", \"diffusion_net.input_blocks.4.1.transformer_blocks.0.attn1.to_k.weight\", \"diffusion_net.input_blocks.4.1.transformer_blocks.0.attn1.to_v.weight\", \"diffusion_net.input_blocks.4.1.transformer_blocks.0.attn1.to_out.0.weight\", \"diffusion_net.input_blocks.4.1.transformer_blocks.0.attn1.to_out.0.bias\", \"diffusion_net.input_blocks.4.1.transformer_blocks.0.ff.net.0.proj.weight\", \"diffusion_net.input_blocks.4.1.transformer_blocks.0.ff.net.0.proj.bias\", \"diffusion_net.input_blocks.4.1.transformer_blocks.0.ff.net.2.weight\", \"diffusion_net.input_blocks.4.1.transformer_blocks.0.ff.net.2.bias\", \"diffusion_net.input_blocks.4.1.transformer_blocks.0.attn2.to_q.weight\", \"diffusion_net.input_blocks.4.1.transformer_blocks.0.attn2.to_k.weight\", \"diffusion_net.input_blocks.4.1.transformer_blocks.0.attn2.to_v.weight\", \"diffusion_net.input_blocks.4.1.transformer_blocks.0.attn2.to_out.0.weight\", \"diffusion_net.input_blocks.4.1.transformer_blocks.0.attn2.to_out.0.bias\", \"diffusion_net.input_blocks.4.1.transformer_blocks.0.norm1.weight\", \"diffusion_net.input_blocks.4.1.transformer_blocks.0.norm1.bias\", \"diffusion_net.input_blocks.4.1.transformer_blocks.0.norm2.weight\", \"diffusion_net.input_blocks.4.1.transformer_blocks.0.norm2.bias\", \"diffusion_net.input_blocks.4.1.transformer_blocks.0.norm3.weight\", \"diffusion_net.input_blocks.4.1.transformer_blocks.0.norm3.bias\", \"diffusion_net.input_blocks.5.1.proj_in.weight\", \"diffusion_net.input_blocks.5.1.proj_in.bias\", \"diffusion_net.input_blocks.5.1.transformer_blocks.0.attn1.to_q.weight\", \"diffusion_net.input_blocks.5.1.transformer_blocks.0.attn1.to_k.weight\", \"diffusion_net.input_blocks.5.1.transformer_blocks.0.attn1.to_v.weight\", \"diffusion_net.input_blocks.5.1.transformer_blocks.0.attn1.to_out.0.weight\", \"diffusion_net.input_blocks.5.1.transformer_blocks.0.attn1.to_out.0.bias\", \"diffusion_net.input_blocks.5.1.transformer_blocks.0.ff.net.0.proj.weight\", \"diffusion_net.input_blocks.5.1.transformer_blocks.0.ff.net.0.proj.bias\", \"diffusion_net.input_blocks.5.1.transformer_blocks.0.ff.net.2.weight\", \"diffusion_net.input_blocks.5.1.transformer_blocks.0.ff.net.2.bias\", \"diffusion_net.input_blocks.5.1.transformer_blocks.0.attn2.to_q.weight\", \"diffusion_net.input_blocks.5.1.transformer_blocks.0.attn2.to_k.weight\", \"diffusion_net.input_blocks.5.1.transformer_blocks.0.attn2.to_v.weight\", \"diffusion_net.input_blocks.5.1.transformer_blocks.0.attn2.to_out.0.weight\", \"diffusion_net.input_blocks.5.1.transformer_blocks.0.attn2.to_out.0.bias\", \"diffusion_net.input_blocks.5.1.transformer_blocks.0.norm1.weight\", \"diffusion_net.input_blocks.5.1.transformer_blocks.0.norm1.bias\", \"diffusion_net.input_blocks.5.1.transformer_blocks.0.norm2.weight\", \"diffusion_net.input_blocks.5.1.transformer_blocks.0.norm2.bias\", \"diffusion_net.input_blocks.5.1.transformer_blocks.0.norm3.weight\", \"diffusion_net.input_blocks.5.1.transformer_blocks.0.norm3.bias\", \"diffusion_net.input_blocks.7.1.proj_in.weight\", \"diffusion_net.input_blocks.7.1.proj_in.bias\", \"diffusion_net.input_blocks.7.1.transformer_blocks.0.attn1.to_q.weight\", \"diffusion_net.input_blocks.7.1.transformer_blocks.0.attn1.to_k.weight\", \"diffusion_net.input_blocks.7.1.transformer_blocks.0.attn1.to_v.weight\", \"diffusion_net.input_blocks.7.1.transformer_blocks.0.attn1.to_out.0.weight\", \"diffusion_net.input_blocks.7.1.transformer_blocks.0.attn1.to_out.0.bias\", \"diffusion_net.input_blocks.7.1.transformer_blocks.0.ff.net.0.proj.weight\", \"diffusion_net.input_blocks.7.1.transformer_blocks.0.ff.net.0.proj.bias\", \"diffusion_net.input_blocks.7.1.transformer_blocks.0.ff.net.2.weight\", \"diffusion_net.input_blocks.7.1.transformer_blocks.0.ff.net.2.bias\", \"diffusion_net.input_blocks.7.1.transformer_blocks.0.attn2.to_q.weight\", \"diffusion_net.input_blocks.7.1.transformer_blocks.0.attn2.to_k.weight\", \"diffusion_net.input_blocks.7.1.transformer_blocks.0.attn2.to_v.weight\", \"diffusion_net.input_blocks.7.1.transformer_blocks.0.attn2.to_out.0.weight\", \"diffusion_net.input_blocks.7.1.transformer_blocks.0.attn2.to_out.0.bias\", \"diffusion_net.input_blocks.7.1.transformer_blocks.0.norm1.weight\", \"diffusion_net.input_blocks.7.1.transformer_blocks.0.norm1.bias\", \"diffusion_net.input_blocks.7.1.transformer_blocks.0.norm2.weight\", \"diffusion_net.input_blocks.7.1.transformer_blocks.0.norm2.bias\", \"diffusion_net.input_blocks.7.1.transformer_blocks.0.norm3.weight\", \"diffusion_net.input_blocks.7.1.transformer_blocks.0.norm3.bias\", \"diffusion_net.input_blocks.8.1.proj_in.weight\", \"diffusion_net.input_blocks.8.1.proj_in.bias\", \"diffusion_net.input_blocks.8.1.transformer_blocks.0.attn1.to_q.weight\", \"diffusion_net.input_blocks.8.1.transformer_blocks.0.attn1.to_k.weight\", \"diffusion_net.input_blocks.8.1.transformer_blocks.0.attn1.to_v.weight\", \"diffusion_net.input_blocks.8.1.transformer_blocks.0.attn1.to_out.0.weight\", \"diffusion_net.input_blocks.8.1.transformer_blocks.0.attn1.to_out.0.bias\", \"diffusion_net.input_blocks.8.1.transformer_blocks.0.ff.net.0.proj.weight\", \"diffusion_net.input_blocks.8.1.transformer_blocks.0.ff.net.0.proj.bias\", \"diffusion_net.input_blocks.8.1.transformer_blocks.0.ff.net.2.weight\", \"diffusion_net.input_blocks.8.1.transformer_blocks.0.ff.net.2.bias\", \"diffusion_net.input_blocks.8.1.transformer_blocks.0.attn2.to_q.weight\", \"diffusion_net.input_blocks.8.1.transformer_blocks.0.attn2.to_k.weight\", \"diffusion_net.input_blocks.8.1.transformer_blocks.0.attn2.to_v.weight\", \"diffusion_net.input_blocks.8.1.transformer_blocks.0.attn2.to_out.0.weight\", \"diffusion_net.input_blocks.8.1.transformer_blocks.0.attn2.to_out.0.bias\", \"diffusion_net.input_blocks.8.1.transformer_blocks.0.norm1.weight\", \"diffusion_net.input_blocks.8.1.transformer_blocks.0.norm1.bias\", \"diffusion_net.input_blocks.8.1.transformer_blocks.0.norm2.weight\", \"diffusion_net.input_blocks.8.1.transformer_blocks.0.norm2.bias\", \"diffusion_net.input_blocks.8.1.transformer_blocks.0.norm3.weight\", \"diffusion_net.input_blocks.8.1.transformer_blocks.0.norm3.bias\", \"diffusion_net.middle_block.1.proj_in.weight\", \"diffusion_net.middle_block.1.proj_in.bias\", \"diffusion_net.middle_block.1.transformer_blocks.0.attn1.to_q.weight\", \"diffusion_net.middle_block.1.transformer_blocks.0.attn1.to_k.weight\", \"diffusion_net.middle_block.1.transformer_blocks.0.attn1.to_v.weight\", \"diffusion_net.middle_block.1.transformer_blocks.0.attn1.to_out.0.weight\", \"diffusion_net.middle_block.1.transformer_blocks.0.attn1.to_out.0.bias\", \"diffusion_net.middle_block.1.transformer_blocks.0.ff.net.0.proj.weight\", \"diffusion_net.middle_block.1.transformer_blocks.0.ff.net.0.proj.bias\", \"diffusion_net.middle_block.1.transformer_blocks.0.ff.net.2.weight\", \"diffusion_net.middle_block.1.transformer_blocks.0.ff.net.2.bias\", \"diffusion_net.middle_block.1.transformer_blocks.0.attn2.to_q.weight\", \"diffusion_net.middle_block.1.transformer_blocks.0.attn2.to_k.weight\", \"diffusion_net.middle_block.1.transformer_blocks.0.attn2.to_v.weight\", \"diffusion_net.middle_block.1.transformer_blocks.0.attn2.to_out.0.weight\", \"diffusion_net.middle_block.1.transformer_blocks.0.attn2.to_out.0.bias\", \"diffusion_net.middle_block.1.transformer_blocks.0.norm1.weight\", \"diffusion_net.middle_block.1.transformer_blocks.0.norm1.bias\", \"diffusion_net.middle_block.1.transformer_blocks.0.norm2.weight\", \"diffusion_net.middle_block.1.transformer_blocks.0.norm2.bias\", \"diffusion_net.middle_block.1.transformer_blocks.0.norm3.weight\", \"diffusion_net.middle_block.1.transformer_blocks.0.norm3.bias\", \"diffusion_net.output_blocks.0.1.norm.weight\", \"diffusion_net.output_blocks.0.1.norm.bias\", \"diffusion_net.output_blocks.0.1.proj_in.weight\", \"diffusion_net.output_blocks.0.1.proj_in.bias\", \"diffusion_net.output_blocks.0.1.transformer_blocks.0.attn1.to_q.weight\", \"diffusion_net.output_blocks.0.1.transformer_blocks.0.attn1.to_k.weight\", \"diffusion_net.output_blocks.0.1.transformer_blocks.0.attn1.to_v.weight\", \"diffusion_net.output_blocks.0.1.transformer_blocks.0.attn1.to_out.0.weight\", \"diffusion_net.output_blocks.0.1.transformer_blocks.0.attn1.to_out.0.bias\", \"diffusion_net.output_blocks.0.1.transformer_blocks.0.ff.net.0.proj.weight\", \"diffusion_net.output_blocks.0.1.transformer_blocks.0.ff.net.0.proj.bias\", \"diffusion_net.output_blocks.0.1.transformer_blocks.0.ff.net.2.weight\", \"diffusion_net.output_blocks.0.1.transformer_blocks.0.ff.net.2.bias\", \"diffusion_net.output_blocks.0.1.transformer_blocks.0.attn2.to_q.weight\", \"diffusion_net.output_blocks.0.1.transformer_blocks.0.attn2.to_k.weight\", \"diffusion_net.output_blocks.0.1.transformer_blocks.0.attn2.to_v.weight\", \"diffusion_net.output_blocks.0.1.transformer_blocks.0.attn2.to_out.0.weight\", \"diffusion_net.output_blocks.0.1.transformer_blocks.0.attn2.to_out.0.bias\", \"diffusion_net.output_blocks.0.1.transformer_blocks.0.norm1.weight\", \"diffusion_net.output_blocks.0.1.transformer_blocks.0.norm1.bias\", \"diffusion_net.output_blocks.0.1.transformer_blocks.0.norm2.weight\", \"diffusion_net.output_blocks.0.1.transformer_blocks.0.norm2.bias\", \"diffusion_net.output_blocks.0.1.transformer_blocks.0.norm3.weight\", \"diffusion_net.output_blocks.0.1.transformer_blocks.0.norm3.bias\", \"diffusion_net.output_blocks.0.1.proj_out.weight\", \"diffusion_net.output_blocks.0.1.proj_out.bias\", \"diffusion_net.output_blocks.1.1.norm.weight\", \"diffusion_net.output_blocks.1.1.norm.bias\", \"diffusion_net.output_blocks.1.1.proj_in.weight\", \"diffusion_net.output_blocks.1.1.proj_in.bias\", \"diffusion_net.output_blocks.1.1.transformer_blocks.0.attn1.to_q.weight\", \"diffusion_net.output_blocks.1.1.transformer_blocks.0.attn1.to_k.weight\", \"diffusion_net.output_blocks.1.1.transformer_blocks.0.attn1.to_v.weight\", \"diffusion_net.output_blocks.1.1.transformer_blocks.0.attn1.to_out.0.weight\", \"diffusion_net.output_blocks.1.1.transformer_blocks.0.attn1.to_out.0.bias\", \"diffusion_net.output_blocks.1.1.transformer_blocks.0.ff.net.0.proj.weight\", \"diffusion_net.output_blocks.1.1.transformer_blocks.0.ff.net.0.proj.bias\", \"diffusion_net.output_blocks.1.1.transformer_blocks.0.ff.net.2.weight\", \"diffusion_net.output_blocks.1.1.transformer_blocks.0.ff.net.2.bias\", \"diffusion_net.output_blocks.1.1.transformer_blocks.0.attn2.to_q.weight\", \"diffusion_net.output_blocks.1.1.transformer_blocks.0.attn2.to_k.weight\", \"diffusion_net.output_blocks.1.1.transformer_blocks.0.attn2.to_v.weight\", \"diffusion_net.output_blocks.1.1.transformer_blocks.0.attn2.to_out.0.weight\", \"diffusion_net.output_blocks.1.1.transformer_blocks.0.attn2.to_out.0.bias\", \"diffusion_net.output_blocks.1.1.transformer_blocks.0.norm1.weight\", \"diffusion_net.output_blocks.1.1.transformer_blocks.0.norm1.bias\", \"diffusion_net.output_blocks.1.1.transformer_blocks.0.norm2.weight\", \"diffusion_net.output_blocks.1.1.transformer_blocks.0.norm2.bias\", \"diffusion_net.output_blocks.1.1.transformer_blocks.0.norm3.weight\", \"diffusion_net.output_blocks.1.1.transformer_blocks.0.norm3.bias\", \"diffusion_net.output_blocks.1.1.proj_out.weight\", \"diffusion_net.output_blocks.1.1.proj_out.bias\", \"diffusion_net.output_blocks.2.1.norm.weight\", \"diffusion_net.output_blocks.2.1.norm.bias\", \"diffusion_net.output_blocks.2.1.proj_in.weight\", \"diffusion_net.output_blocks.2.1.proj_in.bias\", \"diffusion_net.output_blocks.2.1.transformer_blocks.0.attn1.to_q.weight\", \"diffusion_net.output_blocks.2.1.transformer_blocks.0.attn1.to_k.weight\", \"diffusion_net.output_blocks.2.1.transformer_blocks.0.attn1.to_v.weight\", \"diffusion_net.output_blocks.2.1.transformer_blocks.0.attn1.to_out.0.weight\", \"diffusion_net.output_blocks.2.1.transformer_blocks.0.attn1.to_out.0.bias\", \"diffusion_net.output_blocks.2.1.transformer_blocks.0.ff.net.0.proj.weight\", \"diffusion_net.output_blocks.2.1.transformer_blocks.0.ff.net.0.proj.bias\", \"diffusion_net.output_blocks.2.1.transformer_blocks.0.ff.net.2.weight\", \"diffusion_net.output_blocks.2.1.transformer_blocks.0.ff.net.2.bias\", \"diffusion_net.output_blocks.2.1.transformer_blocks.0.attn2.to_q.weight\", \"diffusion_net.output_blocks.2.1.transformer_blocks.0.attn2.to_k.weight\", \"diffusion_net.output_blocks.2.1.transformer_blocks.0.attn2.to_v.weight\", \"diffusion_net.output_blocks.2.1.transformer_blocks.0.attn2.to_out.0.weight\", \"diffusion_net.output_blocks.2.1.transformer_blocks.0.attn2.to_out.0.bias\", \"diffusion_net.output_blocks.2.1.transformer_blocks.0.norm1.weight\", \"diffusion_net.output_blocks.2.1.transformer_blocks.0.norm1.bias\", \"diffusion_net.output_blocks.2.1.transformer_blocks.0.norm2.weight\", \"diffusion_net.output_blocks.2.1.transformer_blocks.0.norm2.bias\", \"diffusion_net.output_blocks.2.1.transformer_blocks.0.norm3.weight\", \"diffusion_net.output_blocks.2.1.transformer_blocks.0.norm3.bias\", \"diffusion_net.output_blocks.2.1.proj_out.weight\", \"diffusion_net.output_blocks.2.1.proj_out.bias\", \"diffusion_net.output_blocks.2.2.conv.weight\", \"diffusion_net.output_blocks.2.2.conv.bias\", \"diffusion_net.output_blocks.3.1.proj_in.weight\", \"diffusion_net.output_blocks.3.1.proj_in.bias\", \"diffusion_net.output_blocks.3.1.transformer_blocks.0.attn1.to_q.weight\", \"diffusion_net.output_blocks.3.1.transformer_blocks.0.attn1.to_k.weight\", \"diffusion_net.output_blocks.3.1.transformer_blocks.0.attn1.to_v.weight\", \"diffusion_net.output_blocks.3.1.transformer_blocks.0.attn1.to_out.0.weight\", \"diffusion_net.output_blocks.3.1.transformer_blocks.0.attn1.to_out.0.bias\", \"diffusion_net.output_blocks.3.1.transformer_blocks.0.ff.net.0.proj.weight\", \"diffusion_net.output_blocks.3.1.transformer_blocks.0.ff.net.0.proj.bias\", \"diffusion_net.output_blocks.3.1.transformer_blocks.0.ff.net.2.weight\", \"diffusion_net.output_blocks.3.1.transformer_blocks.0.ff.net.2.bias\", \"diffusion_net.output_blocks.3.1.transformer_blocks.0.attn2.to_q.weight\", \"diffusion_net.output_blocks.3.1.transformer_blocks.0.attn2.to_k.weight\", \"diffusion_net.output_blocks.3.1.transformer_blocks.0.attn2.to_v.weight\", \"diffusion_net.output_blocks.3.1.transformer_blocks.0.attn2.to_out.0.weight\", \"diffusion_net.output_blocks.3.1.transformer_blocks.0.attn2.to_out.0.bias\", \"diffusion_net.output_blocks.3.1.transformer_blocks.0.norm1.weight\", \"diffusion_net.output_blocks.3.1.transformer_blocks.0.norm1.bias\", \"diffusion_net.output_blocks.3.1.transformer_blocks.0.norm2.weight\", \"diffusion_net.output_blocks.3.1.transformer_blocks.0.norm2.bias\", \"diffusion_net.output_blocks.3.1.transformer_blocks.0.norm3.weight\", \"diffusion_net.output_blocks.3.1.transformer_blocks.0.norm3.bias\", \"diffusion_net.output_blocks.4.1.proj_in.weight\", \"diffusion_net.output_blocks.4.1.proj_in.bias\", \"diffusion_net.output_blocks.4.1.transformer_blocks.0.attn1.to_q.weight\", \"diffusion_net.output_blocks.4.1.transformer_blocks.0.attn1.to_k.weight\", \"diffusion_net.output_blocks.4.1.transformer_blocks.0.attn1.to_v.weight\", \"diffusion_net.output_blocks.4.1.transformer_blocks.0.attn1.to_out.0.weight\", \"diffusion_net.output_blocks.4.1.transformer_blocks.0.attn1.to_out.0.bias\", \"diffusion_net.output_blocks.4.1.transformer_blocks.0.ff.net.0.proj.weight\", \"diffusion_net.output_blocks.4.1.transformer_blocks.0.ff.net.0.proj.bias\", \"diffusion_net.output_blocks.4.1.transformer_blocks.0.ff.net.2.weight\", \"diffusion_net.output_blocks.4.1.transformer_blocks.0.ff.net.2.bias\", \"diffusion_net.output_blocks.4.1.transformer_blocks.0.attn2.to_q.weight\", \"diffusion_net.output_blocks.4.1.transformer_blocks.0.attn2.to_k.weight\", \"diffusion_net.output_blocks.4.1.transformer_blocks.0.attn2.to_v.weight\", \"diffusion_net.output_blocks.4.1.transformer_blocks.0.attn2.to_out.0.weight\", \"diffusion_net.output_blocks.4.1.transformer_blocks.0.attn2.to_out.0.bias\", \"diffusion_net.output_blocks.4.1.transformer_blocks.0.norm1.weight\", \"diffusion_net.output_blocks.4.1.transformer_blocks.0.norm1.bias\", \"diffusion_net.output_blocks.4.1.transformer_blocks.0.norm2.weight\", \"diffusion_net.output_blocks.4.1.transformer_blocks.0.norm2.bias\", \"diffusion_net.output_blocks.4.1.transformer_blocks.0.norm3.weight\", \"diffusion_net.output_blocks.4.1.transformer_blocks.0.norm3.bias\", \"diffusion_net.output_blocks.5.1.proj_in.weight\", \"diffusion_net.output_blocks.5.1.proj_in.bias\", \"diffusion_net.output_blocks.5.1.transformer_blocks.0.attn1.to_q.weight\", \"diffusion_net.output_blocks.5.1.transformer_blocks.0.attn1.to_k.weight\", \"diffusion_net.output_blocks.5.1.transformer_blocks.0.attn1.to_v.weight\", \"diffusion_net.output_blocks.5.1.transformer_blocks.0.attn1.to_out.0.weight\", \"diffusion_net.output_blocks.5.1.transformer_blocks.0.attn1.to_out.0.bias\", \"diffusion_net.output_blocks.5.1.transformer_blocks.0.ff.net.0.proj.weight\", \"diffusion_net.output_blocks.5.1.transformer_blocks.0.ff.net.0.proj.bias\", \"diffusion_net.output_blocks.5.1.transformer_blocks.0.ff.net.2.weight\", \"diffusion_net.output_blocks.5.1.transformer_blocks.0.ff.net.2.bias\", \"diffusion_net.output_blocks.5.1.transformer_blocks.0.attn2.to_q.weight\", \"diffusion_net.output_blocks.5.1.transformer_blocks.0.attn2.to_k.weight\", \"diffusion_net.output_blocks.5.1.transformer_blocks.0.attn2.to_v.weight\", \"diffusion_net.output_blocks.5.1.transformer_blocks.0.attn2.to_out.0.weight\", \"diffusion_net.output_blocks.5.1.transformer_blocks.0.attn2.to_out.0.bias\", \"diffusion_net.output_blocks.5.1.transformer_blocks.0.norm1.weight\", \"diffusion_net.output_blocks.5.1.transformer_blocks.0.norm1.bias\", \"diffusion_net.output_blocks.5.1.transformer_blocks.0.norm2.weight\", \"diffusion_net.output_blocks.5.1.transformer_blocks.0.norm2.bias\", \"diffusion_net.output_blocks.5.1.transformer_blocks.0.norm3.weight\", \"diffusion_net.output_blocks.5.1.transformer_blocks.0.norm3.bias\". \n\tUnexpected key(s) in state_dict: \"diffusion_net.input_blocks.9.0.op.weight\", \"diffusion_net.input_blocks.9.0.op.bias\", \"diffusion_net.input_blocks.10.0.in_layers.0.weight\", \"diffusion_net.input_blocks.10.0.in_layers.0.bias\", \"diffusion_net.input_blocks.10.0.in_layers.2.weight\", \"diffusion_net.input_blocks.10.0.in_layers.2.bias\", \"diffusion_net.input_blocks.10.0.emb_layers.1.weight\", \"diffusion_net.input_blocks.10.0.emb_layers.1.bias\", \"diffusion_net.input_blocks.10.0.out_layers.0.weight\", \"diffusion_net.input_blocks.10.0.out_layers.0.bias\", \"diffusion_net.input_blocks.10.0.out_layers.3.weight\", \"diffusion_net.input_blocks.10.0.out_layers.3.bias\", \"diffusion_net.input_blocks.11.0.in_layers.0.weight\", \"diffusion_net.input_blocks.11.0.in_layers.0.bias\", \"diffusion_net.input_blocks.11.0.in_layers.2.weight\", \"diffusion_net.input_blocks.11.0.in_layers.2.bias\", \"diffusion_net.input_blocks.11.0.emb_layers.1.weight\", \"diffusion_net.input_blocks.11.0.emb_layers.1.bias\", \"diffusion_net.input_blocks.11.0.out_layers.0.weight\", \"diffusion_net.input_blocks.11.0.out_layers.0.bias\", \"diffusion_net.input_blocks.11.0.out_layers.3.weight\", \"diffusion_net.input_blocks.11.0.out_layers.3.bias\", \"diffusion_net.input_blocks.1.1.norm.weight\", \"diffusion_net.input_blocks.1.1.norm.bias\", \"diffusion_net.input_blocks.1.1.qkv.weight\", \"diffusion_net.input_blocks.1.1.qkv.bias\", \"diffusion_net.input_blocks.1.1.proj_out.weight\", \"diffusion_net.input_blocks.1.1.proj_out.bias\", \"diffusion_net.input_blocks.2.1.norm.weight\", \"diffusion_net.input_blocks.2.1.norm.bias\", \"diffusion_net.input_blocks.2.1.qkv.weight\", \"diffusion_net.input_blocks.2.1.qkv.bias\", \"diffusion_net.input_blocks.2.1.proj_out.weight\", \"diffusion_net.input_blocks.2.1.proj_out.bias\", \"diffusion_net.input_blocks.4.1.qkv.weight\", \"diffusion_net.input_blocks.4.1.qkv.bias\", \"diffusion_net.input_blocks.5.1.qkv.weight\", \"diffusion_net.input_blocks.5.1.qkv.bias\", \"diffusion_net.input_blocks.7.1.qkv.weight\", \"diffusion_net.input_blocks.7.1.qkv.bias\", \"diffusion_net.input_blocks.8.1.qkv.weight\", \"diffusion_net.input_blocks.8.1.qkv.bias\", \"diffusion_net.middle_block.1.qkv.weight\", \"diffusion_net.middle_block.1.qkv.bias\", \"diffusion_net.output_blocks.9.0.in_layers.0.weight\", \"diffusion_net.output_blocks.9.0.in_layers.0.bias\", \"diffusion_net.output_blocks.9.0.in_layers.2.weight\", \"diffusion_net.output_blocks.9.0.in_layers.2.bias\", \"diffusion_net.output_blocks.9.0.emb_layers.1.weight\", \"diffusion_net.output_blocks.9.0.emb_layers.1.bias\", \"diffusion_net.output_blocks.9.0.out_layers.0.weight\", \"diffusion_net.output_blocks.9.0.out_layers.0.bias\", \"diffusion_net.output_blocks.9.0.out_layers.3.weight\", \"diffusion_net.output_blocks.9.0.out_layers.3.bias\", \"diffusion_net.output_blocks.9.0.skip_connection.weight\", \"diffusion_net.output_blocks.9.0.skip_connection.bias\", \"diffusion_net.output_blocks.9.1.norm.weight\", \"diffusion_net.output_blocks.9.1.norm.bias\", \"diffusion_net.output_blocks.9.1.qkv.weight\", \"diffusion_net.output_blocks.9.1.qkv.bias\", \"diffusion_net.output_blocks.9.1.proj_out.weight\", \"diffusion_net.output_blocks.9.1.proj_out.bias\", \"diffusion_net.output_blocks.10.0.in_layers.0.weight\", \"diffusion_net.output_blocks.10.0.in_layers.0.bias\", \"diffusion_net.output_blocks.10.0.in_layers.2.weight\", \"diffusion_net.output_blocks.10.0.in_layers.2.bias\", \"diffusion_net.output_blocks.10.0.emb_layers.1.weight\", \"diffusion_net.output_blocks.10.0.emb_layers.1.bias\", \"diffusion_net.output_blocks.10.0.out_layers.0.weight\", \"diffusion_net.output_blocks.10.0.out_layers.0.bias\", \"diffusion_net.output_blocks.10.0.out_layers.3.weight\", \"diffusion_net.output_blocks.10.0.out_layers.3.bias\", \"diffusion_net.output_blocks.10.0.skip_connection.weight\", \"diffusion_net.output_blocks.10.0.skip_connection.bias\", \"diffusion_net.output_blocks.10.1.norm.weight\", \"diffusion_net.output_blocks.10.1.norm.bias\", \"diffusion_net.output_blocks.10.1.qkv.weight\", \"diffusion_net.output_blocks.10.1.qkv.bias\", \"diffusion_net.output_blocks.10.1.proj_out.weight\", \"diffusion_net.output_blocks.10.1.proj_out.bias\", \"diffusion_net.output_blocks.11.0.in_layers.0.weight\", \"diffusion_net.output_blocks.11.0.in_layers.0.bias\", \"diffusion_net.output_blocks.11.0.in_layers.2.weight\", \"diffusion_net.output_blocks.11.0.in_layers.2.bias\", \"diffusion_net.output_blocks.11.0.emb_layers.1.weight\", \"diffusion_net.output_blocks.11.0.emb_layers.1.bias\", \"diffusion_net.output_blocks.11.0.out_layers.0.weight\", \"diffusion_net.output_blocks.11.0.out_layers.0.bias\", \"diffusion_net.output_blocks.11.0.out_layers.3.weight\", \"diffusion_net.output_blocks.11.0.out_layers.3.bias\", \"diffusion_net.output_blocks.11.0.skip_connection.weight\", \"diffusion_net.output_blocks.11.0.skip_connection.bias\", \"diffusion_net.output_blocks.11.1.norm.weight\", \"diffusion_net.output_blocks.11.1.norm.bias\", \"diffusion_net.output_blocks.11.1.qkv.weight\", \"diffusion_net.output_blocks.11.1.qkv.bias\", \"diffusion_net.output_blocks.11.1.proj_out.weight\", \"diffusion_net.output_blocks.11.1.proj_out.bias\", \"diffusion_net.output_blocks.2.1.conv.weight\", \"diffusion_net.output_blocks.2.1.conv.bias\", \"diffusion_net.output_blocks.3.1.qkv.weight\", \"diffusion_net.output_blocks.3.1.qkv.bias\", \"diffusion_net.output_blocks.4.1.qkv.weight\", \"diffusion_net.output_blocks.4.1.qkv.bias\", \"diffusion_net.output_blocks.5.1.qkv.weight\", \"diffusion_net.output_blocks.5.1.qkv.bias\", \"diffusion_net.output_blocks.6.1.norm.weight\", \"diffusion_net.output_blocks.6.1.norm.bias\", \"diffusion_net.output_blocks.6.1.qkv.weight\", \"diffusion_net.output_blocks.6.1.qkv.bias\", \"diffusion_net.output_blocks.6.1.proj_out.weight\", \"diffusion_net.output_blocks.6.1.proj_out.bias\", \"diffusion_net.output_blocks.7.1.norm.weight\", \"diffusion_net.output_blocks.7.1.norm.bias\", \"diffusion_net.output_blocks.7.1.qkv.weight\", \"diffusion_net.output_blocks.7.1.qkv.bias\", \"diffusion_net.output_blocks.7.1.proj_out.weight\", \"diffusion_net.output_blocks.7.1.proj_out.bias\", \"diffusion_net.output_blocks.8.1.norm.weight\", \"diffusion_net.output_blocks.8.1.norm.bias\", \"diffusion_net.output_blocks.8.1.qkv.weight\", \"diffusion_net.output_blocks.8.1.qkv.bias\", \"diffusion_net.output_blocks.8.1.proj_out.weight\", \"diffusion_net.output_blocks.8.1.proj_out.bias\", \"diffusion_net.output_blocks.8.2.conv.weight\", \"diffusion_net.output_blocks.8.2.conv.bias\". \n\tsize mismatch for diffusion_net.time_embed.0.weight: copying a param with shape torch.Size([768, 192]) from checkpoint, the shape in current model is torch.Size([896, 224]).\n\tsize mismatch for diffusion_net.time_embed.0.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([896]).\n\tsize mismatch for diffusion_net.time_embed.2.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([896, 896]).\n\tsize mismatch for diffusion_net.time_embed.2.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([896]).\n\tsize mismatch for diffusion_net.input_blocks.0.0.weight: copying a param with shape torch.Size([192, 3, 3, 3, 3]) from checkpoint, the shape in current model is torch.Size([224, 3, 3, 3, 3]).\n\tsize mismatch for diffusion_net.input_blocks.0.0.bias: copying a param with shape torch.Size([192]) from checkpoint, the shape in current model is torch.Size([224]).\n\tsize mismatch for diffusion_net.input_blocks.1.0.in_layers.0.weight: copying a param with shape torch.Size([192]) from checkpoint, the shape in current model is torch.Size([224]).\n\tsize mismatch for diffusion_net.input_blocks.1.0.in_layers.0.bias: copying a param with shape torch.Size([192]) from checkpoint, the shape in current model is torch.Size([224]).\n\tsize mismatch for diffusion_net.input_blocks.1.0.in_layers.2.weight: copying a param with shape torch.Size([192, 192, 3, 3, 3]) from checkpoint, the shape in current model is torch.Size([224, 224, 3, 3, 3]).\n\tsize mismatch for diffusion_net.input_blocks.1.0.in_layers.2.bias: copying a param with shape torch.Size([192]) from checkpoint, the shape in current model is torch.Size([224]).\n\tsize mismatch for diffusion_net.input_blocks.1.0.emb_layers.1.weight: copying a param with shape torch.Size([192, 768]) from checkpoint, the shape in current model is torch.Size([224, 896]).\n\tsize mismatch for diffusion_net.input_blocks.1.0.emb_layers.1.bias: copying a param with shape torch.Size([192]) from checkpoint, the shape in current model is torch.Size([224]).\n\tsize mismatch for diffusion_net.input_blocks.1.0.out_layers.0.weight: copying a param with shape torch.Size([192]) from checkpoint, the shape in current model is torch.Size([224]).\n\tsize mismatch for diffusion_net.input_blocks.1.0.out_layers.0.bias: copying a param with shape torch.Size([192]) from checkpoint, the shape in current model is torch.Size([224]).\n\tsize mismatch for diffusion_net.input_blocks.1.0.out_layers.3.weight: copying a param with shape torch.Size([192, 192, 3, 3, 3]) from checkpoint, the shape in current model is torch.Size([224, 224, 3, 3, 3]).\n\tsize mismatch for diffusion_net.input_blocks.1.0.out_layers.3.bias: copying a param with shape torch.Size([192]) from checkpoint, the shape in current model is torch.Size([224]).\n\tsize mismatch for diffusion_net.input_blocks.2.0.in_layers.0.weight: copying a param with shape torch.Size([192]) from checkpoint, the shape in current model is torch.Size([224]).\n\tsize mismatch for diffusion_net.input_blocks.2.0.in_layers.0.bias: copying a param with shape torch.Size([192]) from checkpoint, the shape in current model is torch.Size([224]).\n\tsize mismatch for diffusion_net.input_blocks.2.0.in_layers.2.weight: copying a param with shape torch.Size([192, 192, 3, 3, 3]) from checkpoint, the shape in current model is torch.Size([224, 224, 3, 3, 3]).\n\tsize mismatch for diffusion_net.input_blocks.2.0.in_layers.2.bias: copying a param with shape torch.Size([192]) from checkpoint, the shape in current model is torch.Size([224]).\n\tsize mismatch for diffusion_net.input_blocks.2.0.emb_layers.1.weight: copying a param with shape torch.Size([192, 768]) from checkpoint, the shape in current model is torch.Size([224, 896]).\n\tsize mismatch for diffusion_net.input_blocks.2.0.emb_layers.1.bias: copying a param with shape torch.Size([192]) from checkpoint, the shape in current model is torch.Size([224]).\n\tsize mismatch for diffusion_net.input_blocks.2.0.out_layers.0.weight: copying a param with shape torch.Size([192]) from checkpoint, the shape in current model is torch.Size([224]).\n\tsize mismatch for diffusion_net.input_blocks.2.0.out_layers.0.bias: copying a param with shape torch.Size([192]) from checkpoint, the shape in current model is torch.Size([224]).\n\tsize mismatch for diffusion_net.input_blocks.2.0.out_layers.3.weight: copying a param with shape torch.Size([192, 192, 3, 3, 3]) from checkpoint, the shape in current model is torch.Size([224, 224, 3, 3, 3]).\n\tsize mismatch for diffusion_net.input_blocks.2.0.out_layers.3.bias: copying a param with shape torch.Size([192]) from checkpoint, the shape in current model is torch.Size([224]).\n\tsize mismatch for diffusion_net.input_blocks.3.0.op.weight: copying a param with shape torch.Size([192, 192, 3, 3, 3]) from checkpoint, the shape in current model is torch.Size([224, 224, 3, 3, 3]).\n\tsize mismatch for diffusion_net.input_blocks.3.0.op.bias: copying a param with shape torch.Size([192]) from checkpoint, the shape in current model is torch.Size([224]).\n\tsize mismatch for diffusion_net.input_blocks.4.0.in_layers.0.weight: copying a param with shape torch.Size([192]) from checkpoint, the shape in current model is torch.Size([224]).\n\tsize mismatch for diffusion_net.input_blocks.4.0.in_layers.0.bias: copying a param with shape torch.Size([192]) from checkpoint, the shape in current model is torch.Size([224]).\n\tsize mismatch for diffusion_net.input_blocks.4.0.in_layers.2.weight: copying a param with shape torch.Size([384, 192, 3, 3, 3]) from checkpoint, the shape in current model is torch.Size([448, 224, 3, 3, 3]).\n\tsize mismatch for diffusion_net.input_blocks.4.0.in_layers.2.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([448]).\n\tsize mismatch for diffusion_net.input_blocks.4.0.emb_layers.1.weight: copying a param with shape torch.Size([384, 768]) from checkpoint, the shape in current model is torch.Size([448, 896]).\n\tsize mismatch for diffusion_net.input_blocks.4.0.emb_layers.1.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([448]).\n\tsize mismatch for diffusion_net.input_blocks.4.0.out_layers.0.weight: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([448]).\n\tsize mismatch for diffusion_net.input_blocks.4.0.out_layers.0.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([448]).\n\tsize mismatch for diffusion_net.input_blocks.4.0.out_layers.3.weight: copying a param with shape torch.Size([384, 384, 3, 3, 3]) from checkpoint, the shape in current model is torch.Size([448, 448, 3, 3, 3]).\n\tsize mismatch for diffusion_net.input_blocks.4.0.out_layers.3.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([448]).\n\tsize mismatch for diffusion_net.input_blocks.4.0.skip_connection.weight: copying a param with shape torch.Size([384, 192, 1, 1, 1]) from checkpoint, the shape in current model is torch.Size([448, 224, 1, 1, 1]).\n\tsize mismatch for diffusion_net.input_blocks.4.0.skip_connection.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([448]).\n\tsize mismatch for diffusion_net.input_blocks.4.1.norm.weight: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([448]).\n\tsize mismatch for diffusion_net.input_blocks.4.1.norm.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([448]).\n\tsize mismatch for diffusion_net.input_blocks.4.1.proj_out.weight: copying a param with shape torch.Size([384, 384, 1]) from checkpoint, the shape in current model is torch.Size([448, 448, 1, 1, 1]).\n\tsize mismatch for diffusion_net.input_blocks.4.1.proj_out.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([448]).\n\tsize mismatch for diffusion_net.input_blocks.5.0.in_layers.0.weight: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([448]).\n\tsize mismatch for diffusion_net.input_blocks.5.0.in_layers.0.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([448]).\n\tsize mismatch for diffusion_net.input_blocks.5.0.in_layers.2.weight: copying a param with shape torch.Size([384, 384, 3, 3, 3]) from checkpoint, the shape in current model is torch.Size([448, 448, 3, 3, 3]).\n\tsize mismatch for diffusion_net.input_blocks.5.0.in_layers.2.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([448]).\n\tsize mismatch for diffusion_net.input_blocks.5.0.emb_layers.1.weight: copying a param with shape torch.Size([384, 768]) from checkpoint, the shape in current model is torch.Size([448, 896]).\n\tsize mismatch for diffusion_net.input_blocks.5.0.emb_layers.1.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([448]).\n\tsize mismatch for diffusion_net.input_blocks.5.0.out_layers.0.weight: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([448]).\n\tsize mismatch for diffusion_net.input_blocks.5.0.out_layers.0.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([448]).\n\tsize mismatch for diffusion_net.input_blocks.5.0.out_layers.3.weight: copying a param with shape torch.Size([384, 384, 3, 3, 3]) from checkpoint, the shape in current model is torch.Size([448, 448, 3, 3, 3]).\n\tsize mismatch for diffusion_net.input_blocks.5.0.out_layers.3.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([448]).\n\tsize mismatch for diffusion_net.input_blocks.5.1.norm.weight: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([448]).\n\tsize mismatch for diffusion_net.input_blocks.5.1.norm.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([448]).\n\tsize mismatch for diffusion_net.input_blocks.5.1.proj_out.weight: copying a param with shape torch.Size([384, 384, 1]) from checkpoint, the shape in current model is torch.Size([448, 448, 1, 1, 1]).\n\tsize mismatch for diffusion_net.input_blocks.5.1.proj_out.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([448]).\n\tsize mismatch for diffusion_net.input_blocks.6.0.op.weight: copying a param with shape torch.Size([384, 384, 3, 3, 3]) from checkpoint, the shape in current model is torch.Size([448, 448, 3, 3, 3]).\n\tsize mismatch for diffusion_net.input_blocks.6.0.op.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([448]).\n\tsize mismatch for diffusion_net.input_blocks.7.0.in_layers.0.weight: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([448]).\n\tsize mismatch for diffusion_net.input_blocks.7.0.in_layers.0.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([448]).\n\tsize mismatch for diffusion_net.input_blocks.7.0.in_layers.2.weight: copying a param with shape torch.Size([768, 384, 3, 3, 3]) from checkpoint, the shape in current model is torch.Size([672, 448, 3, 3, 3]).\n\tsize mismatch for diffusion_net.input_blocks.7.0.in_layers.2.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([672]).\n\tsize mismatch for diffusion_net.input_blocks.7.0.emb_layers.1.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([672, 896]).\n\tsize mismatch for diffusion_net.input_blocks.7.0.emb_layers.1.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([672]).\n\tsize mismatch for diffusion_net.input_blocks.7.0.out_layers.0.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([672]).\n\tsize mismatch for diffusion_net.input_blocks.7.0.out_layers.0.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([672]).\n\tsize mismatch for diffusion_net.input_blocks.7.0.out_layers.3.weight: copying a param with shape torch.Size([768, 768, 3, 3, 3]) from checkpoint, the shape in current model is torch.Size([672, 672, 3, 3, 3]).\n\tsize mismatch for diffusion_net.input_blocks.7.0.out_layers.3.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([672]).\n\tsize mismatch for diffusion_net.input_blocks.7.0.skip_connection.weight: copying a param with shape torch.Size([768, 384, 1, 1, 1]) from checkpoint, the shape in current model is torch.Size([672, 448, 1, 1, 1]).\n\tsize mismatch for diffusion_net.input_blocks.7.0.skip_connection.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([672]).\n\tsize mismatch for diffusion_net.input_blocks.7.1.norm.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([672]).\n\tsize mismatch for diffusion_net.input_blocks.7.1.norm.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([672]).\n\tsize mismatch for diffusion_net.input_blocks.7.1.proj_out.weight: copying a param with shape torch.Size([768, 768, 1]) from checkpoint, the shape in current model is torch.Size([672, 672, 1, 1, 1]).\n\tsize mismatch for diffusion_net.input_blocks.7.1.proj_out.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([672]).\n\tsize mismatch for diffusion_net.input_blocks.8.0.in_layers.0.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([672]).\n\tsize mismatch for diffusion_net.input_blocks.8.0.in_layers.0.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([672]).\n\tsize mismatch for diffusion_net.input_blocks.8.0.in_layers.2.weight: copying a param with shape torch.Size([768, 768, 3, 3, 3]) from checkpoint, the shape in current model is torch.Size([672, 672, 3, 3, 3]).\n\tsize mismatch for diffusion_net.input_blocks.8.0.in_layers.2.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([672]).\n\tsize mismatch for diffusion_net.input_blocks.8.0.emb_layers.1.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([672, 896]).\n\tsize mismatch for diffusion_net.input_blocks.8.0.emb_layers.1.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([672]).\n\tsize mismatch for diffusion_net.input_blocks.8.0.out_layers.0.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([672]).\n\tsize mismatch for diffusion_net.input_blocks.8.0.out_layers.0.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([672]).\n\tsize mismatch for diffusion_net.input_blocks.8.0.out_layers.3.weight: copying a param with shape torch.Size([768, 768, 3, 3, 3]) from checkpoint, the shape in current model is torch.Size([672, 672, 3, 3, 3]).\n\tsize mismatch for diffusion_net.input_blocks.8.0.out_layers.3.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([672]).\n\tsize mismatch for diffusion_net.input_blocks.8.1.norm.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([672]).\n\tsize mismatch for diffusion_net.input_blocks.8.1.norm.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([672]).\n\tsize mismatch for diffusion_net.input_blocks.8.1.proj_out.weight: copying a param with shape torch.Size([768, 768, 1]) from checkpoint, the shape in current model is torch.Size([672, 672, 1, 1, 1]).\n\tsize mismatch for diffusion_net.input_blocks.8.1.proj_out.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([672]).\n\tsize mismatch for diffusion_net.middle_block.0.in_layers.0.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([672]).\n\tsize mismatch for diffusion_net.middle_block.0.in_layers.0.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([672]).\n\tsize mismatch for diffusion_net.middle_block.0.in_layers.2.weight: copying a param with shape torch.Size([768, 768, 3, 3, 3]) from checkpoint, the shape in current model is torch.Size([672, 672, 3, 3, 3]).\n\tsize mismatch for diffusion_net.middle_block.0.in_layers.2.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([672]).\n\tsize mismatch for diffusion_net.middle_block.0.emb_layers.1.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([672, 896]).\n\tsize mismatch for diffusion_net.middle_block.0.emb_layers.1.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([672]).\n\tsize mismatch for diffusion_net.middle_block.0.out_layers.0.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([672]).\n\tsize mismatch for diffusion_net.middle_block.0.out_layers.0.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([672]).\n\tsize mismatch for diffusion_net.middle_block.0.out_layers.3.weight: copying a param with shape torch.Size([768, 768, 3, 3, 3]) from checkpoint, the shape in current model is torch.Size([672, 672, 3, 3, 3]).\n\tsize mismatch for diffusion_net.middle_block.0.out_layers.3.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([672]).\n\tsize mismatch for diffusion_net.middle_block.1.norm.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([672]).\n\tsize mismatch for diffusion_net.middle_block.1.norm.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([672]).\n\tsize mismatch for diffusion_net.middle_block.1.proj_out.weight: copying a param with shape torch.Size([768, 768, 1]) from checkpoint, the shape in current model is torch.Size([672, 672, 1, 1, 1]).\n\tsize mismatch for diffusion_net.middle_block.1.proj_out.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([672]).\n\tsize mismatch for diffusion_net.middle_block.2.in_layers.0.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([672]).\n\tsize mismatch for diffusion_net.middle_block.2.in_layers.0.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([672]).\n\tsize mismatch for diffusion_net.middle_block.2.in_layers.2.weight: copying a param with shape torch.Size([768, 768, 3, 3, 3]) from checkpoint, the shape in current model is torch.Size([672, 672, 3, 3, 3]).\n\tsize mismatch for diffusion_net.middle_block.2.in_layers.2.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([672]).\n\tsize mismatch for diffusion_net.middle_block.2.emb_layers.1.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([672, 896]).\n\tsize mismatch for diffusion_net.middle_block.2.emb_layers.1.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([672]).\n\tsize mismatch for diffusion_net.middle_block.2.out_layers.0.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([672]).\n\tsize mismatch for diffusion_net.middle_block.2.out_layers.0.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([672]).\n\tsize mismatch for diffusion_net.middle_block.2.out_layers.3.weight: copying a param with shape torch.Size([768, 768, 3, 3, 3]) from checkpoint, the shape in current model is torch.Size([672, 672, 3, 3, 3]).\n\tsize mismatch for diffusion_net.middle_block.2.out_layers.3.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([672]).\n\tsize mismatch for diffusion_net.output_blocks.0.0.in_layers.0.weight: copying a param with shape torch.Size([1536]) from checkpoint, the shape in current model is torch.Size([1344]).\n\tsize mismatch for diffusion_net.output_blocks.0.0.in_layers.0.bias: copying a param with shape torch.Size([1536]) from checkpoint, the shape in current model is torch.Size([1344]).\n\tsize mismatch for diffusion_net.output_blocks.0.0.in_layers.2.weight: copying a param with shape torch.Size([768, 1536, 3, 3, 3]) from checkpoint, the shape in current model is torch.Size([672, 1344, 3, 3, 3]).\n\tsize mismatch for diffusion_net.output_blocks.0.0.in_layers.2.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([672]).\n\tsize mismatch for diffusion_net.output_blocks.0.0.emb_layers.1.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([672, 896]).\n\tsize mismatch for diffusion_net.output_blocks.0.0.emb_layers.1.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([672]).\n\tsize mismatch for diffusion_net.output_blocks.0.0.out_layers.0.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([672]).\n\tsize mismatch for diffusion_net.output_blocks.0.0.out_layers.0.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([672]).\n\tsize mismatch for diffusion_net.output_blocks.0.0.out_layers.3.weight: copying a param with shape torch.Size([768, 768, 3, 3, 3]) from checkpoint, the shape in current model is torch.Size([672, 672, 3, 3, 3]).\n\tsize mismatch for diffusion_net.output_blocks.0.0.out_layers.3.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([672]).\n\tsize mismatch for diffusion_net.output_blocks.0.0.skip_connection.weight: copying a param with shape torch.Size([768, 1536, 1, 1, 1]) from checkpoint, the shape in current model is torch.Size([672, 1344, 1, 1, 1]).\n\tsize mismatch for diffusion_net.output_blocks.0.0.skip_connection.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([672]).\n\tsize mismatch for diffusion_net.output_blocks.1.0.in_layers.0.weight: copying a param with shape torch.Size([1536]) from checkpoint, the shape in current model is torch.Size([1344]).\n\tsize mismatch for diffusion_net.output_blocks.1.0.in_layers.0.bias: copying a param with shape torch.Size([1536]) from checkpoint, the shape in current model is torch.Size([1344]).\n\tsize mismatch for diffusion_net.output_blocks.1.0.in_layers.2.weight: copying a param with shape torch.Size([768, 1536, 3, 3, 3]) from checkpoint, the shape in current model is torch.Size([672, 1344, 3, 3, 3]).\n\tsize mismatch for diffusion_net.output_blocks.1.0.in_layers.2.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([672]).\n\tsize mismatch for diffusion_net.output_blocks.1.0.emb_layers.1.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([672, 896]).\n\tsize mismatch for diffusion_net.output_blocks.1.0.emb_layers.1.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([672]).\n\tsize mismatch for diffusion_net.output_blocks.1.0.out_layers.0.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([672]).\n\tsize mismatch for diffusion_net.output_blocks.1.0.out_layers.0.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([672]).\n\tsize mismatch for diffusion_net.output_blocks.1.0.out_layers.3.weight: copying a param with shape torch.Size([768, 768, 3, 3, 3]) from checkpoint, the shape in current model is torch.Size([672, 672, 3, 3, 3]).\n\tsize mismatch for diffusion_net.output_blocks.1.0.out_layers.3.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([672]).\n\tsize mismatch for diffusion_net.output_blocks.1.0.skip_connection.weight: copying a param with shape torch.Size([768, 1536, 1, 1, 1]) from checkpoint, the shape in current model is torch.Size([672, 1344, 1, 1, 1]).\n\tsize mismatch for diffusion_net.output_blocks.1.0.skip_connection.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([672]).\n\tsize mismatch for diffusion_net.output_blocks.2.0.in_layers.0.weight: copying a param with shape torch.Size([1536]) from checkpoint, the shape in current model is torch.Size([1120]).\n\tsize mismatch for diffusion_net.output_blocks.2.0.in_layers.0.bias: copying a param with shape torch.Size([1536]) from checkpoint, the shape in current model is torch.Size([1120]).\n\tsize mismatch for diffusion_net.output_blocks.2.0.in_layers.2.weight: copying a param with shape torch.Size([768, 1536, 3, 3, 3]) from checkpoint, the shape in current model is torch.Size([672, 1120, 3, 3, 3]).\n\tsize mismatch for diffusion_net.output_blocks.2.0.in_layers.2.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([672]).\n\tsize mismatch for diffusion_net.output_blocks.2.0.emb_layers.1.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([672, 896]).\n\tsize mismatch for diffusion_net.output_blocks.2.0.emb_layers.1.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([672]).\n\tsize mismatch for diffusion_net.output_blocks.2.0.out_layers.0.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([672]).\n\tsize mismatch for diffusion_net.output_blocks.2.0.out_layers.0.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([672]).\n\tsize mismatch for diffusion_net.output_blocks.2.0.out_layers.3.weight: copying a param with shape torch.Size([768, 768, 3, 3, 3]) from checkpoint, the shape in current model is torch.Size([672, 672, 3, 3, 3]).\n\tsize mismatch for diffusion_net.output_blocks.2.0.out_layers.3.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([672]).\n\tsize mismatch for diffusion_net.output_blocks.2.0.skip_connection.weight: copying a param with shape torch.Size([768, 1536, 1, 1, 1]) from checkpoint, the shape in current model is torch.Size([672, 1120, 1, 1, 1]).\n\tsize mismatch for diffusion_net.output_blocks.2.0.skip_connection.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([672]).\n\tsize mismatch for diffusion_net.output_blocks.3.0.in_layers.0.weight: copying a param with shape torch.Size([1536]) from checkpoint, the shape in current model is torch.Size([1120]).\n\tsize mismatch for diffusion_net.output_blocks.3.0.in_layers.0.bias: copying a param with shape torch.Size([1536]) from checkpoint, the shape in current model is torch.Size([1120]).\n\tsize mismatch for diffusion_net.output_blocks.3.0.in_layers.2.weight: copying a param with shape torch.Size([768, 1536, 3, 3, 3]) from checkpoint, the shape in current model is torch.Size([448, 1120, 3, 3, 3]).\n\tsize mismatch for diffusion_net.output_blocks.3.0.in_layers.2.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([448]).\n\tsize mismatch for diffusion_net.output_blocks.3.0.emb_layers.1.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([448, 896]).\n\tsize mismatch for diffusion_net.output_blocks.3.0.emb_layers.1.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([448]).\n\tsize mismatch for diffusion_net.output_blocks.3.0.out_layers.0.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([448]).\n\tsize mismatch for diffusion_net.output_blocks.3.0.out_layers.0.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([448]).\n\tsize mismatch for diffusion_net.output_blocks.3.0.out_layers.3.weight: copying a param with shape torch.Size([768, 768, 3, 3, 3]) from checkpoint, the shape in current model is torch.Size([448, 448, 3, 3, 3]).\n\tsize mismatch for diffusion_net.output_blocks.3.0.out_layers.3.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([448]).\n\tsize mismatch for diffusion_net.output_blocks.3.0.skip_connection.weight: copying a param with shape torch.Size([768, 1536, 1, 1, 1]) from checkpoint, the shape in current model is torch.Size([448, 1120, 1, 1, 1]).\n\tsize mismatch for diffusion_net.output_blocks.3.0.skip_connection.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([448]).\n\tsize mismatch for diffusion_net.output_blocks.3.1.norm.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([448]).\n\tsize mismatch for diffusion_net.output_blocks.3.1.norm.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([448]).\n\tsize mismatch for diffusion_net.output_blocks.3.1.proj_out.weight: copying a param with shape torch.Size([768, 768, 1]) from checkpoint, the shape in current model is torch.Size([448, 448, 1, 1, 1]).\n\tsize mismatch for diffusion_net.output_blocks.3.1.proj_out.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([448]).\n\tsize mismatch for diffusion_net.output_blocks.4.0.in_layers.0.weight: copying a param with shape torch.Size([1536]) from checkpoint, the shape in current model is torch.Size([896]).\n\tsize mismatch for diffusion_net.output_blocks.4.0.in_layers.0.bias: copying a param with shape torch.Size([1536]) from checkpoint, the shape in current model is torch.Size([896]).\n\tsize mismatch for diffusion_net.output_blocks.4.0.in_layers.2.weight: copying a param with shape torch.Size([768, 1536, 3, 3, 3]) from checkpoint, the shape in current model is torch.Size([448, 896, 3, 3, 3]).\n\tsize mismatch for diffusion_net.output_blocks.4.0.in_layers.2.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([448]).\n\tsize mismatch for diffusion_net.output_blocks.4.0.emb_layers.1.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([448, 896]).\n\tsize mismatch for diffusion_net.output_blocks.4.0.emb_layers.1.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([448]).\n\tsize mismatch for diffusion_net.output_blocks.4.0.out_layers.0.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([448]).\n\tsize mismatch for diffusion_net.output_blocks.4.0.out_layers.0.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([448]).\n\tsize mismatch for diffusion_net.output_blocks.4.0.out_layers.3.weight: copying a param with shape torch.Size([768, 768, 3, 3, 3]) from checkpoint, the shape in current model is torch.Size([448, 448, 3, 3, 3]).\n\tsize mismatch for diffusion_net.output_blocks.4.0.out_layers.3.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([448]).\n\tsize mismatch for diffusion_net.output_blocks.4.0.skip_connection.weight: copying a param with shape torch.Size([768, 1536, 1, 1, 1]) from checkpoint, the shape in current model is torch.Size([448, 896, 1, 1, 1]).\n\tsize mismatch for diffusion_net.output_blocks.4.0.skip_connection.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([448]).\n\tsize mismatch for diffusion_net.output_blocks.4.1.norm.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([448]).\n\tsize mismatch for diffusion_net.output_blocks.4.1.norm.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([448]).\n\tsize mismatch for diffusion_net.output_blocks.4.1.proj_out.weight: copying a param with shape torch.Size([768, 768, 1]) from checkpoint, the shape in current model is torch.Size([448, 448, 1, 1, 1]).\n\tsize mismatch for diffusion_net.output_blocks.4.1.proj_out.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([448]).\n\tsize mismatch for diffusion_net.output_blocks.5.0.in_layers.0.weight: copying a param with shape torch.Size([1152]) from checkpoint, the shape in current model is torch.Size([672]).\n\tsize mismatch for diffusion_net.output_blocks.5.0.in_layers.0.bias: copying a param with shape torch.Size([1152]) from checkpoint, the shape in current model is torch.Size([672]).\n\tsize mismatch for diffusion_net.output_blocks.5.0.in_layers.2.weight: copying a param with shape torch.Size([768, 1152, 3, 3, 3]) from checkpoint, the shape in current model is torch.Size([448, 672, 3, 3, 3]).\n\tsize mismatch for diffusion_net.output_blocks.5.0.in_layers.2.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([448]).\n\tsize mismatch for diffusion_net.output_blocks.5.0.emb_layers.1.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([448, 896]).\n\tsize mismatch for diffusion_net.output_blocks.5.0.emb_layers.1.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([448]).\n\tsize mismatch for diffusion_net.output_blocks.5.0.out_layers.0.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([448]).\n\tsize mismatch for diffusion_net.output_blocks.5.0.out_layers.0.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([448]).\n\tsize mismatch for diffusion_net.output_blocks.5.0.out_layers.3.weight: copying a param with shape torch.Size([768, 768, 3, 3, 3]) from checkpoint, the shape in current model is torch.Size([448, 448, 3, 3, 3]).\n\tsize mismatch for diffusion_net.output_blocks.5.0.out_layers.3.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([448]).\n\tsize mismatch for diffusion_net.output_blocks.5.0.skip_connection.weight: copying a param with shape torch.Size([768, 1152, 1, 1, 1]) from checkpoint, the shape in current model is torch.Size([448, 672, 1, 1, 1]).\n\tsize mismatch for diffusion_net.output_blocks.5.0.skip_connection.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([448]).\n\tsize mismatch for diffusion_net.output_blocks.5.1.norm.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([448]).\n\tsize mismatch for diffusion_net.output_blocks.5.1.norm.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([448]).\n\tsize mismatch for diffusion_net.output_blocks.5.1.proj_out.weight: copying a param with shape torch.Size([768, 768, 1]) from checkpoint, the shape in current model is torch.Size([448, 448, 1, 1, 1]).\n\tsize mismatch for diffusion_net.output_blocks.5.1.proj_out.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([448]).\n\tsize mismatch for diffusion_net.output_blocks.5.2.conv.weight: copying a param with shape torch.Size([768, 768, 3, 3, 3]) from checkpoint, the shape in current model is torch.Size([448, 448, 3, 3, 3]).\n\tsize mismatch for diffusion_net.output_blocks.5.2.conv.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([448]).\n\tsize mismatch for diffusion_net.output_blocks.6.0.in_layers.0.weight: copying a param with shape torch.Size([1152]) from checkpoint, the shape in current model is torch.Size([672]).\n\tsize mismatch for diffusion_net.output_blocks.6.0.in_layers.0.bias: copying a param with shape torch.Size([1152]) from checkpoint, the shape in current model is torch.Size([672]).\n\tsize mismatch for diffusion_net.output_blocks.6.0.in_layers.2.weight: copying a param with shape torch.Size([384, 1152, 3, 3, 3]) from checkpoint, the shape in current model is torch.Size([224, 672, 3, 3, 3]).\n\tsize mismatch for diffusion_net.output_blocks.6.0.in_layers.2.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([224]).\n\tsize mismatch for diffusion_net.output_blocks.6.0.emb_layers.1.weight: copying a param with shape torch.Size([384, 768]) from checkpoint, the shape in current model is torch.Size([224, 896]).\n\tsize mismatch for diffusion_net.output_blocks.6.0.emb_layers.1.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([224]).\n\tsize mismatch for diffusion_net.output_blocks.6.0.out_layers.0.weight: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([224]).\n\tsize mismatch for diffusion_net.output_blocks.6.0.out_layers.0.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([224]).\n\tsize mismatch for diffusion_net.output_blocks.6.0.out_layers.3.weight: copying a param with shape torch.Size([384, 384, 3, 3, 3]) from checkpoint, the shape in current model is torch.Size([224, 224, 3, 3, 3]).\n\tsize mismatch for diffusion_net.output_blocks.6.0.out_layers.3.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([224]).\n\tsize mismatch for diffusion_net.output_blocks.6.0.skip_connection.weight: copying a param with shape torch.Size([384, 1152, 1, 1, 1]) from checkpoint, the shape in current model is torch.Size([224, 672, 1, 1, 1]).\n\tsize mismatch for diffusion_net.output_blocks.6.0.skip_connection.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([224]).\n\tsize mismatch for diffusion_net.output_blocks.7.0.in_layers.0.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([448]).\n\tsize mismatch for diffusion_net.output_blocks.7.0.in_layers.0.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([448]).\n\tsize mismatch for diffusion_net.output_blocks.7.0.in_layers.2.weight: copying a param with shape torch.Size([384, 768, 3, 3, 3]) from checkpoint, the shape in current model is torch.Size([224, 448, 3, 3, 3]).\n\tsize mismatch for diffusion_net.output_blocks.7.0.in_layers.2.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([224]).\n\tsize mismatch for diffusion_net.output_blocks.7.0.emb_layers.1.weight: copying a param with shape torch.Size([384, 768]) from checkpoint, the shape in current model is torch.Size([224, 896]).\n\tsize mismatch for diffusion_net.output_blocks.7.0.emb_layers.1.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([224]).\n\tsize mismatch for diffusion_net.output_blocks.7.0.out_layers.0.weight: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([224]).\n\tsize mismatch for diffusion_net.output_blocks.7.0.out_layers.0.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([224]).\n\tsize mismatch for diffusion_net.output_blocks.7.0.out_layers.3.weight: copying a param with shape torch.Size([384, 384, 3, 3, 3]) from checkpoint, the shape in current model is torch.Size([224, 224, 3, 3, 3]).\n\tsize mismatch for diffusion_net.output_blocks.7.0.out_layers.3.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([224]).\n\tsize mismatch for diffusion_net.output_blocks.7.0.skip_connection.weight: copying a param with shape torch.Size([384, 768, 1, 1, 1]) from checkpoint, the shape in current model is torch.Size([224, 448, 1, 1, 1]).\n\tsize mismatch for diffusion_net.output_blocks.7.0.skip_connection.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([224]).\n\tsize mismatch for diffusion_net.output_blocks.8.0.in_layers.0.weight: copying a param with shape torch.Size([576]) from checkpoint, the shape in current model is torch.Size([448]).\n\tsize mismatch for diffusion_net.output_blocks.8.0.in_layers.0.bias: copying a param with shape torch.Size([576]) from checkpoint, the shape in current model is torch.Size([448]).\n\tsize mismatch for diffusion_net.output_blocks.8.0.in_layers.2.weight: copying a param with shape torch.Size([384, 576, 3, 3, 3]) from checkpoint, the shape in current model is torch.Size([224, 448, 3, 3, 3]).\n\tsize mismatch for diffusion_net.output_blocks.8.0.in_layers.2.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([224]).\n\tsize mismatch for diffusion_net.output_blocks.8.0.emb_layers.1.weight: copying a param with shape torch.Size([384, 768]) from checkpoint, the shape in current model is torch.Size([224, 896]).\n\tsize mismatch for diffusion_net.output_blocks.8.0.emb_layers.1.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([224]).\n\tsize mismatch for diffusion_net.output_blocks.8.0.out_layers.0.weight: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([224]).\n\tsize mismatch for diffusion_net.output_blocks.8.0.out_layers.0.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([224]).\n\tsize mismatch for diffusion_net.output_blocks.8.0.out_layers.3.weight: copying a param with shape torch.Size([384, 384, 3, 3, 3]) from checkpoint, the shape in current model is torch.Size([224, 224, 3, 3, 3]).\n\tsize mismatch for diffusion_net.output_blocks.8.0.out_layers.3.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([224]).\n\tsize mismatch for diffusion_net.output_blocks.8.0.skip_connection.weight: copying a param with shape torch.Size([384, 576, 1, 1, 1]) from checkpoint, the shape in current model is torch.Size([224, 448, 1, 1, 1]).\n\tsize mismatch for diffusion_net.output_blocks.8.0.skip_connection.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([224]).\n\tsize mismatch for diffusion_net.out.0.weight: copying a param with shape torch.Size([192]) from checkpoint, the shape in current model is torch.Size([224]).\n\tsize mismatch for diffusion_net.out.0.bias: copying a param with shape torch.Size([192]) from checkpoint, the shape in current model is torch.Size([224]).\n\tsize mismatch for diffusion_net.out.2.weight: copying a param with shape torch.Size([3, 192, 3, 3, 3]) from checkpoint, the shape in current model is torch.Size([3, 224, 3, 3, 3]).",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-21-92e181132ad7>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit_model_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mckpt_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mckpt_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mSDFusion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mcprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'[*] \"{SDFusion.name()}\" loaded.'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'cyan'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/SDFusion/models/base_model.py\u001b[0m in \u001b[0;36mcreate_model\u001b[0;34m(opt)\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Model [%s] not recognized.\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minitialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m     \u001b[0mcprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"[*] Model has been created: %s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'blue'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/SDFusion/models/sdfusion_txt2shape_model.py\u001b[0m in \u001b[0;36minitialize\u001b[0;34m(self, opt)\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mckpt\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 112\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_ckpt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mckpt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mload_opt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misTrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[0;31m# transforms\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/SDFusion/models/sdfusion_txt2shape_model.py\u001b[0m in \u001b[0;36mload_ckpt\u001b[0;34m(self, ckpt, load_opt)\u001b[0m\n\u001b[1;32m    570\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    571\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvqvae\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'vqvae'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 572\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'df'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    573\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcond_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'cond_model'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    574\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolored\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'[*] weight successfully load from: %s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mckpt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'blue'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mload_state_dict\u001b[0;34m(self, state_dict, strict, assign)\u001b[0m\n\u001b[1;32m   2579\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2580\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_msgs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2581\u001b[0;31m             raise RuntimeError(\n\u001b[0m\u001b[1;32m   2582\u001b[0m                 \"Error(s) in loading state_dict for {}:\\n\\t{}\".format(\n\u001b[1;32m   2583\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"\\n\\t\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_msgs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for DiffusionUNet:\n\tMissing key(s) in state_dict: \"diffusion_net.input_blocks.4.1.proj_in.weight\", \"diffusion_net.input_blocks.4.1.proj_in.bias\", \"diffusion_net.input_blocks.4.1.transformer_blocks.0.attn1.to_q.weight\", \"diffusion_net.input_blocks.4.1.transformer_blocks.0.attn1.to_k.weight\", \"diffusion_net.input_blocks.4.1.transformer_blocks.0.attn1.to_v.weight\", \"diffusion_net.input_blocks.4.1.transformer_blocks.0.attn1.to_out.0.weight\", \"diffusion_net.input_blocks.4.1.transformer_blocks.0.attn1.to_out.0.bias\", \"diffusion_net.input_blocks.4.1.transformer_blocks.0.ff.net.0.proj.weight\", \"diffusion_net.input_blocks.4.1.transformer_blocks.0.ff.net.0.proj.bias\", \"diffusion_net.input_blocks.4.1.transformer_blocks.0.ff.net.2.weight\", \"diffusion_net.input_blocks.4.1.transformer_blocks.0.ff.net.2.bias\", \"diffusion_net.input_blocks.4.1.transformer_blocks.0.attn2.to_q.weight\", \"diffusion_net.input_blocks.4.1.transformer_blocks.0.attn2.to_k.weight\", \"diffusion_net.input_blocks.4.1.transformer_blocks.0.attn2.to_v.weight\", \"diffusion_net.input_blocks.4.1.transformer_blocks.0.attn2.to_out.0.weight\", \"diffusion_net.input_blocks.4.1.transformer_blocks.0.attn2.to_out.0.bias\", \"diffusion_net.input_blocks.4.1.transformer_blocks.0.norm1.weight\", \"diffusion_net.input_blocks.4.1.transformer_blocks.0.norm1.bias\", \"diffusion_net.input_blocks.4.1.transformer_blocks.0.norm2.weight\", \"diffusion_net.input_blocks.4.1.transformer_blocks.0.norm2.bias\", \"diffusion_net.input_blocks.4.1.transformer_blocks.0.norm3.weight\", \"d...\n\tUnexpected key(s) in state_dict: \"diffusion_net.input_blocks.9.0.op.weight\", \"diffusion_net.input_blocks.9.0.op.bias\", \"diffusion_net.input_blocks.10.0.in_layers.0.weight\", \"diffusion_net.input_blocks.10.0.in_layers.0.bias\", \"diffusion_net.input_blocks.10.0.in_layers.2.weight\", \"diffusion_net.input_blocks.10.0.in_layers.2.bias\", \"diffusion_net.input_blocks.10.0.emb_layers.1.weight\", \"diffusion_net.input_blocks.10.0.emb_layers.1.bias\", \"diffusion_net.input_blocks.10.0.out_layers.0.weight\", \"diffusion_net.input_blocks.10.0.out_layers.0.bias\", \"diffusion_net.input_blocks.10.0.out_layers.3.weight\", \"diffusion_net.input_blocks.10.0.out_layers.3.bias\", \"diffusion_net.input_blocks.11.0.in_layers.0.weight\", \"diffusion_net.input_blocks.11.0.in_layers.0.bias\", \"diffusion_net.input_blocks.11.0.in_layers.2.weight\", \"diffusion_net.input_blocks.11.0.in_layers.2.bias\", \"diffusion_net.input_blocks.11.0.emb_layers.1.weight\", \"diffusion_net.input_blocks.11.0.emb_layers.1.bias\", \"diffusion_net.input_blocks.11.0.out_layers.0.weight\", \"diffusion_net.input_blocks.11.0.out_layers.0.bias\", \"diffusion_net.input_blocks.11.0.out_layers.3.weight\", \"diffusion_net.input_blocks.11.0.out_layers.3.bias\", \"diffusion_net.input_blocks.1.1.norm.weight\", \"diffusion_net.input_blocks.1.1.norm.bias\", \"diffusion_net.input_blocks.1.1.qkv.weight\", \"diffusion_net.input_blocks.1.1.qkv.bias\", \"diffusion_net.input_blocks.1.1.proj_out.weight\", \"diffusion_net.input_blocks.1.1.proj_out.bias\", \"diffusion_net.input_blocks.2...\n\tsize mismatch for diffusion_net.time_embed.0.weight: copying a param with shape torch.Size([768, 192]) from checkpoint, the shape in current model is torch.Size([896, 224]).\n\tsize mismatch for diffusion_net.time_embed.0.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([896]).\n\tsize mismatch for diffusion_net.time_embed.2.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([896, 896]).\n\tsize mismatch for diffusion_net.time_embed.2.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([896]).\n\tsize mismatch for diffusion_net.input_blocks.0.0.weight: copying a param with shape torch.Size([192, 3, 3, 3, 3]) from checkpoint, the shape in current model is torch.Size([224, 3, 3, 3, 3]).\n\tsize mismatch for diffusion_net.input_blocks.0.0.bias: copying a param with shape torch.Size([192]) from checkpoint, the shape in current model is torch.Size([224]).\n\tsize mismatch for diffusion_net.input_blocks.1.0.in_layers.0.weight: copying a param with shape torch.Size([192]) from checkpoint, the shape in current model is torch.Size([224]).\n\tsize mismatch for diffusion_net.input_blocks.1.0.in_layers.0.bias: copying a param with shape torch.Size([192]) from checkpoint, the shape in current model is torch.Size([224]).\n\tsize mismatch for diffusion_net.input_blocks.1.0.in_layers.2.weight: copying a param with shape torch.Size([192, 192, 3, 3, 3]) from checkpoint, the shape in current model is torch.Size([224, 224, 3, 3, 3]).\n\tsize mismatch for diffusion_net.input_blocks.1.0.in_layers.2.bias: copying a param with shape torch.Size([192]) from checkpoint, the shape in current model is torch.Size([224]).\n\tsize mismatch for diffusion_net.input_blocks.1.0.emb_layers.1.weight: copying a param with shape torch.Size([192, 768]) from checkpoint, the shape in current model is torch.Size([224, 896]).\n\tsize mismatch for diffusion_net.input_blocks.1.0.emb_layers.1.bias: copying a param with shape torch.Size([192]) from checkpoint, the shape in current model is torch.Size([224]).\n\tsize mismatch for diffusion_net.input_blocks.1.0.out_layers.0.weight: copying a param with shape torch.Size([192]) from checkpoint, the shape in current model is torch.Size([224]).\n\tsize mismatch for diffusion_net.input_blocks.1.0.out_layers.0.bias: copying a param with shape torch.Size([192]) from checkpoint, the shape in current model is torch.Size([224]).\n\tsize mismatch for diffusion_net.input_blocks.1.0.out_layers.3.weight: copying a param with shape torch.Size([192, 192, 3, 3, 3]) from checkpoint, the shape in current model is torch.Size([224, 224, 3, 3, 3]).\n\tsize mismatch for diffusion_net.input_blocks.1.0.out_layers.3.bias: copying a param with shape torch.Size([192]) from checkpoint, the shape in current model is torch.Size([224]).\n\tsize mismatch for diffusion_net.input_blocks.2.0.in_layers.0.weight: copying a param with shape torch.Size([192]) from checkpoint, the shape in current model is torch.Size([224]).\n\tsize mismatch for diffusion_net.input_blocks.2.0.in_layers.0.bias: copying a param with shape torch.Size([192]) from checkpoint, the shape in current model is torch.Size([224]).\n\tsize mismatch for diffusion_net.input_blocks.2.0.in_layers.2.weight: copying a param with shape torch.Size([192, 192, 3, 3, 3]) from checkpoint, the shape in current model is torch.Size([224, 224, 3, 3, 3]).\n\tsize mismatch for diffusion_net.input_blocks.2.0.in_layers.2.bias: copying a param with shape torch.Size([192]) from checkpoint, the shape in current model is torch.Size([224]).\n\tsize mismatch for diffusion_net.input_blocks.2.0.emb_layers.1.weight: copying a param with shape torch.Size([192, 768]) from checkpoint, the shape in current model is torch.Size([224, 896]).\n\tsize mismatch for diffusion_net.input_blocks.2.0.emb_layers.1.bias: copying a param with shape torch.Size([192]) from checkpoint, the shape in current model is torch.Size([224]).\n\tsize mismatch for diffusion_net.input_blocks.2.0.out_layers.0.weight: copying a param with shape torch.Size([192]) from checkpoint, the shape in current model is torch.Size([224]).\n\tsize mismatch for diffusion_net.input_blocks.2.0.out_layers.0.bias: copying a param with shape torch.Size([192]) from checkpoint, the shape in current model is torch.Size([224]).\n\tsize mismatch for diffusion_net.input_blocks.2.0.out_layers.3.weight: copying a param with shape torch.Size([192, 192, 3, 3, 3]) from checkpoint, the shape in current model is torch.Size([224, 224, 3, 3, 3]).\n\tsize mismatch for diffusion_net.input_blocks.2.0.out_layers.3.bias: copying a param with shape torch.Size([192]) from checkpoint, the shape in current model is torch.Size([224]).\n\tsize mismatch for diffusion_net.input_blocks.3.0.op.weight: copying a param with shape torch.Size([192, 192, 3, 3, 3]) from checkpoint, the shape in current model is torch.Size([224, 224, 3, 3, 3]).\n\tsize mismatch for diffusion_net.input_blocks.3.0.op.bias: copying a param with shape torch.Size([192]) from checkpoint, the shape in current model is torch.Size([224]).\n\tsize mismatch for diffusion_net.input_blocks.4.0.in_layers.0.weight: copying a param with shape torch.Size([192]) from checkpoint, the shape in current model is torch.Size([224]).\n\tsize mismatch for diffusion_net.input_blocks.4.0.in_layers.0.bias: copying a param with shape torch.Size([192]) from checkpoint, the shape in current model is torch.Size([224]).\n\tsize mismatch for diffusion_net.input_blocks.4.0.in_layers.2.weight: copying a param with shape torch.Size([384, 192, 3, 3, 3]) from checkpoint, the shape in current model is torch.Size([448, 224, 3, 3, 3]).\n\tsize mismatch for diffusion_net.input_blocks.4.0.in_layers.2.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([448]).\n\tsize mismatch for diffusion_net.input_blocks.4.0.emb_layers.1.weight: copying a param with shape torch.Size([384, 768]) from checkpoint, the shape in current model is torch.Size([448, 896]).\n\tsize mismatch for diffusion_net.input_blocks.4.0.emb_layers.1.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([448]).\n\tsize mismatch for diffusion_net.input_blocks.4.0.out_layers.0.weight: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([448]).\n\tsize mismatch for diffusion_net.input_blocks.4.0.out_layers.0.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([448]).\n\tsize mismatch for diffusion_net.input_blocks.4.0.out_layers.3.weight: copying a param with shape torch.Size([384, 384, 3, 3, 3]) from checkpoint, the shape in current model is torch.Size([448, 448, 3, 3, 3]).\n\tsize mismatch for diffusion_net.input_blocks.4.0.out_layers.3.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([448]).\n\tsize mismatch for diffusion_net.input_blocks.4.0.skip_connection.weight: copying a param with shape torch.Size([384, 192, 1, 1, 1]) from checkpoint, the shape in current model is torch.Size([448, 224, 1, 1, 1]).\n\tsize mismatch for diffusion_net.input_blocks.4.0.skip_connection.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([448]).\n\tsize mismatch for diffusion_net.input_blocks.4.1.norm.weight: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([448]).\n\tsize mismatch for diffusion_net.input_blocks.4.1.norm.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([448]).\n\tsize mismatch for diffusion_net.input_blocks.4.1.proj_out.weight: copying a param with shape torch.Size([384, 384, 1]) from checkpoint, the shape in current model is torch.Size([448, 448, 1, 1, 1]).\n\tsize mismatch for diffusion_net.input_blocks.4.1.proj_out.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([448]).\n\tsize mismatch for diffusion_net.input_blocks.5.0.in_layers.0.weight: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([448]).\n\tsize mismatch for diffusion_net.input_blocks.5.0.in_layers.0.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([448]).\n\tsize mismatch for diffusion_net.input_blocks.5.0.in_layers.2.weight: copying a param with shape torch.Size([384, 384, 3, 3, 3]) from checkpoint, the shape in current model is torch.Size([448, 448, 3, 3, 3]).\n\tsize mismatch for diffusion_net.input_blocks.5.0.in_layers.2.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([448]).\n\tsize mismatch for diffusion_net.input_blocks.5.0.emb_layers.1.weight: copying a param with shape torch.Size([384, 768]) from checkpoint, the shape in current model is torch.Size([448, 896]).\n\tsize mismatch for diffusion_net.input_blocks.5.0.emb_layers.1.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([448]).\n\tsize mismatch for diffusion_net.input_blocks.5.0.out_layers.0.weight: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([448]).\n\tsize mismatch for diffusion_net.input_blocks.5.0.out_layers.0.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([448]).\n\tsize mismatch for diffusion_net.input_blocks.5.0.out_layers.3.weight: copying a param with shape torch.Size([384, 384, 3, 3, 3]) from checkpoint, the shape in current model is torch.Size([448, 448, 3, 3, 3]).\n\tsize mismatch for diffusion_net.input_blocks.5.0.out_layers.3.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([448]).\n\tsize mismatch for diffusion_net.input_blocks.5.1.norm.weight: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([448]).\n\tsize mismatch for diffusion_net.input_blocks.5.1.norm.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([448]).\n\tsize mismatch for diffusion_net.input_blocks.5.1.proj_out.weight: copying a param with shape torch.Size([384, 384, 1]) from checkpoint, the shape in current model is torch.Size([448, 448, 1, 1, 1]).\n\tsize mismatch for diffusion_net.input_blocks.5.1.proj_out.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([448]).\n\tsize mismatch for diffusion_net.input_blocks.6.0.op.weight: copying a param with shape torch.Size([384, 384, 3, 3, 3]) from checkpoint, the shape in current model is torch.Size([448, 448, 3, 3, 3]).\n\tsize mismatch for diffusion_net.input_blocks.6.0.op.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([448]).\n\tsize mismatch for diffusion_net.input_blocks.7.0.in_layers.0.weight: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([448]).\n\tsize mismatch for diffusion_net.input_blocks.7.0.in_layers.0.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([448]).\n\tsize mismatch for diffusion_net.input_blocks.7.0.in_layers.2.weight: copying a param with shape torch.Size([768, 384, 3, 3, 3]) from checkpoint, the shape in current model is torch.Size([672, 448, 3, 3, 3]).\n\tsize mismatch for diffusion_net.input_blocks.7.0.in_layers.2.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([672]).\n\tsize mismatch for diffusion_net.input_blocks.7.0.emb_layers.1.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([672, 896]).\n\tsize mismatch for diffusion_net.input_blocks.7.0.emb_layers.1.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([672]).\n\tsize mismatch for diffusion_net.input_blocks.7.0.out_layers.0.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([672]).\n\tsize mismatch for diffusion_net.input_blocks.7.0.out_layers.0.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([672]).\n\tsize mismatch for diffusion_net.input_blocks.7.0.out_layers.3.weight: copying a param with shape torch.Size([768, 768, 3, 3, 3]) from checkpoint, the shape in current model is torch.Size([672, 672, 3, 3, 3]).\n\tsize mismatch for diffusion_net.input_blocks.7.0.out_layers.3.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([672]).\n\tsize mismatch for diffusion_net.input_blocks.7.0.skip_connection.weight: copying a param with shape torch.Size([768, 384, 1, 1, 1]) from checkpoint, the shape in current model is torch.Size([672, 448, 1, 1, 1]).\n\tsize mismatch for diffusion_net.input_blocks.7.0.skip_connection.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([672]).\n\tsize mismatch for diffusion_net.input_blocks.7.1.norm.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([672]).\n\tsize mismatch for diffusion_net.input_blocks.7.1.norm.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([672]).\n\tsize mismatch for diffusion_net.input_blocks.7.1.proj_out.weight: copying a param with shape torch.Size([768, 768, 1]) from checkpoint, the shape in current model is torch.Size([672, 672, 1, 1, 1]).\n\tsize mismatch for diffusion_net.input_blocks.7.1.proj_out.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([672]).\n\tsize mismatch for diffusion_net.input_blocks.8.0.in_layers.0.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([672]).\n\tsize mismatch for diffusion_net.input_blocks.8.0.in_layers.0.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([672]).\n\tsize mismatch for diffusion_net.input_blocks.8.0.in_layers.2.weight: copying a param with shape torch.Size([768, 768, 3, 3, 3]) from checkpoint, the shape in current model is torch.Size([672, 672, 3, 3, 3]).\n\tsize mismatch for diffusion_net.input_blocks.8.0.in_layers.2.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([672]).\n\tsize mismatch for diffusion_net.input_blocks.8.0.emb_layers.1.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([672, 896]).\n\tsize mismatch for diffusion_net.input_blocks.8.0.emb_layers.1.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([672]).\n\tsize mismatch for diffusion_net.input_blocks.8.0.out_layers.0.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([672]).\n\tsize mismatch for diffusion_net.input_blocks.8.0.out_layers.0.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([672]).\n\tsize mismatch for diffusion_net.input_blocks.8.0.out_layers.3.weight: copying a param with shape torch.Size([768, 768, 3, 3, 3]) from checkpoint, the shape in current model is torch.Size([672, 672, 3, 3, 3]).\n\tsize mismatch for diffusion_net.input_blocks.8.0.out_layers.3.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([672]).\n\tsize mismatch for diffusion_net.input_blocks.8.1.norm.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([672]).\n\tsize mismatch for diffusion_net.input_blocks.8.1.norm.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([672]).\n\tsize mismatch for diffusion_net.input_blocks.8.1.proj_out.weight: copying a param with shape torch.Size([768, 768, 1]) from checkpoint, the shape in current model is torch.Size([672, 672, 1, 1, 1]).\n\tsize mismatch for diffusion_net.input_blocks.8.1.proj_out.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([672]).\n\tsize mismatch for diffusion_net.middle_block.0.in_layers.0.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([672]).\n\tsize mismatch for diffusion_net.middle_block.0.in_layers.0.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([672]).\n\tsize mismatch for diffusion_net.middle_block.0.in_layers.2.weight: copying a param with shape torch.Size([768, 768, 3, 3, 3]) from checkpoint, the shape in current model is torch.Size([672, 672, 3, 3, 3]).\n\tsize mismatch for diffusion_net.middle_block.0.in_layers.2.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([672]).\n\tsize mismatch for diffusion_net.middle_block.0.emb_layers.1.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([672, 896]).\n\tsize mismatch for diffusion_net.middle_block.0.emb_layers.1.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([672]).\n\tsize mismatch for diffusion_net.middle_block.0.out_layers.0.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([672]).\n\tsize mismatch for diffusion_net.middle_block.0.out_layers.0.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([672]).\n\tsize mismatch for diffusion_net.middle_block.0.out_layers.3.weight: copying a param with shape torch.Size([768, 768, 3, 3, 3]) from checkpoint, the shape in current model is torch.Size([672, 672, 3, 3, 3]).\n\tsize mismatch for diffusion_net.middle_block.0.out_layers.3.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([672]).\n\tsize mismatch for diffusion_net.middle_block.1.norm.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([672]).\n\tsize mismatch for diffusion_net.middle_block.1.norm.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([672]).\n\tsize mismatch for diffusion_net.middle_block.1.proj_out.weight: copying a param with shape torch.Size([768, 768, 1]) from checkpoint, the shape in current model is torch.Size([672, 672, 1, 1, 1]).\n\tsize mismatch for diffusion_net.middle_block.1.proj_out.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([672]).\n\tsize mismatch for diffusion_net.middle_block.2.in_layers.0.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([672]).\n\tsize mismatch for diffusion_net.middle_block.2.in_layers.0.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([672]).\n\tsize mismatch for diffusion_net.middle_block.2.in_layers.2.weight: copying a param with shape torch.Size([768, 768, 3, 3, 3]) from checkpoint, the shape in current model is torch.Size([672, 672, 3, 3, 3]).\n\tsize mismatch for diffusion_net.middle_block.2.in_layers.2.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([672]).\n\tsize mismatch for diffusion_net.middle_block.2.emb_layers.1.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([672, 896]).\n\tsize mismatch for diffusion_net.middle_block.2.emb_layers.1.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([672]).\n\tsize mismatch for diffusion_net.middle_block.2.out_layers.0.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([672]).\n\tsize mismatch for diffusion_net.middle_block.2.out_layers.0.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([672]).\n\tsize mismatch for diffusion_net.middle_block.2.out_layers.3.weight: copying a param with shape torch.Size([768, 768, 3, 3, 3]) from checkpoint, the shape in current model is torch.Size([672, 672, 3, 3, 3]).\n\tsize mismatch for diffusion_net.middle_block.2.out_layers.3.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([672]).\n\tsize mismatch for diffusion_net.output_blocks.0.0.in_layers.0.weight: copying a param with shape torch.Size([1536]) from checkpoint, the shape in current model is torch.Size([1344]).\n\tsize mismatch for diffusion_net.output_blocks.0.0.in_layers.0.bias: copying a param with shape torch.Size([1536]) from checkpoint, the shape in current model is torch.Size([1344]).\n\tsize mismatch for diffusion_net.output_blocks.0.0.in_layers.2.weight: copying a param with shape torch.Size([768, 1536, 3, 3, 3]) from checkpoint, the shape in current model is torch.Size([672, 1344, 3, 3, 3]).\n\tsize mismatch for diffusion_net.output_blocks.0.0.in_layers.2.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([672]).\n\tsize mismatch for diffusion_net.output_blocks.0.0.emb_layers.1.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([672, 896]).\n\tsize mismatch for diffusion_net.output_blocks.0.0.emb_layers.1.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([672]).\n\tsize mismatch for diffusion_net.output_blocks.0.0.out_layers.0.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([672]).\n\tsize mismatch for diffusion_net.output_blocks.0.0.out_layers.0.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([672]).\n\tsize mismatch for diffusion_net.output_blocks.0.0.out_layers.3.weight: copying a param with shape torch.Size([768, 768, 3, 3, 3]) from checkpoint, the shape in current model is torch.Size([672, 672, 3, 3, 3]).\n\tsize mismatch for diffusion_net.output_blocks.0.0.out_layers.3.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([672]).\n\tsize mismatch for diffusion_net.output_blocks.0.0.skip_connection.weight: copying a param with shape torch.Size([768, 1536, 1, 1, 1]) from checkpoint, the shape in current model is torch.Size([672, 1344, 1, 1, 1]).\n\tsize mismatch for diffusion_net.output_blocks.0.0.skip_connection.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([672]).\n\tsize mismatch for diffusion_net.output_blocks.1.0.in_layers.0.weight: copying a param with shape torch.Size([1536]) from checkpoint, the shape in current model is torch.Size([1344]).\n\tsize mismatch for diffusion_net.output_blocks.1.0.in_layers.0.bias: copying a param with shape torch.Size([1536]) from checkpoint, the shape in current model is torch.Size([1344]).\n\tsize mismatch for diffusion_net.output_blocks.1.0.in_layers.2.weight: copying a param with shape torch.Size([768, 1536, 3, 3, 3]) from checkpoint, the shape in current model is torch.Size([672, 1344, 3, 3, 3]).\n\tsize mismatch for diffusion_net.output_blocks.1.0.in_layers.2.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([672]).\n\tsize mismatch for diffusion_net.output_blocks.1.0.emb_layers.1.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([672, 896]).\n\tsize mismatch for diffusion_net.output_blocks.1.0.emb_layers.1.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([672]).\n\tsize mismatch for diffusion_net.output_blocks.1.0.out_layers.0.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([672]).\n\tsize mismatch for diffusion_net.output_blocks.1.0.out_layers.0.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([672]).\n\tsize mismatch for diffusion_net.output_blocks.1.0.out_layers.3.weight: copying a param with shape torch.Size([768, 768, 3, 3, 3]) from checkpoint, the shape in current model is torch.Size([672, 672, 3, 3, 3]).\n\tsize mismatch for diffusion_net.output_blocks.1.0.out_layers.3.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([672]).\n\tsize mismatch for diffusion_net.output_blocks.1.0.skip_connection.weight: copying a param with shape torch.Size([768, 1536, 1, 1, 1]) from checkpoint, the shape in current model is torch.Size([672, 1344, 1, 1, 1]).\n\tsize mismatch for diffusion_net.output_blocks.1.0.skip_connection.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([672]).\n\tsize mismatch for diffusion_net.output_blocks.2.0.in_layers.0.weight: copying a param with shape torch.Size([1536]) from checkpoint, the shape in current model is torch.Size([1120]).\n\tsize mismatch for diffusion_net.output_blocks.2.0.in_layers.0.bias: copying a param with shape torch.Size([1536]) from checkpoint, the shape in current model is torch.Size([1120]).\n\tsize mismatch for diffusion_net.output_blocks.2.0.in_layers.2.weight: copying a param with shape torch.Size([768, 1536, 3, 3, 3]) from checkpoint, the shape in current model is torch.Size([672, 1120, 3, 3, 3]).\n\tsize mismatch for diffusion_net.output_blocks.2.0.in_layers.2.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([672]).\n\tsize mismatch for diffusion_net.output_blocks.2.0.emb_layers.1.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([672, 896]).\n\tsize mismatch for diffusion_net.output_blocks.2.0.emb_layers.1.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([672]).\n\tsize mismatch for diffusion_net.output_blocks.2.0.out_layers.0.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([672]).\n\tsize mismatch for diffusion_net.output_blocks.2.0.out_layers.0.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([672]).\n\tsize mismatch for diffusion_net.output_blocks.2.0.out_layers.3.weight: copying a param with shape torch.Size([768, 768, 3, 3, 3]) from checkpoint, the shape in current model is torch.Size([672, 672, 3, 3, 3]).\n\tsize mismatch for diffusion_net.output_blocks.2.0.out_layers.3.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([672]).\n\tsize mismatch for diffusion_net.output_blocks.2.0.skip_connection.weight: copying a param with shape torch.Size([768, 1536, 1, 1, 1]) from checkpoint, the shape in current model is torch.Size([672, 1120, 1, 1, 1]).\n\tsize mismatch for diffusion_net.output_blocks.2.0.skip_connection.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([672]).\n\tsize mismatch for diffusion_net.output_blocks.3.0.in_layers.0.weight: copying a param with shape torch.Size([1536]) from checkpoint, the shape in current model is torch.Size([1120]).\n\tsize mismatch for diffusion_net.output_blocks.3.0.in_layers.0.bias: copying a param with shape torch.Size([1536]) from checkpoint, the shape in current model is torch.Size([1120]).\n\tsize mismatch for diffusion_net.output_blocks.3.0.in_layers.2.weight: copying a param with shape torch.Size([768, 1536, 3, 3, 3]) from checkpoint, the shape in current model is torch.Size([448, 1120, 3, 3, 3]).\n\tsize mismatch for diffusion_net.output_blocks.3.0.in_layers.2.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([448]).\n\tsize mismatch for diffusion_net.output_blocks.3.0.emb_layers.1.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([448, 896]).\n\tsize mismatch for diffusion_net.output_blocks.3.0.emb_layers.1.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([448]).\n\tsize mismatch for diffusion_net.output_blocks.3.0.out_layers.0.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([448]).\n\tsize mismatch for diffusion_net.output_blocks.3.0.out_layers.0.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([448]).\n\tsize mismatch for diffusion_net.output_blocks.3.0.out_layers.3.weight: copying a param with shape torch.Size([768, 768, 3, 3, 3]) from checkpoint, the shape in current model is torch.Size([448, 448, 3, 3, 3]).\n\tsize mismatch for diffusion_net.output_blocks.3.0.out_layers.3.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([448]).\n\tsize mismatch for diffusion_net.output_blocks.3.0.skip_connection.weight: copying a param with shape torch.Size([768, 1536, 1, 1, 1]) from checkpoint, the shape in current model is torch.Size([448, 1120, 1, 1, 1]).\n\tsize mismatch for diffusion_net.output_blocks.3.0.skip_connection.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([448]).\n\tsize mismatch for diffusion_net.output_blocks.3.1.norm.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([448]).\n\tsize mismatch for diffusion_net.output_blocks.3.1.norm.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([448]).\n\tsize mismatch for diffusion_net.output_blocks.3.1.proj_out.weight: copying a param with shape torch.Size([768, 768, 1]) from checkpoint, the shape in current model is torch.Size([448, 448, 1, 1, 1]).\n\tsize mismatch for diffusion_net.output_blocks.3.1.proj_out.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([448]).\n\tsize mismatch for diffusion_net.output_blocks.4.0.in_layers.0.weight: copying a param with shape torch.Size([1536]) from checkpoint, the shape in current model is torch.Size([896]).\n\tsize mismatch for diffusion_net.output_blocks.4.0.in_layers.0.bias: copying a param with shape torch.Size([1536]) from checkpoint, the shape in current model is torch.Size([896]).\n\tsize mismatch for diffusion_net.output_blocks.4.0.in_layers.2.weight: copying a param with shape torch.Size([768, 1536, 3, 3, 3]) from checkpoint, the shape in current model is torch.Size([448, 896, 3, 3, 3]).\n\tsize mismatch for diffusion_net.output_blocks.4.0.in_layers.2.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([448]).\n\tsize mismatch for diffusion_net.output_blocks.4.0.emb_layers.1.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([448, 896]).\n\tsize mismatch for diffusion_net.output_blocks.4.0.emb_layers.1.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([448]).\n\tsize mismatch for diffusion_net.output_blocks.4.0.out_layers.0.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([448]).\n\tsize mismatch for diffusion_net.output_blocks.4.0.out_layers.0.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([448]).\n\tsize mismatch for diffusion_net.output_blocks.4.0.out_layers.3.weight: copying a param with shape torch.Size([768, 768, 3, 3, 3]) from checkpoint, the shape in current model is torch.Size([448, 448, 3, 3, 3]).\n\tsize mismatch for diffusion_net.output_blocks.4.0.out_layers.3.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([448]).\n\tsize mismatch for diffusion_net.output_blocks.4.0.skip_connection.weight: copying a param with shape torch.Size([768, 1536, 1, 1, 1]) from checkpoint, the shape in current model is torch.Size([448, 896, 1, 1, 1]).\n\tsize mismatch for diffusion_net.output_blocks.4.0.skip_connection.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([448]).\n\tsize mismatch for diffusion_net.output_blocks.4.1.norm.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([448]).\n\tsize mismatch for diffusion_net.output_blocks.4.1.norm.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([448]).\n\tsize mismatch for diffusion_net.output_blocks.4.1.proj_out.weight: copying a param with shape torch.Size([768, 768, 1]) from checkpoint, the shape in current model is torch.Size([448, 448, 1, 1, 1]).\n\tsize mismatch for diffusion_net.output_blocks.4.1.proj_out.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([448]).\n\tsize mismatch for diffusion_net.output_blocks.5.0.in_layers.0.weight: copying a param with shape torch.Size([1152]) from checkpoint, the shape in current model is torch.Size([672]).\n\tsize mismatch for diffusion_net.output_blocks.5.0.in_layers.0.bias: copying a param with shape torch.Size([1152]) from checkpoint, the shape in current model is torch.Size([672]).\n\tsize mismatch for diffusion_net.output_blocks.5.0.in_layers.2.weight: copying a param with shape torch.Size([768, 1152, 3, 3, 3]) from checkpoint, the shape in current model is torch.Size([448, 672, 3, 3, 3]).\n\tsize mismatch for diffusion_net.output_blocks.5.0.in_layers.2.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([448]).\n\tsize mismatch for diffusion_net.output_blocks.5.0.emb_layers.1.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([448, 896]).\n\tsize mismatch for diffusion_net.output_blocks.5.0.emb_layers.1.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([448]).\n\tsize mismatch for diffusion_net.output_blocks.5.0.out_layers.0.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([448]).\n\tsize mismatch for diffusion_net.output_blocks.5.0.out_layers.0.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([448]).\n\tsize mismatch for diffusion_net.output_blocks.5.0.out_layers.3.weight: copying a param with shape torch.Size([768, 768, 3, 3, 3]) from checkpoint, the shape in current model is torch.Size([448, 448, 3, 3, 3]).\n\tsize mismatch for diffusion_net.output_blocks.5.0.out_layers.3.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([448]).\n\tsize mismatch for diffusion_net.output_blocks.5.0.skip_connection.weight: copying a param with shape torch.Size([768, 1152, 1, 1, 1]) from checkpoint, the shape in current model is torch.Size([448, 672, 1, 1, 1]).\n\tsize mismatch for diffusion_net.output_blocks.5.0.skip_connection.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([448]).\n\tsize mismatch for diffusion_net.output_blocks.5.1.norm.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([448]).\n\tsize mismatch for diffusion_net.output_blocks.5.1.norm.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([448]).\n\tsize mismatch for diffusion_net.output_blocks.5.1.proj_out.weight: copying a param with shape torch.Size([768, 768, 1]) from checkpoint, the shape in current model is torch.Size([448, 448, 1, 1, 1]).\n\tsize mismatch for diffusion_net.output_blocks.5.1.proj_out.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([448]).\n\tsize mismatch for diffusion_net.output_blocks.5.2.conv.weight: copying a param with shape torch.Size([768, 768, 3, 3, 3]) from checkpoint, the shape in current model is torch.Size([448, 448, 3, 3, 3]).\n\tsize mismatch for diffusion_net.output_blocks.5.2.conv.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([448]).\n\tsize mismatch for diffusion_net.output_blocks.6.0.in_layers.0.weight: copying a param with shape torch.Size([1152]) from checkpoint, the shape in current model is torch.Size([672]).\n\tsize mismatch for diffusion_net.output_blocks.6.0.in_layers.0.bias: copying a param with shape torch.Size([1152]) from checkpoint, the shape in current model is torch.Size([672]).\n\tsize mismatch for diffusion_net.output_blocks.6.0.in_layers.2.weight: copying a param with shape torch.Size([384, 1152, 3, 3, 3]) from checkpoint, the shape in current model is torch.Size([224, 672, 3, 3, 3]).\n\tsize mismatch for diffusion_net.output_blocks.6.0.in_layers.2.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([224]).\n\tsize mismatch for diffusion_net.output_blocks.6.0.emb_layers.1.weight: copying a param with shape torch.Size([384, 768]) from checkpoint, the shape in current model is torch.Size([224, 896]).\n\tsize mismatch for diffusion_net.output_blocks.6.0.emb_layers.1.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([224]).\n\tsize mismatch for diffusion_net.output_blocks.6.0.out_layers.0.weight: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([224]).\n\tsize mismatch for diffusion_net.output_blocks.6.0.out_layers.0.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([224]).\n\tsize mismatch for diffusion_net.output_blocks.6.0.out_layers.3.weight: copying a param with shape torch.Size([384, 384, 3, 3, 3]) from checkpoint, the shape in current model is torch.Size([224, 224, 3, 3, 3]).\n\tsize mismatch for diffusion_net.output_blocks.6.0.out_layers.3.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([224]).\n\tsize mismatch for diffusion_net.output_blocks.6.0.skip_connection.weight: copying a param with shape torch.Size([384, 1152, 1, 1, 1]) from checkpoint, the shape in current model is torch.Size([224, 672, 1, 1, 1]).\n\tsize mismatch for diffusion_net.output_blocks.6.0.skip_connection.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([224]).\n\tsize mismatch for diffusion_net.output_blocks.7.0.in_layers.0.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([448]).\n\tsize mismatch for diffusion_net.output_blocks.7.0.in_layers.0.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([448]).\n\tsize mismatch for diffusion_net.output_blocks.7.0.in_layers.2.weight: copying a param with shape torch.Size([384, 768, 3, 3, 3]) from checkpoint, the shape in current model is torch.Size([224, 448, 3, 3, 3]).\n\tsize mismatch for diffusion_net.output_blocks.7.0.in_layers.2.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([224]).\n\tsize mismatch for diffusion_net.output_blocks.7.0.emb_layers.1.weight: copying a param with shape torch.Size([384, 768]) from checkpoint, the shape in current model is torch.Size([224, 896]).\n\tsize mismatch for diffusion_net.output_blocks.7.0.emb_layers.1.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([224]).\n\tsize mismatch for diffusion_net.output_blocks.7.0.out_layers.0.weight: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([224]).\n\tsize mismatch for diffusion_net.output_blocks.7.0.out_layers.0.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([224]).\n\tsize mismatch for diffusion_net.output_blocks.7.0.out_layers.3.weight: copying a param with shape torch.Size([384, 384, 3, 3, 3]) from checkpoint, the shape in current model is torch.Size([224, 224, 3, 3, 3]).\n\tsize mismatch for diffusion_net.output_blocks.7.0.out_layers.3.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([224]).\n\tsize mismatch for diffusion_net.output_blocks.7.0.skip_connection.weight: copying a param with shape torch.Size([384, 768, 1, 1, 1]) from checkpoint, the shape in current model is torch.Size([224, 448, 1, 1, 1]).\n\tsize mismatch for diffusion_net.output_blocks.7.0.skip_connection.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([224]).\n\tsize mismatch for diffusion_net.output_blocks.8.0.in_layers.0.weight: copying a param with shape torch.Size([576]) from checkpoint, the shape in current model is torch.Size([448]).\n\tsize mismatch for diffusion_net.output_blocks.8.0.in_layers.0.bias: copying a param with shape torch.Size([576]) from checkpoint, the shape in current model is torch.Size([448]).\n\tsize mismatch for diffusion_net.output_blocks.8.0.in_layers.2.weight: copying a param with shape torch.Size([384, 576, 3, 3, 3]) from checkpoint, the shape in current model is torch.Size([224, 448, 3, 3, 3]).\n\tsize mismatch for diffusion_net.output_blocks.8.0.in_layers.2.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([224]).\n\tsize mismatch for diffusion_net.output_blocks.8.0.emb_layers.1.weight: copying a param with shape torch.Size([384, 768]) from checkpoint, the shape in current model is torch.Size([224, 896]).\n\tsize mismatch for diffusion_net.output_blocks.8.0.emb_layers.1.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([224]).\n\tsize mismatch for diffusion_net.output_blocks.8.0.out_layers.0.weight: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([224]).\n\tsize mismatch for diffusion_net.output_blocks.8.0.out_layers.0.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([224]).\n\tsize mismatch for diffusion_net.output_blocks.8.0.out_layers.3.weight: copying a param with shape torch.Size([384, 384, 3, 3, 3]) from checkpoint, the shape in current model is torch.Size([224, 224, 3, 3, 3]).\n\tsize mismatch for diffusion_net.output_blocks.8.0.out_layers.3.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([224]).\n\tsize mismatch for diffusion_net.output_blocks.8.0.skip_connection.weight: copying a param with shape torch.Size([384, 576, 1, 1, 1]) from checkpoint, the shape in current model is torch.Size([224, 448, 1, 1, 1]).\n\tsize mismatch for diffusion_net.output_blocks.8.0.skip_connection.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([224]).\n\tsize mismatch for diffusion_net.out.0.weight: copying a param with shape torch.Size([192]) from checkpoint, the shape in current model is torch.Size([224]).\n\tsize mismatch for diffusion_net.out.0.bias: copying a param with shape torch.Size([192]) from checkpoint, the shape in current model is torch.Size([224]).\n\tsize mismatch for diffusion_net.out.2.weight: copying a param with shape torch.Size([3, 192, 3, 3, 3]) from checkpoint, the shape in current model is torch.Size([3, 224, 3, 3, 3])."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tGnRcZk0ovon"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from datetime import datetime\n",
        "\n",
        "# txt2shape\n",
        "out_dir = 'demo_results'\n",
        "if not os.path.exists(out_dir): os.makedirs(out_dir)\n",
        "\n",
        "# change the input text here to generate different chairs/tables!\n",
        "input_txt_old = \"A round red color chair with four legs 0.5 meters in length each\"\n",
        "input_txt = \"Generate a chair with four legs of 0.53 meters in length, a round seat cushion with a radius of 1/2 of leg length, and a slat back with gaps of 0.11 meters.\"\n",
        "book_case_txt = \"Generate a bookcase with 4 drawers on top of short bolt legs 0.3 meters each and 3 wide open shelves\"\n",
        "watercraft = \"lamp\"\n",
        "ngen = 1 # number of generated shapes\n",
        "ddim_steps = 100\n",
        "ddim_eta = 0.\n",
        "uc_scale = 3.\n",
        "# Log start time\n",
        "start_time = datetime.now()\n",
        "print(f\"{start_time}: Starting txt2shape generation\")\n",
        "\n",
        "sdf_gen = SDFusion.txt2shape(input_txt=watercraft, ngen=ngen, ddim_steps=ddim_steps, ddim_eta=ddim_eta, uc_scale=uc_scale)\n",
        "\n",
        "# Log after sdf_to_mesh generation\n",
        "end_time = datetime.now()\n",
        "print(f\"{end_time}: Completed sdf_to_mesh\")\n",
        "\n",
        "mesh_gen = sdf_to_mesh(sdf_gen)\n",
        "\n",
        "'''# Save meshes to .obj and/or .ply\n",
        "for i, mesh in enumerate(mesh_gen):\n",
        "    verts, faces = mesh.verts_packed(), mesh.faces_packed()\n",
        "    # Save to .obj format\n",
        "    obj_path = f\"{out_dir}/shape_{i}_{input_txt.replace(' ', '_')}.obj\"\n",
        "    save_obj(obj_path, verts, faces)\n",
        "    print(f\"Saved OBJ to {obj_path}\")'''\n",
        "\n",
        "# vis as gif\n",
        "#gen_name = f'{out_dir}/txt2shape-{input_txt}.gif'\n",
        "#save_mesh_as_gif(SDFusion.renderer, mesh_gen, nrow=3, out_name=gen_name)\n",
        "\n",
        "#print(f'Input: \"{input_txt}\"')\n",
        "#for name in [gen_name]:\n",
        "#    display(ipy_image(name))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title save Mesh\n",
        "#from pytorch3d.io import save_mesh\n",
        "from pathlib import Path\n",
        "from pytorch3d.io import IO\n",
        "\n",
        "out_dir = 'demo_results'\n",
        "\n",
        "for i, mesh in enumerate(mesh_gen):\n",
        "  #mesh.textures.faces_verts_textures_packed()\n",
        "\n",
        "    mesh_path = Path(out_dir) / f\"seed2024shape_wlamp{i}_{input_txt.split(maxsplit=1)[0]}.ply\"\n",
        "    IO().save_mesh(mesh, mesh_path,include_textures=True) #binary=False, colors_as_uint8=True\n",
        "\n",
        "# Directory for saving meshes\n",
        "#out_dir = 'demo_results'\n",
        "#Path(out_dir).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Iterate through generated meshes and save each one\n",
        "'''for i, mesh in enumerate(mesh_gen):\n",
        "    # Ensure the mesh has only one element\n",
        "    #mesh = mesh.extend(1)  # Create a single-element Meshes object\n",
        "\n",
        "    # Define the output path\n",
        "\n",
        "\n",
        "    # Save the mesh with textures included\n",
        "    save_mesh(mesh, mesh_path, binary=True, include_textures=True)\n",
        "    print(f\"Saved mesh to {mesh_path}\")'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "LelV0_kc6dvB",
        "outputId": "ca6eab38-79d8-45a0-b872-66ac3f60efc8"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'for i, mesh in enumerate(mesh_gen):\\n    # Ensure the mesh has only one element\\n    #mesh = mesh.extend(1)  # Create a single-element Meshes object\\n\\n    # Define the output path\\n\\n\\n    # Save the mesh with textures included\\n    save_mesh(mesh, mesh_path, binary=True, include_textures=True)\\n    print(f\"Saved mesh to {mesh_path}\")'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "\n",
        "files.download(mesh_path)"
      ],
      "metadata": {
        "id": "5561EXwhktfS",
        "outputId": "3e508a07-a2a0-423d-f86d-3db30eafd8a5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        }
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_cf5eb9e4-4bd0-4a7a-8ef5-1bb73f5911de\", \"seed2024shape_watercraft0_Generate.ply\", 466682)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "'''\n",
        "# Util function for loading meshes\n",
        "from pytorch3d.io import load_objs_as_meshes, load_obj\n",
        "\n",
        "# Data structures and functions for rendering\n",
        "from pytorch3d.structures import Meshes\n",
        "from pytorch3d.vis.plotly_vis import AxisArgs, plot_batch_individually, plot_scene\n",
        "from pytorch3d.vis.texture_vis import texturesuv_image_matplotlib\n",
        "from pytorch3d.renderer import (\n",
        "    look_at_view_transform,\n",
        "    FoVPerspectiveCameras,\n",
        "    PointLights,\n",
        "    DirectionalLights,\n",
        "    Materials,\n",
        "    RasterizationSettings,\n",
        "    MeshRenderer,\n",
        "    MeshRasterizer,\n",
        "    SoftPhongShader,\n",
        "    TexturesUV,\n",
        "    TexturesVertex\n",
        ")\n",
        "'''\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "id": "463EMJah__kC",
        "outputId": "3ef0216a-2bea-498b-bae4-8e974af1a664"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n# Util function for loading meshes\\nfrom pytorch3d.io import load_objs_as_meshes, load_obj\\n\\n# Data structures and functions for rendering\\nfrom pytorch3d.structures import Meshes\\nfrom pytorch3d.vis.plotly_vis import AxisArgs, plot_batch_individually, plot_scene\\nfrom pytorch3d.vis.texture_vis import texturesuv_image_matplotlib\\nfrom pytorch3d.renderer import (\\n    look_at_view_transform,\\n    FoVPerspectiveCameras, \\n    PointLights, \\n    DirectionalLights, \\n    Materials, \\n    RasterizationSettings, \\n    MeshRenderer, \\n    MeshRasterizer,  \\n    SoftPhongShader,\\n    TexturesUV,\\n    TexturesVertex\\n)\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''!wget https://raw.githubusercontent.com/facebookresearch/pytorch3d/main/docs/tutorials/utils/plot_image_grid.py\n",
        "from plot_image_grid import image_grid'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dqvQf26v_1lF",
        "outputId": "657fa8ea-427b-4964-d76b-766b47d54876"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-01-06 00:25:21--  https://raw.githubusercontent.com/facebookresearch/pytorch3d/main/docs/tutorials/utils/plot_image_grid.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1608 (1.6K) [text/plain]\n",
            "Saving to: ‘plot_image_grid.py’\n",
            "\n",
            "plot_image_grid.py  100%[===================>]   1.57K  --.-KB/s    in 0s      \n",
            "\n",
            "2025-01-06 00:25:21 (29.3 MB/s) - ‘plot_image_grid.py’ saved [1608/1608]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''from pytorch3d.io import save_ply\n",
        "\n",
        "verts = mesh_gen.verts_packed()\n",
        "faces = mesh_gen.faces_packed()\n",
        "verts_rgb = mesh_gen.textures.verts_features_packed()\n",
        "\n",
        "# Save to PLY\n",
        "ply_path = Path(out_dir) / \"textured_mesh.ply\"\n",
        "save_ply(ply_path, verts, faces)\n",
        "print(f\"Saved PLY with vertex colors to {ply_path}\")'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "avmsiubDCwi1",
        "outputId": "e2a95a6e-a3c0-4a76-85da-700060ec5063"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'Path' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-b642a432e0e7>\u001b[0m in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# Save to PLY\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mply_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_dir\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m\"textured_mesh.ply\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0msave_ply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mply_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfaces\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Saved PLY with vertex colors to {ply_path}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'Path' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## display render"
      ],
      "metadata": {
        "id": "sy1HD1-5HZdr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#images = SDFusion.renderer(mesh)\n",
        "#mesh.textures.faces_verts_textures_packed()\n",
        "from pytorch3d.renderer import PointLights\n",
        "\n",
        "lights = PointLights(device=device, location=[[0.0, 0.0, -3.0]])\n",
        "lights.location = torch.tensor([0.2, 0.0, 0.0], device=device)[None]\n",
        "\n",
        "for i, mesh in enumerate(mesh_gen):\n",
        "    texture_image = mesh.textures.verts_features_padded().squeeze().cpu().numpy()\n",
        "    img_comb = SDFusion.renderer(mesh, lights=lights)\n",
        "    print(f\"TEXTURES ---- {mesh.textures}---\")\n",
        "    #img_comb = render_mesh(SDFusion.renderer, mesh, norm=False)\n",
        "    plt.figure(figsize=(10, 10))\n",
        "    plt.imshow(img_comb[0, ..., :3].cpu().numpy())\n",
        "   # plt.imshow(img_comb.cpu().numpy())\n",
        "    plt.axis(\"off\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 824
        },
        "id": "MsWhUHpQ_sVd",
        "outputId": "8b357055-f20b-4254-8436-a5d449ac80a7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TEXTURES ---- <pytorch3d.renderer.mesh.textures.TexturesVertex object at 0x7a033c7674f0>---\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x1000 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''for i, mesh in enumerate(mesh_gen):\n",
        "  plt.figure(figsize=(7,7))\n",
        "  texture_image=mesh.textures.verts_features_padded()\n",
        "  plt.imshow(texture_image.squeeze().cpu().numpy())\n",
        "  plt.axis(\"off\");'''\n",
        "for i, mesh in enumerate(mesh_gen):\n",
        "    plt.figure(figsize=(7, 7))\n",
        "\n",
        "    # Get vertex features (e.g., RGB values)\n",
        "    texture_image = mesh.textures.verts_features_padded()  # Shape: (1, N, 3)\n",
        "\n",
        "    # Check the shape of the texture\n",
        "    print(f\"Texture shape: {texture_image.shape}\")\n",
        "\n",
        "    # Visualize the texture as an array (flattening may be needed)\n",
        "    texture_image_np = texture_image.squeeze().cpu().numpy()  # Shape: (N, 3)\n",
        "    plt.imshow(texture_image_np, aspect='auto')  # Display as a continuous line\n",
        "    plt.axis(\"off\")"
      ],
      "metadata": {
        "id": "Yqh-o5PDX-uS",
        "outputId": "938d414c-6dcf-4de6-b4f9-51632f54f0fe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 593
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Texture shape: torch.Size([1, 8086, 3])\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 700x700 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import trimesh\n",
        "import numpy as np\n",
        "\n",
        "# Extract vertex positions, faces, and vertex colors\n",
        "verts = mesh.verts_packed().cpu().numpy()  # (N, 3)\n",
        "faces = mesh.faces_packed().cpu().numpy()  # (F, 3)\n",
        "verts_rgb = mesh.textures.verts_features_packed().cpu().numpy()  # (N, 3)\n",
        "\n",
        "# Create a Trimesh object with vertex colors\n",
        "trimesh_mesh = trimesh.Trimesh(vertices=verts, faces=faces, vertex_colors=verts_rgb)#vertex_colors=(verts_rgb * 255).astype(np.uint8)\n",
        "\n",
        "# Save to .ply\n",
        "ply_path = Path(out_dir) / f\"newshape_{i}_{input_txt.replace(' ', '_')}.ply\"\n",
        "trimesh_mesh.export(ply_path)\n",
        "print(f\"Saved mesh with vertex colors to {ply_path}\")\n"
      ],
      "metadata": {
        "id": "H47SNXyvaArx",
        "outputId": "b79351be-c1a7-4ba9-be06-a4a8cf6f6459",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved mesh with vertex colors to demo_results/newshape_0_A_round_red_color_chair_with_four_legs_0.5_meters_in_length_each.ply\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pytorch3d.io import save_obj, save_ply\n",
        "\n",
        " # Assuming each mesh has a `textures` attribute with a Textures object\n",
        "for i, mesh in enumerate(mesh_gen):\n",
        "    verts, faces = mesh.verts_packed(), mesh.faces_packed()\n",
        "\n",
        "    # Save geometry and textures to .obj format\n",
        "    if hasattr(mesh, 'textures') and mesh.textures is not None:\n",
        "        vertex_colors = mesh.textures.verts_features_packed()  # Extract vertex colors\n",
        "\n",
        "        # Save to .obj without UVs but with vertex colors\n",
        "        obj_path = f\"{out_dir}/shape_{i}_{input_txt.replace(' ', '_')}_textured.obj\"\n",
        "        save_obj(obj_path, verts, faces)  # OBJ doesn't natively support vertex colors\n",
        "        print(f\"Saved OBJ to {obj_path} (vertex colors not directly supported in OBJ)\")\n",
        "\n",
        "        # Save to .ply with vertex colors\n",
        "        ply_path = f\"{out_dir}/shape_{i}_{input_txt.replace(' ', '_')}_textured.ply\"\n",
        "        save_ply(ply_path, verts, faces, verts_rgb=vertex_colors)\n",
        "        print(f\"Saved PLY to {ply_path} (with vertex colors)\")\n",
        "\n",
        "    else:\n",
        "        # Save without textures\n",
        "        obj_path = f\"{out_dir}/shape_{i}_{input_txt.replace(' ', '_')}.obj\"\n",
        "        save_obj(obj_path, verts, faces)\n",
        "        print(f\"Saved OBJ (no textures) to {obj_path}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 248
        },
        "id": "hrStyMmu3zUU",
        "outputId": "63f33869-29b8-49bb-dfeb-fcc3db5b8a0f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved OBJ to demo_results/shape_0_A_round_red_color_chair_with_four_legs_0.5_meters_in_length_each_textured.obj (vertex colors not directly supported in OBJ)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "save_ply() got an unexpected keyword argument 'verts_rgb'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-21-8adbce2a43ba>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0;31m# Save to .ply with vertex colors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mply_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"{out_dir}/shape_{i}_{input_txt.replace(' ', '_')}_textured.ply\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0msave_ply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mply_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfaces\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverts_rgb\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvertex_colors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Saved PLY to {ply_path} (with vertex colors)\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: save_ply() got an unexpected keyword argument 'verts_rgb'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J1ZQCHmXovoo"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train"
      ],
      "metadata": {
        "id": "dzJRDnYIfERJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gdown"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mrvT6qzSziwP",
        "outputId": "0e67d847-8783-464e-e226-1637a6524e30"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gdown in /usr/local/lib/python3.11/dist-packages (5.2.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (from gdown) (4.13.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from gdown) (3.18.0)\n",
            "Requirement already satisfied: requests[socks] in /usr/local/lib/python3.11/dist-packages (from gdown) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from gdown) (4.67.1)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->gdown) (2.7)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->gdown) (4.13.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->gdown) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->gdown) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->gdown) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->gdown) (2025.4.26)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->gdown) (1.7.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title test\n",
        "\n",
        "import gdown\n",
        "\n",
        "dataset_url = \"https://drive.google.com/drive/folders/1xtraL2jb9t3dQCbsJ91exnEJJohGQPIX\"\n",
        "id=\"1xtraL2jb9t3dQCbsJ91exnEJJohGQPIX\"\n",
        "output = \"data\"\n",
        "gdown.download_folder(id=id,output=output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form",
        "id": "gSX_D9S93ed2",
        "outputId": "5f4bc318-0400-45c4-e31e-d3569d5b53ce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Retrieving folder contents\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing file 1IsFlpu0HXJHe7yHwGprLwSkx8gy1qYHl bird.jpg\n",
            "Processing file 1BPvngBphD8X4_5a4ixGK53Z0YKkCNBZL catstatue.jpg\n",
            "Processing file 1AAk9Axdfu81qeqGwQU0lv_Wu6-mYnBRr frog_sweater.jpg\n",
            "Processing file 1ZAcpTT_JlPP4GutcGUDSmQrNI23ybwbO gsorabbit.jpg\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Retrieving folder contents completed\n",
            "Building directory structure\n",
            "Building directory structure completed\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1IsFlpu0HXJHe7yHwGprLwSkx8gy1qYHl\n",
            "To: /content/data/bird.jpg\n",
            "100%|██████████| 29.2k/29.2k [00:00<00:00, 45.4MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1BPvngBphD8X4_5a4ixGK53Z0YKkCNBZL\n",
            "To: /content/data/catstatue.jpg\n",
            "100%|██████████| 10.8k/10.8k [00:00<00:00, 23.1MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1AAk9Axdfu81qeqGwQU0lv_Wu6-mYnBRr\n",
            "To: /content/data/frog_sweater.jpg\n",
            "100%|██████████| 36.3k/36.3k [00:00<00:00, 15.7MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1ZAcpTT_JlPP4GutcGUDSmQrNI23ybwbO\n",
            "To: /content/data/gsorabbit.jpg\n",
            "100%|██████████| 15.9k/15.9k [00:00<00:00, 31.1MB/s]\n",
            "Download completed\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['data/bird.jpg',\n",
              " 'data/catstatue.jpg',\n",
              " 'data/frog_sweater.jpg',\n",
              " 'data/gsorabbit.jpg']"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir -p data/BuildingNet_dataset_v0_1\n",
        "#&& cd data/BuildingNet_dataset_v0_1\n"
      ],
      "metadata": {
        "id": "Lj_cPh-0Dx4M"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd data\n",
        "!ls\n",
        "#'''import os\n",
        "\n",
        "#os.mkdirs(\"BuildingNet_dataset_v0_1\",exist_ok=True)'''"
      ],
      "metadata": {
        "id": "o5Z457_7EIjK",
        "outputId": "1fd25789-8d02-4500-8e2f-afbe7b70c947",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/SDFusion/data\n",
            "BuildingNet_dataset_v0_1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gdown\n",
        "\n",
        "dataset_url = \"https://drive.google.com/drive/folders/169v-PBSDj-DIFiWI7HTUAQbna5atUgFW\"\n",
        "id=\"169v-PBSDj-DIFiWI7HTUAQbna5atUgFW\"\n",
        "\n",
        "output = \"BuildingNet_dataset_v0_1\"\n",
        "\n",
        "gdown.download_folder(id=id,output=output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FENS6kq7zol6",
        "outputId": "7352038e-ce34-4d8f-d19a-caaa25a08744"
      },
      "execution_count": 9,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Retrieving folder contents\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Retrieving folder 1JtjkhTq3KNgZtnUh4R11T_kezIDWCSCx 3DWarehouse_IDs\n",
            "Processing file 1HtRoIpNYhMhkl5K0X4yxb-NWxNfAcwlt 3dwarehouse_ids.csv\n",
            "Processing file 1ZgZ5bux8edo93bu8vV4Umtb7wDtU8WVB 3dwarehouse_ids.json\n",
            "Retrieving folder 1AK2jk3WS6az74xlmIdBM-1BatRz0_4bQ model_data\n",
            "Retrieving folder 1ixOFib3WjHBEKGQXIWHEcodR9qawNHvu GNN\n",
            "Processing file 1OsW0K3pRaS0RBFt94BdoAXGLb6s4Spv5 adjacency.zip\n",
            "Processing file 1bPfKEuR9cPsmgAU-rXM_FYspAiFNOQaU containment.zip\n",
            "Processing file 1CJHkI5VEUvUur1-lDZddqlkbOvEoAlhy label.zip\n",
            "Processing file 1zyA8PUDZUo5GdAyZNEwnkbEbMRRYSDtH node.zip\n",
            "Processing file 1gKe6RzE_C9nulSvNZkXraFC2_BEL7gBS pretrained_avgpool_minkownormal_features.zip\n",
            "Processing file 1_wNGk4cmmoQ8I0Bves0oUGgSZ7OZhmRe similarity.zip\n",
            "Processing file 1XaLnXXBYdPg54ZcYlNZS_p_anOO80nun support.zip\n",
            "Processing file 18mh4QXT-AINGwtYiYw6bED461g-w5vrE surfacearea.zip\n",
            "Processing file 1hbevvRGRc_uKH30iPF8wugxBvsGiHkWy test.txt\n",
            "Processing file 1P3SwrivpZGDoLmD3iVrv96CkJog89yCF train.txt\n",
            "Processing file 1x7QuPy0dQNU_kdHMxUPCqdqGkcsNIvnK val.txt\n",
            "Retrieving folder 1TsW0xizIKrQFSfwHuxgNLZ9vOn2csdfa obj\n",
            "Processing file 1D7K91ZQW5Qf2nJIs1dLpGZcjq7_2jfkJ component_labels.zip\n",
            "Processing file 1RncKUqLKHDDLdKTIQsJU1gDzy8PlGiz- face_labels.zip\n",
            "Processing file 1MoQSy1A83zWYstui73BXURFImBh08tpw faceindex_componentID.zip\n",
            "Retrieving folder 1DNF4FMAZgOSA1oh4a6IRDxIGLdo0m35H point_cloud\n",
            "Processing file 1VH0tOoLzoz-pyAD2nANbS26XkbHc-xA6 point_faceindex.zip\n",
            "Processing file 1Eos7sq9JCemVfeJDfnz6TtMsUnDxBUeD point_labels.zip\n",
            "Retrieving folder 1hX4tNzfYEBTl2O8lqFgSUFFl3kZBOWIZ splits\n",
            "Processing file 10atMR9EA1rdFkYXG1wsdFsBYV6sDMV6f dataset_models.txt\n",
            "Processing file 17vyint0-SdmPtybOeLt09sUl4lL4b4NB test_split.txt\n",
            "Processing file 1Qw5j-UF_sPrVEbhcJkmn-fot21qBEDio train_split.txt\n",
            "Processing file 1CdLYAiko02qPEdR3kQj_ZMbyO9MbqOzx val_split.txt\n",
            "Processing file 1rgGuDgBl9JlYdvMhPe2Vgg4uc2Z6g3WL OBJ_MODELS.zip\n",
            "Processing file 1_lZEcr80qRh2puNQd3o8FsH_VQl5xSIk POINT_CLOUDS.zip\n",
            "Processing file 16-bzzILFPj2dddU6Y7FcmOpQkEig_Ho- README.txt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Retrieving folder contents completed\n",
            "Building directory structure\n",
            "Building directory structure completed\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1HtRoIpNYhMhkl5K0X4yxb-NWxNfAcwlt\n",
            "To: /content/SDFusion/data/BuildingNet_dataset_v0_1/3DWarehouse_IDs/3dwarehouse_ids.csv\n",
            "100%|██████████| 263k/263k [00:00<00:00, 6.03MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1ZgZ5bux8edo93bu8vV4Umtb7wDtU8WVB\n",
            "To: /content/SDFusion/data/BuildingNet_dataset_v0_1/3DWarehouse_IDs/3dwarehouse_ids.json\n",
            "100%|██████████| 315k/315k [00:00<00:00, 5.39MB/s]\n",
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1OsW0K3pRaS0RBFt94BdoAXGLb6s4Spv5\n",
            "From (redirected): https://drive.google.com/uc?id=1OsW0K3pRaS0RBFt94BdoAXGLb6s4Spv5&confirm=t&uuid=ea746b0e-b161-45c9-8c5f-a0690f1870f2\n",
            "To: /content/SDFusion/data/BuildingNet_dataset_v0_1/model_data/GNN/adjacency.zip\n",
            "100%|██████████| 62.9M/62.9M [00:01<00:00, 53.4MB/s]\n",
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1bPfKEuR9cPsmgAU-rXM_FYspAiFNOQaU\n",
            "From (redirected): https://drive.google.com/uc?id=1bPfKEuR9cPsmgAU-rXM_FYspAiFNOQaU&confirm=t&uuid=263d37d8-95e4-4b49-8922-f91df01ca673\n",
            "To: /content/SDFusion/data/BuildingNet_dataset_v0_1/model_data/GNN/containment.zip\n",
            "100%|██████████| 94.9M/94.9M [00:01<00:00, 56.3MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1CJHkI5VEUvUur1-lDZddqlkbOvEoAlhy\n",
            "To: /content/SDFusion/data/BuildingNet_dataset_v0_1/model_data/GNN/label.zip\n",
            "100%|██████████| 3.22M/3.22M [00:00<00:00, 24.6MB/s]\n",
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1zyA8PUDZUo5GdAyZNEwnkbEbMRRYSDtH\n",
            "From (redirected): https://drive.google.com/uc?id=1zyA8PUDZUo5GdAyZNEwnkbEbMRRYSDtH&confirm=t&uuid=2a46b132-a4c3-41ae-90e9-42efcc408785\n",
            "To: /content/SDFusion/data/BuildingNet_dataset_v0_1/model_data/GNN/node.zip\n",
            "100%|██████████| 82.3M/82.3M [00:02<00:00, 28.4MB/s]\n",
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1gKe6RzE_C9nulSvNZkXraFC2_BEL7gBS\n",
            "From (redirected): https://drive.google.com/uc?id=1gKe6RzE_C9nulSvNZkXraFC2_BEL7gBS&confirm=t&uuid=51a71ab9-44db-4e50-aab0-b95e7b2ba894\n",
            "To: /content/SDFusion/data/BuildingNet_dataset_v0_1/model_data/GNN/pretrained_avgpool_minkownormal_features.zip\n",
            "100%|██████████| 281M/281M [00:03<00:00, 74.7MB/s]\n",
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1_wNGk4cmmoQ8I0Bves0oUGgSZ7OZhmRe\n",
            "From (redirected): https://drive.google.com/uc?id=1_wNGk4cmmoQ8I0Bves0oUGgSZ7OZhmRe&confirm=t&uuid=5e8029a7-085d-4f8d-a0a6-53595e4a0d14\n",
            "To: /content/SDFusion/data/BuildingNet_dataset_v0_1/model_data/GNN/similarity.zip\n",
            "100%|██████████| 25.5M/25.5M [00:00<00:00, 62.3MB/s]\n",
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1XaLnXXBYdPg54ZcYlNZS_p_anOO80nun\n",
            "From (redirected): https://drive.google.com/uc?id=1XaLnXXBYdPg54ZcYlNZS_p_anOO80nun&confirm=t&uuid=fcf8b212-d256-4029-af8e-eaa2175f744f\n",
            "To: /content/SDFusion/data/BuildingNet_dataset_v0_1/model_data/GNN/support.zip\n",
            "100%|██████████| 24.8M/24.8M [00:00<00:00, 36.9MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=18mh4QXT-AINGwtYiYw6bED461g-w5vrE\n",
            "To: /content/SDFusion/data/BuildingNet_dataset_v0_1/model_data/GNN/surfacearea.zip\n",
            "100%|██████████| 11.4M/11.4M [00:00<00:00, 42.3MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1hbevvRGRc_uKH30iPF8wugxBvsGiHkWy\n",
            "To: /content/SDFusion/data/BuildingNet_dataset_v0_1/model_data/GNN/test.txt\n",
            "100%|██████████| 5.31k/5.31k [00:00<00:00, 13.3MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1P3SwrivpZGDoLmD3iVrv96CkJog89yCF\n",
            "To: /content/SDFusion/data/BuildingNet_dataset_v0_1/model_data/GNN/train.txt\n",
            "100%|██████████| 42.3k/42.3k [00:00<00:00, 17.2MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1x7QuPy0dQNU_kdHMxUPCqdqGkcsNIvnK\n",
            "To: /content/SDFusion/data/BuildingNet_dataset_v0_1/model_data/GNN/val.txt\n",
            "100%|██████████| 5.32k/5.32k [00:00<00:00, 9.19MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1D7K91ZQW5Qf2nJIs1dLpGZcjq7_2jfkJ\n",
            "To: /content/SDFusion/data/BuildingNet_dataset_v0_1/model_data/obj/component_labels.zip\n",
            "100%|██████████| 3.32M/3.32M [00:00<00:00, 24.9MB/s]\n",
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1RncKUqLKHDDLdKTIQsJU1gDzy8PlGiz-\n",
            "From (redirected): https://drive.google.com/uc?id=1RncKUqLKHDDLdKTIQsJU1gDzy8PlGiz-&confirm=t&uuid=e9ec4c6d-069e-4712-b619-bba65b0a7707\n",
            "To: /content/SDFusion/data/BuildingNet_dataset_v0_1/model_data/obj/face_labels.zip\n",
            "100%|██████████| 267M/267M [00:03<00:00, 86.9MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1MoQSy1A83zWYstui73BXURFImBh08tpw\n",
            "To: /content/SDFusion/data/BuildingNet_dataset_v0_1/model_data/obj/faceindex_componentID.zip\n",
            "100%|██████████| 10.1M/10.1M [00:00<00:00, 57.5MB/s]\n",
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1VH0tOoLzoz-pyAD2nANbS26XkbHc-xA6\n",
            "From (redirected): https://drive.google.com/uc?id=1VH0tOoLzoz-pyAD2nANbS26XkbHc-xA6&confirm=t&uuid=6f4ff46d-c0f3-47f5-8c49-5065e555f6ee\n",
            "To: /content/SDFusion/data/BuildingNet_dataset_v0_1/model_data/point_cloud/point_faceindex.zip\n",
            "100%|██████████| 379M/379M [00:03<00:00, 101MB/s]\n",
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1Eos7sq9JCemVfeJDfnz6TtMsUnDxBUeD\n",
            "From (redirected): https://drive.google.com/uc?id=1Eos7sq9JCemVfeJDfnz6TtMsUnDxBUeD&confirm=t&uuid=63dbdf89-e8ab-45ce-a824-ed2c1fb6b361\n",
            "To: /content/SDFusion/data/BuildingNet_dataset_v0_1/model_data/point_cloud/point_labels.zip\n",
            "100%|██████████| 542M/542M [00:07<00:00, 69.3MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=10atMR9EA1rdFkYXG1wsdFsBYV6sDMV6f\n",
            "To: /content/SDFusion/data/BuildingNet_dataset_v0_1/splits/dataset_models.txt\n",
            "100%|██████████| 52.9k/52.9k [00:00<00:00, 9.62MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=17vyint0-SdmPtybOeLt09sUl4lL4b4NB\n",
            "To: /content/SDFusion/data/BuildingNet_dataset_v0_1/splits/test_split.txt\n",
            "100%|██████████| 5.31k/5.31k [00:00<00:00, 13.2MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1Qw5j-UF_sPrVEbhcJkmn-fot21qBEDio\n",
            "To: /content/SDFusion/data/BuildingNet_dataset_v0_1/splits/train_split.txt\n",
            "100%|██████████| 42.3k/42.3k [00:00<00:00, 11.7MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1CdLYAiko02qPEdR3kQj_ZMbyO9MbqOzx\n",
            "To: /content/SDFusion/data/BuildingNet_dataset_v0_1/splits/val_split.txt\n",
            "100%|██████████| 5.32k/5.32k [00:00<00:00, 12.9MB/s]\n",
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1rgGuDgBl9JlYdvMhPe2Vgg4uc2Z6g3WL\n",
            "From (redirected): https://drive.google.com/uc?id=1rgGuDgBl9JlYdvMhPe2Vgg4uc2Z6g3WL&confirm=t&uuid=c8cf0b43-ef0b-4d56-9728-a5734febbe13\n",
            "To: /content/SDFusion/data/BuildingNet_dataset_v0_1/OBJ_MODELS.zip\n",
            "100%|██████████| 6.58G/6.58G [01:18<00:00, 83.6MB/s]\n",
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1_lZEcr80qRh2puNQd3o8FsH_VQl5xSIk\n",
            "From (redirected): https://drive.google.com/uc?id=1_lZEcr80qRh2puNQd3o8FsH_VQl5xSIk&confirm=t&uuid=2975c401-e57f-4878-9153-d5a46be57c3d\n",
            "To: /content/SDFusion/data/BuildingNet_dataset_v0_1/POINT_CLOUDS.zip\n",
            "100%|██████████| 4.25G/4.25G [01:00<00:00, 70.3MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=16-bzzILFPj2dddU6Y7FcmOpQkEig_Ho-\n",
            "To: /content/SDFusion/data/BuildingNet_dataset_v0_1/README.txt\n",
            "100%|██████████| 5.15k/5.15k [00:00<00:00, 12.6MB/s]\n",
            "Download completed\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['BuildingNet_dataset_v0_1/3DWarehouse_IDs/3dwarehouse_ids.csv',\n",
              " 'BuildingNet_dataset_v0_1/3DWarehouse_IDs/3dwarehouse_ids.json',\n",
              " 'BuildingNet_dataset_v0_1/model_data/GNN/adjacency.zip',\n",
              " 'BuildingNet_dataset_v0_1/model_data/GNN/containment.zip',\n",
              " 'BuildingNet_dataset_v0_1/model_data/GNN/label.zip',\n",
              " 'BuildingNet_dataset_v0_1/model_data/GNN/node.zip',\n",
              " 'BuildingNet_dataset_v0_1/model_data/GNN/pretrained_avgpool_minkownormal_features.zip',\n",
              " 'BuildingNet_dataset_v0_1/model_data/GNN/similarity.zip',\n",
              " 'BuildingNet_dataset_v0_1/model_data/GNN/support.zip',\n",
              " 'BuildingNet_dataset_v0_1/model_data/GNN/surfacearea.zip',\n",
              " 'BuildingNet_dataset_v0_1/model_data/GNN/test.txt',\n",
              " 'BuildingNet_dataset_v0_1/model_data/GNN/train.txt',\n",
              " 'BuildingNet_dataset_v0_1/model_data/GNN/val.txt',\n",
              " 'BuildingNet_dataset_v0_1/model_data/obj/component_labels.zip',\n",
              " 'BuildingNet_dataset_v0_1/model_data/obj/face_labels.zip',\n",
              " 'BuildingNet_dataset_v0_1/model_data/obj/faceindex_componentID.zip',\n",
              " 'BuildingNet_dataset_v0_1/model_data/point_cloud/point_faceindex.zip',\n",
              " 'BuildingNet_dataset_v0_1/model_data/point_cloud/point_labels.zip',\n",
              " 'BuildingNet_dataset_v0_1/splits/dataset_models.txt',\n",
              " 'BuildingNet_dataset_v0_1/splits/test_split.txt',\n",
              " 'BuildingNet_dataset_v0_1/splits/train_split.txt',\n",
              " 'BuildingNet_dataset_v0_1/splits/val_split.txt',\n",
              " 'BuildingNet_dataset_v0_1/OBJ_MODELS.zip',\n",
              " 'BuildingNet_dataset_v0_1/POINT_CLOUDS.zip',\n",
              " 'BuildingNet_dataset_v0_1/README.txt']"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls\n",
        "%cd data\n",
        "!ls"
      ],
      "metadata": {
        "id": "grRASaGoQBYC",
        "outputId": "94d8147f-afed-4cf7-e414-063c10b9bb4c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "configs\t\t    demo_img2shape.ipynb\t  launchers   README.md\n",
            "data\t\t    demo_mm2shape.ipynb\t\t  LICENSE     saved_ckpt\n",
            "dataset_info_files  demo_txt2shape.ipynb\t  models      setup_env.sh\n",
            "datasets\t    demo_uncond_shape_comp.ipynb  options     train.py\n",
            "demo_data\t    external\t\t\t  preprocess  utils\n",
            "/content/SDFusion/data\n",
            "BuildingNet_dataset_v0_1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title unzip the dataset\n",
        "# Replace this with the actual URL you get after filling the form\n",
        "\n",
        "#dataset_filename = \"BuildingNet_dataset_v0_1.zip\"\n",
        "folder_target_1=\"BuildingNet_dataset_v0_1/OBJ_MODELS.zip\"\n",
        "folder_target_2=\"BuildingNet_dataset_v0_1/POINT_CLOUDS.zip\"\n",
        "# Unzip to ./data/\n",
        "!unzip -q {folder_target_1} -d 'BuildingNet_dataset_v0_1'\n",
        "#!unzip -q {folder_target_2} -d \"BuildingNet_dataset_v0_1\"\n"
      ],
      "metadata": {
        "id": "dbPQOsGyfHF4"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd ..\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rLSYuaBc9y2V",
        "outputId": "b04df158-cb11-464d-8a97-5338c0dd28ff"
      },
      "execution_count": 11,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/SDFusion\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "id": "9B_uds4-YhhX",
        "outputId": "01fbcfbb-ea28-4c5b-abd7-5d88ddd60ba2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "create_buildingnet_split.py  create_snet_text_split.py\tlaunchers\n",
            "create_sdf.py\t\t     isosurface\t\t\tprocess_one_mesh.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gp5H9XE0CFQt",
        "outputId": "d489b31e-0035-4085-97d4-169bd5e5f1ca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "create_buildingnet_split.py  create_snet_text_split.py\tlaunchers\n",
            "create_sdf.py\t\t     isosurface\t\t\tprocess_one_mesh.py\n",
            "sh: 1: source: not found\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/SDFusion/preprocess/create_sdf.py\", line 914, in <module>\n",
            "    create_sdf_building(sdf_cmd, mcube_cmd, \"source %s\" % lib_cmd,\n",
            "  File \"/content/SDFusion/preprocess/create_sdf.py\", line 643, in create_sdf_building\n",
            "    list_obj = [os.path.join(model_dir, f) for f in os.listdir(model_dir) if '.obj' in f]\n",
            "                                                    ^^^^^^^^^^^^^^^^^^^^^\n",
            "FileNotFoundError: [Errno 2] No such file or directory: '../data/ShapeNet/ShapeNetCore.v1/OBJ_MODELS'\n",
            "/content/SDFusion\n",
            "configs\t\t    demo_img2shape.ipynb\t  launchers   README.md\n",
            "data\t\t    demo_mm2shape.ipynb\t\t  LICENSE     saved_ckpt\n",
            "dataset_info_files  demo_txt2shape.ipynb\t  models      setup_env.sh\n",
            "datasets\t    demo_uncond_shape_comp.ipynb  options     train.py\n",
            "demo_data\t    external\t\t\t  preprocess  utils\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd preprocess\n",
        "# Delete a specific Python file\n",
        "!rm -f create_sdf.py"
      ],
      "metadata": {
        "id": "nDoXQ8ixbiXp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e28538b9-c8dc-45d8-b20f-9ad9118ec646"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/SDFusion/preprocess\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### create_sdf.py"
      ],
      "metadata": {
        "id": "HnK8w-_dcAAd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile create_sdf.py\n",
        "\n",
        "# modify from https://github.com/Xharlie/DISN/blob/master/preprocessing/create_point_sdf_grid.py\n",
        "\n",
        "from random import choices\n",
        "import h5py\n",
        "import os\n",
        "import argparse\n",
        "import numpy as np\n",
        "import glob\n",
        "\n",
        "from joblib import Parallel, delayed\n",
        "import trimesh\n",
        "from scipy.interpolate import RegularGridInterpolator\n",
        "import time\n",
        "import json\n",
        "\n",
        "CUR_PATH = os.path.dirname(os.path.realpath(__file__))\n",
        "\n",
        "parser = argparse.ArgumentParser()\n",
        "parser.add_argument('--dset', type=str, choices=['shapenet', 'abc', 'pix3d', 'building'], default='shapenet', help='which dataset to extract sdf')\n",
        "parser.add_argument('--thread_num', type=int, default='9', help='how many objs are creating at the same time')\n",
        "parser.add_argument('--reduce', type=int, default=4, help='define resolution. res=256//reduce')\n",
        "parser.add_argument('--category', type=str, default=\"all\", help='Which single class to generate on [default: all, can '\n",
        "                                                                'be chair or plane, etc.]')\n",
        "FLAGS = parser.parse_args()\n",
        "\n",
        "def get_sdf_value(sdf_pt, sdf_params_ph, sdf_ph, sdf_res):\n",
        "    num_point = sdf_pt.shape[0]\n",
        "    # x = np.linspace(sdf_params_ph[0], sdf_params_ph[3], num=sdf_res+1)\n",
        "    # y = np.linspace(sdf_params_ph[1], sdf_params_ph[4], num=sdf_res+1)\n",
        "    # z = np.linspace(sdf_params_ph[2], sdf_params_ph[5], num=sdf_res+1)\n",
        "    x = np.linspace(sdf_params_ph[0], sdf_params_ph[3], num=sdf_res)\n",
        "    y = np.linspace(sdf_params_ph[1], sdf_params_ph[4], num=sdf_res)\n",
        "    z = np.linspace(sdf_params_ph[2], sdf_params_ph[5], num=sdf_res)\n",
        "    my_interpolating_function = RegularGridInterpolator((z, y, x), sdf_ph)\n",
        "    sdf_value = my_interpolating_function(sdf_pt)\n",
        "    print(\"sdf_value:\", sdf_value.shape)\n",
        "    return np.expand_dims(sdf_value, axis=1)\n",
        "\n",
        "def get_sdf(sdf_file, sdf_res):\n",
        "    intsize = 4\n",
        "    floatsize = 8\n",
        "    sdf = {\n",
        "        \"param\": [],\n",
        "        \"value\": []\n",
        "    }\n",
        "    with open(sdf_file, \"rb\") as f:\n",
        "        try:\n",
        "            bytes = f.read()\n",
        "            ress = np.fromstring(bytes[:intsize * 3], dtype=np.int32)\n",
        "            if -1 * ress[0] != sdf_res or ress[1] != sdf_res or ress[2] != sdf_res:\n",
        "                raise Exception(sdf_file, \"res not consistent with \", str(sdf_res))\n",
        "            positions = np.fromstring(bytes[intsize * 3:intsize * 3 + floatsize * 6], dtype=np.float64)\n",
        "            # bottom left corner, x,y,z and top right corner, x, y, z\n",
        "            sdf[\"param\"] = [positions[0], positions[1], positions[2],\n",
        "                            positions[3], positions[4], positions[5]]\n",
        "            sdf[\"param\"] = np.float32(sdf[\"param\"])\n",
        "            sdf[\"value\"] = np.fromstring(bytes[intsize * 3 + floatsize * 6:], dtype=np.float32)\n",
        "            sdf[\"value\"] = np.reshape(sdf[\"value\"], (sdf_res + 1, sdf_res + 1, sdf_res + 1)) # somehow the cube is sdf_res+1 rather than sdf_res... need to investigate why\n",
        "        finally:\n",
        "            f.close()\n",
        "    return sdf\n",
        "\n",
        "def get_offset_ball(num, bandwidth):\n",
        "    u = np.random.normal(0, 1, size=(num,1))\n",
        "    v = np.random.normal(0, 1, size=(num,1))\n",
        "    w = np.random.normal(0, 1, size=(num,1))\n",
        "    r = np.random.uniform(0, 1, size=(num,1)) ** (1. / 3) * bandwidth\n",
        "    norm = np.linalg.norm(np.concatenate([u, v, w], axis=1),axis=1, keepdims=1)\n",
        "    # print(\"u.shape\",u.shape)\n",
        "    # print(\"norm.shape\",norm.shape)\n",
        "    # print(\"r.shape\",r.shape)\n",
        "    (x, y, z) = r * (u, v, w) / norm\n",
        "    return np.concatenate([x,y,z],axis=1)\n",
        "\n",
        "def get_offset_cube(num, bandwidth):\n",
        "    u = np.random.normal(0, 1, size=(num,1))\n",
        "    v = np.random.normal(0, 1, size=(num,1))\n",
        "    w = np.random.normal(0, 1, size=(num,1))\n",
        "    r = np.random.uniform(0, 1, size=(num,1)) ** (1. / 3) * bandwidth\n",
        "    norm = np.linalg.norm(np.concatenate([u, v, w], axis=1),axis=1, keepdims=1)\n",
        "    # print(\"u.shape\",u.shape)\n",
        "    # print(\"norm.shape\",norm.shape)\n",
        "    # print(\"r.shape\",r.shape)\n",
        "    (x, y, z) = r * (u, v, w) / norm\n",
        "    return np.concatenate([x,y,z],axis=1)\n",
        "\n",
        "def sample_sdf(num_sample, bandwidth, iso_val, sdf_dict, sdf_res, reduce):\n",
        "    start = time.time()\n",
        "    params = sdf_dict[\"param\"]\n",
        "    sdf_values = sdf_dict[\"value\"].flatten()\n",
        "    # print(\"np.min(sdf_values), np.mean(sdf_values), np.max(sdf_values)\",\n",
        "    #       np.min(sdf_values), np.mean(sdf_values), np.max(sdf_values))\n",
        "\n",
        "    # n_sample = sdf_res // reduce + 1\n",
        "    n_sample = sdf_res // reduce # want 64 * 64 * 64\n",
        "\n",
        "    x = np.linspace(params[0], params[3], num=n_sample).astype(np.float32)\n",
        "    y = np.linspace(params[1], params[4], num=n_sample).astype(np.float32)\n",
        "    z = np.linspace(params[2], params[5], num=n_sample).astype(np.float32)\n",
        "    z_vals, y_vals, x_vals = np.meshgrid(z, y, x, indexing='ij')\n",
        "    print(\"x_vals\", x_vals[0, 0, sdf_res // reduce - 1])\n",
        "    # x_original = np.linspace(params[0], params[3], num=sdf_res + 1).astype(np.float32)\n",
        "    # y_original = np.linspace(params[1], params[4], num=sdf_res + 1).astype(np.float32)\n",
        "    # z_original = np.linspace(params[2], params[5], num=sdf_res + 1).astype(np.float32)\n",
        "    x_original = np.linspace(params[0], params[3], num=sdf_res+1).astype(np.float32)\n",
        "    y_original = np.linspace(params[1], params[4], num=sdf_res+1).astype(np.float32)\n",
        "    z_original = np.linspace(params[2], params[5], num=sdf_res+1).astype(np.float32)\n",
        "    x_ind = np.arange(n_sample).astype(np.int32)\n",
        "    y_ind = np.arange(n_sample).astype(np.int32)\n",
        "    z_ind = np.arange(n_sample).astype(np.int32)\n",
        "    zv, yv, xv = np.meshgrid(z_ind, y_ind, x_ind, indexing='ij')\n",
        "    choosen_ind = xv * reduce + yv * (sdf_res+1) * reduce + zv * (sdf_res+1)**2 * reduce\n",
        "    choosen_ind = np.asarray(choosen_ind, dtype=np.int32).reshape(-1)\n",
        "    vals = sdf_values[choosen_ind]\n",
        "    x_vals = x[xv.reshape(-1)]\n",
        "    y_vals = y[yv.reshape(-1)]\n",
        "    z_vals = z[zv.reshape(-1)]\n",
        "\n",
        "    # pdb.set_trace()\n",
        "    # sdf_pt_val = np.stack((x_vals, y_vals, z_vals, vals), axis = -1)\n",
        "    sdf_pt_val = np.expand_dims(vals, axis= -1 )\n",
        "    # print(\"np.min(vals), np.mean(vals), np.max(vals)\", np.min(vals), np.mean(vals), np.max(vals))\n",
        "    print(\"sdf_pt_val.shape\", sdf_pt_val.shape)\n",
        "    print(\"sample_sdf: {} s\".format(time.time()-start))\n",
        "    return sdf_pt_val, check_insideout(sdf_values, sdf_res, x_original,y_original,z_original)\n",
        "\n",
        "def check_insideout(sdf_val, sdf_res, x, y, z):\n",
        "    # \"chair\": \"03001627\",\n",
        "    # \"bench\": \"02828884\",\n",
        "    # \"cabinet\": \"02933112\",\n",
        "    # \"car\": \"02958343\",\n",
        "    # \"airplane\": \"02691156\",\n",
        "    # \"display\": \"03211117\",\n",
        "    # \"lamp\": \"03636649\",\n",
        "    # \"speaker\": \"03691459\",\n",
        "    # \"rifle\": \"04090263\",\n",
        "    # \"sofa\": \"04256520\",\n",
        "    # \"table\": \"04379243\",\n",
        "    # \"phone\": \"04401088\",\n",
        "    # \"watercraft\": \"04530566\"\n",
        "\n",
        "    # if cat_id in [\"02958343\", \"02691156\", \"04530566\"]:\n",
        "    x_ind = np.argmin(np.absolute(x))\n",
        "    y_ind = np.argmin(np.absolute(y))\n",
        "    z_ind = np.argmin(np.absolute(z))\n",
        "    all_val = sdf_val.flatten()\n",
        "    # num_val = all_val[x_ind+y_ind*(sdf_res+1)+z_ind*(sdf_res+1)**2]\n",
        "    num_val = all_val[x_ind+y_ind*(sdf_res)+z_ind*(sdf_res)**2]\n",
        "    return num_val > 0.0\n",
        "    # else:\n",
        "        # return False\n",
        "\n",
        "def create_h5_sdf_pt(h5_file, sdf_file, flag_file, norm_obj_file,\n",
        "         centroid, m, sdf_res, num_sample, bandwidth, iso_val, max_verts, normalize, reduce=8):\n",
        "    sdf_dict = get_sdf(sdf_file, sdf_res)\n",
        "    ori_verts = np.asarray([0.0,0.0,0.0], dtype=np.float32).reshape((1,3))\n",
        "    # Nx3(x,y,z)\n",
        "    print(\"ori_verts\", ori_verts.shape)\n",
        "    samplesdf, is_insideout = sample_sdf(num_sample, bandwidth, iso_val, sdf_dict, sdf_res, reduce)  # (N*8)x4 (x,y,z)\n",
        "    if is_insideout:\n",
        "        with open(flag_file, \"w\") as f:\n",
        "            f.write(\"mid point sdf val > 0\")\n",
        "        print(\"insideout !!:\", sdf_file)\n",
        "    else:\n",
        "        os.remove(flag_file) if os.path.exists(flag_file) else None\n",
        "    print(\"samplesdf\", samplesdf.shape)\n",
        "    print(\"start to write\",h5_file)\n",
        "    norm_params = np.concatenate((centroid, np.asarray([m]).astype(np.float32)))\n",
        "    f1 = h5py.File(h5_file, 'w')\n",
        "    f1.create_dataset('pc_sdf_original', data=ori_verts.astype(np.float32), compression='gzip', compression_opts=4)\n",
        "    f1.create_dataset('pc_sdf_sample', data=samplesdf.astype(np.float32), compression='gzip', compression_opts=4)\n",
        "    f1.create_dataset('norm_params', data=norm_params, compression='gzip', compression_opts=4)\n",
        "    f1.create_dataset('sdf_params', data=sdf_dict[\"param\"], compression='gzip', compression_opts=4)\n",
        "    f1.close()\n",
        "    print(\"end writing\",h5_file)\n",
        "    command_str = \"rm -rf \" + norm_obj_file\n",
        "    print(\"command:\", command_str)\n",
        "    os.system(command_str)\n",
        "    command_str = \"rm -rf \" + sdf_file\n",
        "    print(\"command:\", command_str)\n",
        "    os.system(command_str)\n",
        "\n",
        "def get_param_from_h5(sdf_h5_file, cat_id, obj):\n",
        "    h5_f = h5py.File(sdf_h5_file, 'r')\n",
        "    try:\n",
        "        if 'norm_params' in h5_f.keys():\n",
        "            norm_params = h5_f['norm_params'][:]\n",
        "        else:\n",
        "            raise Exception(cat_id, obj, \"no sdf and sample\")\n",
        "    finally:\n",
        "        h5_f.close()\n",
        "    return norm_params[:3], norm_params[3]\n",
        "\n",
        "\n",
        "def as_mesh(scene_or_mesh):\n",
        "    \"\"\"\n",
        "    Convert a possible scene to a mesh.\n",
        "\n",
        "    If conversion occurs, the returned mesh has only vertex and face data.\n",
        "    \"\"\"\n",
        "    if isinstance(scene_or_mesh, trimesh.Scene):\n",
        "        if len(scene_or_mesh.geometry) == 0:\n",
        "            mesh = None  # empty scene\n",
        "        else:\n",
        "            # we lose texture information here\n",
        "            mesh = trimesh.util.concatenate(\n",
        "                tuple(trimesh.Trimesh(vertices=g.vertices, faces=g.faces)\n",
        "                    for g in scene_or_mesh.geometry.values()))\n",
        "    else:\n",
        "        assert(isinstance(scene_or_mesh, trimesh.Trimesh))\n",
        "        mesh = trimesh.Trimesh(vertices=scene_or_mesh.vertices, faces=scene_or_mesh.faces)\n",
        "    return mesh\n",
        "\n",
        "\n",
        "# def get_normalize_mesh(model_file, norm_sdf_file, cat_id, obj, sdf_sub_dir):\n",
        "\n",
        "#     print(\"load mesh from \", model_file)\n",
        "#     mesh_list = trimesh.load_mesh(model_file, process=False)\n",
        "#     #if not isinstance(mesh_list, list):\n",
        "#     #    mesh_list = [mesh_list]\n",
        "#     #largest_ind = 0\n",
        "#     #largest_sur = 0\n",
        "#     #for idx, mesh in enumerate(mesh_list):\n",
        "#     #    area = np.sum(mesh.area_faces)\n",
        "#     #    if largest_sur < area:\n",
        "#     #        largest_ind = idx\n",
        "#     #        largest_sur = area\n",
        "#     #mesh = mesh_list[largest_ind]\n",
        "#     mesh = as_mesh(mesh_list)\n",
        "#     centroid, m = get_param_from_h5(norm_sdf_file, cat_id, obj)\n",
        "#     mesh.vertices = (mesh.vertices - centroid) / float(m)\n",
        "#     obj_file = os.path.join(sdf_sub_dir,\"pc_norm.obj\")\n",
        "#     print(\"exporting\", obj_file)\n",
        "#     trimesh.exchange.export.export_mesh(mesh, obj_file, file_type=\"obj\")\n",
        "#     print(\"export_mesh\", obj_file)\n",
        "#     return obj_file, centroid, m\n",
        "\n",
        "# from DISN create_point_sdf_grid\n",
        "def get_normalize_mesh(model_file, norm_mesh_sub_dir):\n",
        "    total = 16384\n",
        "    print(\"trimesh_load:\", model_file)\n",
        "    mesh_list = trimesh.load_mesh(model_file, process=False)\n",
        "    print(\"[*] done!\", model_file)\n",
        "\n",
        "    # NOTE: used to load with pymesh!\n",
        "    #       change to trimesh\n",
        "    # pymesh.load_mesh(model_file)\n",
        "\n",
        "    mesh = as_mesh(mesh_list) # from s2s\n",
        "    if not isinstance(mesh, list):\n",
        "        mesh_list = [mesh]\n",
        "\n",
        "    area_sum = 0\n",
        "    area_lst = []\n",
        "    for idx, mesh in enumerate(mesh_list):\n",
        "        area = np.sum(mesh.area_faces)\n",
        "        area_lst.append(area)\n",
        "        area_sum+=area\n",
        "    area_lst = np.asarray(area_lst)\n",
        "    amount_lst = (area_lst * total / area_sum).astype(np.int32)\n",
        "    points_all=np.zeros((0,3), dtype=np.float32)\n",
        "    for i in range(amount_lst.shape[0]):\n",
        "        mesh = mesh_list[i]\n",
        "        print(\"start sample surface of \", mesh.faces.shape[0])\n",
        "        points, index = trimesh.sample.sample_surface(mesh, amount_lst[i])\n",
        "        print(\"end sample surface\")\n",
        "        points_all = np.concatenate([points_all,points], axis=0)\n",
        "    centroid = np.mean(points_all, axis=0)\n",
        "    points_all = points_all - centroid\n",
        "    m = np.max(np.sqrt(np.sum(points_all ** 2, axis=1)))\n",
        "    if '/pix3d/' in model_file:\n",
        "        model_basename = os.path.basename(model_file)\n",
        "        pc_norm_name = model_basename.replace('model', 'pc_norm')\n",
        "        obj_file = os.path.join(norm_mesh_sub_dir, pc_norm_name)\n",
        "    else:\n",
        "        obj_file = os.path.join(norm_mesh_sub_dir, \"pc_norm.obj\")\n",
        "\n",
        "    # NOTE: used to load with pymesh!\n",
        "    #       change to trimesh\n",
        "    # ori_mesh = pymesh.load_mesh(model_file)\n",
        "    # print(\"centroid, m\", centroid, m)\n",
        "    # try:\n",
        "        # pymesh.save_mesh_raw(obj_file, (ori_mesh.vertices - centroid) / float(m), ori_mesh.faces)\n",
        "    # except:\n",
        "        # import pdb; pdb.set_trace()\n",
        "\n",
        "    ori_mesh_list = trimesh.load_mesh(model_file, process=False)\n",
        "    ori_mesh = as_mesh(ori_mesh_list)\n",
        "    ori_mesh.vertices = (ori_mesh.vertices - centroid) / float(m)\n",
        "    ori_mesh.export(obj_file)\n",
        "\n",
        "    # print(\"export_mesh\", obj_file)\n",
        "    # print('EXIST?????', os.path.exists(obj_file), obj_file)\n",
        "    return obj_file, centroid, m\n",
        "\n",
        "\n",
        "def create_one_sdf(sdfcommand, res, expand_rate, sdf_file, obj_file, indx, g=0.0):\n",
        "\n",
        "    command_str = sdfcommand + \" \" + obj_file + \" \" + str(res) + \" \" + str(res) + \\\n",
        "       \" \" + str(res) + \" -s \" + \" -e \" + str(expand_rate) + \" -o \" + str(indx) + \".dist -m 1\"\n",
        "    command_str += ' -c'\n",
        "    if g > 0.0:\n",
        "        command_str += \" -g \" + str(g)\n",
        "    print(\"command:\", command_str)\n",
        "    os.system(command_str)\n",
        "    command_str2 = \"mv \" + str(indx)+\".dist \" + sdf_file\n",
        "    print(\"command:\", command_str2)\n",
        "    os.system(command_str2)\n",
        "\n",
        "\n",
        "# s2s\n",
        "# def create_sdf_obj(sdfcommand, marching_cube_command, cat_mesh_dir, cat_norm_mesh_dir, cat_norm_sdf_dir, cat_sdf_dir, obj,\n",
        "#        res, iso_val, expand_rate, indx, ish5, normalize, num_sample, bandwidth, max_verts, cat_id, g, reduce):\n",
        "\n",
        "def create_sdf_obj(sdfcommand, marching_cube_command, norm_mesh_dir, sdf_dir, obj,\n",
        "       res, iso_val, expand_rate, indx, ish5, normalize, num_sample, bandwidth, max_verts, g, reduce):\n",
        "    # obj = obj.rstrip('\\r\\n')\n",
        "    # model_file = obj\n",
        "\n",
        "    # norm_mesh_sub_dir = os.path.join(norm_mesh_dir, os.path.basename(obj).replace('.obj', ''))\n",
        "    # sdf_sub_dir = os.path.join(sdf_dir, os.path.basename(obj).replace('.obj', ''))\n",
        "\n",
        "    if FLAGS.dset == 'abc':\n",
        "        model_id = os.path.basename(obj).replace('.obj', '')\n",
        "    elif FLAGS.dset == 'pix3d':\n",
        "        model_id = obj.split('/')[-2]\n",
        "    elif FLAGS.dset == 'building':\n",
        "        model_id = os.path.basename(obj).replace('.obj', '')\n",
        "    elif FLAGS.dset == 'shapenet':\n",
        "        model_id = obj.split('/')[-2]\n",
        "    norm_mesh_sub_dir = os.path.join(norm_mesh_dir, model_id)\n",
        "    sdf_sub_dir = os.path.join(sdf_dir, model_id)\n",
        "\n",
        "    if not os.path.exists(norm_mesh_sub_dir): os.makedirs(norm_mesh_sub_dir)\n",
        "    if not os.path.exists(sdf_sub_dir): os.makedirs(sdf_sub_dir)\n",
        "\n",
        "    if FLAGS.dset == 'pix3d':\n",
        "        obj_basename = os.path.basename(obj).replace('.obj', '')\n",
        "        sdf_name = obj_basename.replace('model', 'isosurf')\n",
        "        flag_name = obj_basename.replace('model', 'isinsideout')\n",
        "        h5_name = obj_basename.replace('model', 'ori_sample_grid')\n",
        "\n",
        "        sdf_file = os.path.join(sdf_sub_dir, f\"{sdf_name}.sdf\")\n",
        "        flag_file = os.path.join(sdf_sub_dir, f\"{flag_name}.txt\")\n",
        "        h5_file = os.path.join(sdf_sub_dir, f\"{h5_name}.h5\")\n",
        "    else:\n",
        "        sdf_file = os.path.join(sdf_sub_dir, \"isosurf.sdf\")\n",
        "        flag_file = os.path.join(sdf_sub_dir, \"isinsideout.txt\")\n",
        "        h5_file = os.path.join(sdf_sub_dir, \"ori_sample_grid.h5\")\n",
        "\n",
        "    if ish5 and os.path.exists(h5_file) and not os.path.exists(flag_file):\n",
        "        print(\"skip existed: \", h5_file)\n",
        "    elif not ish5 and os.path.exists(sdf_file):\n",
        "        print(\"skip existed: \", sdf_file)\n",
        "    else:\n",
        "        # model_file = os.path.join(cat_mesh_dir, obj, \"models\", \"model_normalized.obj\")\n",
        "        # model_file = os.path.join(cat_mesh_dir, obj, \"model.obj\")\n",
        "        model_file = os.path.join(obj)\n",
        "        print(\"creating\", sdf_file)\n",
        "        if normalize:\n",
        "            norm_obj_file, centroid, m = get_normalize_mesh(model_file, norm_mesh_sub_dir)\n",
        "\n",
        "        create_one_sdf(sdfcommand, res, expand_rate, sdf_file, norm_obj_file, indx, g=g)\n",
        "        # create_one_cube_obj(marching_cube_command, iso_val, sdf_file, cube_obj_file)\n",
        "        # change to h5\n",
        "        if ish5:\n",
        "            create_h5_sdf_pt(h5_file, sdf_file, flag_file, norm_obj_file,\n",
        "                 centroid, m, res, num_sample, bandwidth, iso_val, max_verts, normalize, reduce=reduce)\n",
        "        # except:\n",
        "        #     print(\"%%%%%%%%%%%%%%%%%%%%%%%% fail to process \", model_file)\n",
        "\n",
        "def create_one_cube_obj(marching_cube_command, i, sdf_file, cube_obj_file):\n",
        "    command_str = marching_cube_command + \" \" + sdf_file + \" \" + cube_obj_file + \" -i \" + str(i)\n",
        "    print(\"command:\", command_str)\n",
        "    os.system(command_str)\n",
        "    return cube_obj_file\n",
        "\n",
        "def create_sdf_abc(sdfcommand, marching_cube_command, LIB_command,\n",
        "               num_sample, bandwidth, res, expand_rate, raw_dirs, iso_val,\n",
        "               max_verts, ish5=True, normalize=True, g=0.00, reduce=4):\n",
        "    '''\n",
        "    Usage: SDFGen <filename> <dx> <padding>\n",
        "    Where:\n",
        "        res is number of grids on xyz dimension\n",
        "        w is narrowband width\n",
        "        expand_rate is sdf range of max x,y,z\n",
        "    '''\n",
        "    #cats_init = cats\n",
        "    #cats = cats_init\n",
        "    #cats['airplane'] = cats_init['airplane']\n",
        "    #print(\"command:\", LIB_command)\n",
        "    os.system(LIB_command)\n",
        "    start=0\n",
        "    for split in ['train', 'test']:\n",
        "\n",
        "        model_dir = os.path.join(raw_dirs['mesh_dir'], split, '2048')\n",
        "        norm_mesh_dir = os.path.join(raw_dirs[\"norm_mesh_dir\"], split)\n",
        "        sdf_dir = os.path.join(raw_dirs[\"sdf_dir\"], split)\n",
        "\n",
        "        if not os.path.exists(sdf_dir): os.makedirs(sdf_dir)\n",
        "        if not os.path.exists(norm_mesh_dir): os.makedirs(norm_mesh_dir)\n",
        "\n",
        "        # list_obj = os.listdir(model_dir)\n",
        "        list_obj = [os.path.join(model_dir, f) for f in os.listdir(model_dir)]\n",
        "        repeat = len(list_obj)\n",
        "        sdfcommand_lst=[sdfcommand for i in range(repeat)]\n",
        "        marching_cube_command_lst=[marching_cube_command for i in range(repeat)]\n",
        "        norm_mesh_dir_lst=[norm_mesh_dir for i in range(repeat)] # by yc\n",
        "        sdf_dir_lst=[sdf_dir for i in range(repeat)]\n",
        "        res_lst=[res for i in range(repeat)]\n",
        "        iso_val_lst=[iso_val for i in range(repeat)]\n",
        "        expand_rate_lst=[expand_rate for i in range(repeat)]\n",
        "        indx_lst = [i for i in range(start, start+repeat)]\n",
        "        ish5_lst=[ish5 for i in range(repeat)]\n",
        "        normalize_lst=[normalize for i in range(repeat)]\n",
        "        num_sample_lst=[num_sample for i in range(repeat)]\n",
        "        bandwidth_lst=[bandwidth for i in range(repeat)]\n",
        "        max_verts_lst=[max_verts for i in range(repeat)]\n",
        "        g_lst=[g for i in range(repeat)]\n",
        "        reduce_lst=[reduce for i in range(repeat)]\n",
        "\n",
        "        with Parallel(n_jobs=5) as parallel:\n",
        "            parallel(delayed(create_sdf_obj)\n",
        "            (sdfcommand, marching_cube_command, norm_mesh_dir, sdf_dir, obj, res, iso_val, expand_rate, indx, ish5, norm, num_sample, bandwidth, max_verts, g, reduce)\n",
        "            for sdfcommand, marching_cube_command, norm_mesh_dir, sdf_dir, obj, res, iso_val,\n",
        "                expand_rate, indx, ish5, norm, num_sample, bandwidth, max_verts, g, reduce\n",
        "                in zip(sdfcommand_lst,\n",
        "                       marching_cube_command_lst,\n",
        "                       norm_mesh_dir_lst,\n",
        "                       sdf_dir_lst,\n",
        "                       list_obj,\n",
        "                       res_lst,\n",
        "                       iso_val_lst,\n",
        "                       expand_rate_lst,\n",
        "                       indx_lst, ish5_lst, normalize_lst, num_sample_lst,\n",
        "                       bandwidth_lst, max_verts_lst, g_lst, reduce_lst))\n",
        "\n",
        "        # debug\n",
        "        # for (sdfcommand, marching_cube_command, norm_mesh_dir, sdf_dir, obj, res, iso_val,\n",
        "        #     expand_rate, indx, ish5, norm, num_sample, bandwidth, max_verts, g, reduce) in \\\n",
        "        #     zip(sdfcommand_lst, marching_cube_command_lst, norm_mesh_dir_lst, sdf_dir_lst, list_obj,\n",
        "        #         res_lst, iso_val_lst, expand_rate_lst, indx_lst, ish5_lst, normalize_lst, num_sample_lst,\n",
        "        #         bandwidth_lst, max_verts_lst, g_lst, reduce_lst):\n",
        "        #         create_sdf_obj(\n",
        "        #             sdfcommand, marching_cube_command, norm_mesh_dir, sdf_dir, obj, res,\n",
        "        #             iso_val, expand_rate, indx, ish5, norm, num_sample, bandwidth, max_verts, g, reduce)\n",
        "        start+=repeat\n",
        "    print(\"finish all\")\n",
        "\n",
        "\n",
        "def create_sdf_pix3d(sdfcommand, marching_cube_command, LIB_command,\n",
        "               num_sample, bandwidth, res, expand_rate,\n",
        "               lst_dir, all_cats, raw_dirs, iso_val,\n",
        "               max_verts, ish5=True, normalize=True, g=0.00, reduce=4):\n",
        "    '''\n",
        "    Usage: SDFGen <filename> <dx> <padding>\n",
        "    Where:\n",
        "        res is number of grids on xyz dimension\n",
        "        w is narrowband width\n",
        "        expand_rate is sdf range of max x,y,z\n",
        "    '''\n",
        "    #cats_init = cats\n",
        "    #cats = cats_init\n",
        "    #cats['airplane'] = cats_init['airplane']\n",
        "    #print(\"command:\", LIB_command)\n",
        "    os.system(LIB_command)\n",
        "    start=0\n",
        "\n",
        "    \"\"\" load input text. this is for chair. \"\"\"\n",
        "    # input_txt = '../../data/pix3d/input.txt'\n",
        "    # with open(input_txt, 'r') as f:\n",
        "    #     lines = [l.strip('\\n')[3:] for l in f.readlines()] # no ../\n",
        "\n",
        "    # gt_txt = input_txt.replace('input', 'gt')\n",
        "    # with open(gt_txt, 'r') as f:\n",
        "    #     gt_lines = [l.strip('\\n')[3:] for l in f.readlines()] # no ../\n",
        "\n",
        "    lst_dir\n",
        "    dataroot = lst_dir.split('/pix3d/filelists')[0]\n",
        "    with open(f'{dataroot}/pix3d/pix3d.json', 'r') as f:\n",
        "        pix3d_info = json.load(f)\n",
        "    pix3d_root = f'{dataroot}/pix3d'\n",
        "\n",
        "    # map_input_to_info = {}\n",
        "    # for d in pix3d_info:\n",
        "    #     img_name = d['img']\n",
        "    #     img_name = os.path.splitext(img_name)[0]\n",
        "    #     map_input_to_info[img_name] = d\n",
        "\n",
        "\n",
        "    for cat in all_cats:\n",
        "\n",
        "        # if cat != 'chair':\n",
        "            # continue\n",
        "\n",
        "        model_dir = os.path.join(raw_dirs['mesh_dir'], cat)\n",
        "        norm_mesh_dir = os.path.join(raw_dirs[\"norm_mesh_dir\"], cat)\n",
        "        sdf_dir = os.path.join(raw_dirs[\"sdf_dir\"], cat)\n",
        "\n",
        "        if not os.path.exists(norm_mesh_dir): os.makedirs(norm_mesh_dir)\n",
        "        if not os.path.exists(sdf_dir): os.makedirs(sdf_dir)\n",
        "\n",
        "        # list_obj = os.listdir(model_dir)\n",
        "        list_model_id = []\n",
        "        train_lst = f'{lst_dir}/{cat}_train.lst'\n",
        "        test_lst = f'{lst_dir}/{cat}_test.lst'\n",
        "        with open(train_lst, 'r') as f:\n",
        "            list_model_id = f.readlines()\n",
        "        with open(test_lst, 'r') as f:\n",
        "            list_model_id += f.readlines()\n",
        "\n",
        "        # get all obj file\n",
        "        list_obj = []\n",
        "        for model_id in list_model_id:\n",
        "\n",
        "            # again, different case for chair\n",
        "            if cat == 'chair':\n",
        "\n",
        "                # here basically just copy from 'pix3d_align_shapenet'\n",
        "                p = model_id\n",
        "                p = p.rstrip('\\n')\n",
        "                model_id = p.split('/')[-2]\n",
        "\n",
        "                # # find gt voxel file.\n",
        "                # img_name = os.path.basename(p)\n",
        "                # img_name = os.path.splitext(img_name)[0]\n",
        "                # key = f'img/{cat}/{img_name}'\n",
        "                # pix3d_img_name = map_input_to_info[key]['img']\n",
        "\n",
        "                # # test file\n",
        "                # if pix3d_img_name in lines:\n",
        "                #     ix = lines.index(pix3d_img_name)\n",
        "                #     gt_voxel_name = gt_lines[ix]\n",
        "                #     gt_voxel_bn = os.path.basename(gt_voxel_name)\n",
        "                #     obj_bn = gt_voxel_bn.replace('voxel', 'model')\n",
        "                #     obj_bn = obj_bn.replace('.mat', '.obj')\n",
        "\n",
        "                #     obj_f = f'{pix3d_root}/model_align/{cat}/{model_id}/{obj_bn}'\n",
        "\n",
        "                #     if 'IKEA_JULES_1' in obj_f:\n",
        "                #         kkk = obj_f\n",
        "                #         import pdb; pdb.set_trace()\n",
        "\n",
        "                # # train file.\n",
        "                # else:\n",
        "                #     # train file.\n",
        "                #     # take the first one. for some model, there are multiple obj files\n",
        "                #     obj_f = glob.glob(f'{pix3d_root}/model_align/{cat}/{model_id}/*.obj')[0]\n",
        "\n",
        "                obj_files = glob.glob(f'{model_dir}/{model_id}/*.obj')#[0]\n",
        "\n",
        "                # list_obj += obj_files\n",
        "                # if obj_f not in list_obj:\n",
        "                #     list_obj.append(obj_f)\n",
        "            else:\n",
        "                model_id = model_id.rstrip('\\n')\n",
        "                obj_files = glob.glob(f'{model_dir}/{model_id}/*.obj')#[0]\n",
        "                # list_obj.append(obj_f)\n",
        "\n",
        "            for obj_f in obj_files:\n",
        "                if obj_f not in list_obj:\n",
        "                    list_obj.append(obj_f)\n",
        "\n",
        "        # list_obj = [kkk]\n",
        "        # import pdb; pdb.set_trace()\n",
        "\n",
        "        repeat = len(list_obj)\n",
        "        sdfcommand_lst=[sdfcommand for i in range(repeat)]\n",
        "        marching_cube_command_lst=[marching_cube_command for i in range(repeat)]\n",
        "        norm_mesh_dir_lst=[norm_mesh_dir for i in range(repeat)] # by yc\n",
        "        sdf_dir_lst=[sdf_dir for i in range(repeat)]\n",
        "        res_lst=[res for i in range(repeat)]\n",
        "        iso_val_lst=[iso_val for i in range(repeat)]\n",
        "        expand_rate_lst=[expand_rate for i in range(repeat)]\n",
        "        indx_lst = [i for i in range(start, start+repeat)]\n",
        "        ish5_lst=[ish5 for i in range(repeat)]\n",
        "        normalize_lst=[normalize for i in range(repeat)]\n",
        "        num_sample_lst=[num_sample for i in range(repeat)]\n",
        "        bandwidth_lst=[bandwidth for i in range(repeat)]\n",
        "        max_verts_lst=[max_verts for i in range(repeat)]\n",
        "        g_lst=[g for i in range(repeat)]\n",
        "        reduce_lst=[reduce for i in range(repeat)]\n",
        "\n",
        "        # not sure why, n_jobs=2 fails...\n",
        "        with Parallel(n_jobs=5) as parallel:\n",
        "            parallel(delayed(create_sdf_obj)\n",
        "            (sdfcommand, marching_cube_command, norm_mesh_dir, sdf_dir, obj, res, iso_val, expand_rate, indx, ish5, norm, num_sample, bandwidth, max_verts, g, reduce)\n",
        "            for sdfcommand, marching_cube_command, norm_mesh_dir, sdf_dir, obj, res, iso_val,\n",
        "                expand_rate, indx, ish5, norm, num_sample, bandwidth, max_verts, g, reduce\n",
        "                in zip(sdfcommand_lst,\n",
        "                       marching_cube_command_lst,\n",
        "                       norm_mesh_dir_lst,\n",
        "                       sdf_dir_lst,\n",
        "                       list_obj,\n",
        "                       res_lst,\n",
        "                       iso_val_lst,\n",
        "                       expand_rate_lst,\n",
        "                       indx_lst, ish5_lst, normalize_lst, num_sample_lst,\n",
        "                       bandwidth_lst, max_verts_lst, g_lst, reduce_lst))\n",
        "\n",
        "        # debug\n",
        "        # for (sdfcommand, marching_cube_command, norm_mesh_dir, sdf_dir, obj, res, iso_val,\n",
        "        #     expand_rate, indx, ish5, norm, num_sample, bandwidth, max_verts, g, reduce) in \\\n",
        "        #     zip(sdfcommand_lst, marching_cube_command_lst, norm_mesh_dir_lst, sdf_dir_lst, list_obj,\n",
        "        #         res_lst, iso_val_lst, expand_rate_lst, indx_lst, ish5_lst, normalize_lst, num_sample_lst,\n",
        "        #         bandwidth_lst, max_verts_lst, g_lst, reduce_lst):\n",
        "        #         create_sdf_obj(\n",
        "        #             sdfcommand, marching_cube_command, norm_mesh_dir, sdf_dir, obj, res,\n",
        "        #             iso_val, expand_rate, indx, ish5, norm, num_sample, bandwidth, max_verts, g, reduce)\n",
        "\n",
        "        start+=repeat\n",
        "    print(\"finish all\")\n",
        "\n",
        "\n",
        "\n",
        "def create_sdf_building(sdfcommand, marching_cube_command, LIB_command,\n",
        "               num_sample, bandwidth, res, expand_rate, raw_dirs, iso_val,\n",
        "               max_verts, ish5=True, normalize=True, g=0.00, reduce=4):\n",
        "    '''\n",
        "    Usage: SDFGen <filename> <dx> <padding>\n",
        "    Where:\n",
        "        res is number of grids on xyz dimension\n",
        "        w is narrowband width\n",
        "        expand_rate is sdf range of max x,y,z\n",
        "    '''\n",
        "    #cats_init = cats\n",
        "    #cats = cats_init\n",
        "    #cats['airplane'] = cats_init['airplane']\n",
        "    #print(\"command:\", LIB_command)\n",
        "    os.system(LIB_command)\n",
        "    start=0\n",
        "\n",
        "    resolution = int(res // reduce)\n",
        "\n",
        "    model_dir = os.path.join(raw_dirs['mesh_dir'], 'OBJ_MODELS')\n",
        "    norm_mesh_dir = os.path.join(raw_dirs[\"norm_mesh_dir\"])\n",
        "    sdf_dir = os.path.join(raw_dirs[\"sdf_dir\"], f'resolution_{resolution}')\n",
        "\n",
        "    if not os.path.exists(sdf_dir): os.makedirs(sdf_dir)\n",
        "    if not os.path.exists(norm_mesh_dir): os.makedirs(norm_mesh_dir)\n",
        "\n",
        "    # list_obj = os.listdir(model_dir)\n",
        "    list_obj = [os.path.join(model_dir, f) for f in os.listdir(model_dir) if '.obj' in f]\n",
        "\n",
        "    repeat = len(list_obj)\n",
        "    sdfcommand_lst=[sdfcommand for i in range(repeat)]\n",
        "    marching_cube_command_lst=[marching_cube_command for i in range(repeat)]\n",
        "    norm_mesh_dir_lst=[norm_mesh_dir for i in range(repeat)] # by yc\n",
        "    sdf_dir_lst=[sdf_dir for i in range(repeat)]\n",
        "    res_lst=[res for i in range(repeat)]\n",
        "    iso_val_lst=[iso_val for i in range(repeat)]\n",
        "    expand_rate_lst=[expand_rate for i in range(repeat)]\n",
        "    indx_lst = [i for i in range(start, start+repeat)]\n",
        "    ish5_lst=[ish5 for i in range(repeat)]\n",
        "    normalize_lst=[normalize for i in range(repeat)]\n",
        "    num_sample_lst=[num_sample for i in range(repeat)]\n",
        "    bandwidth_lst=[bandwidth for i in range(repeat)]\n",
        "    max_verts_lst=[max_verts for i in range(repeat)]\n",
        "    g_lst=[g for i in range(repeat)]\n",
        "    reduce_lst=[reduce for i in range(repeat)]\n",
        "\n",
        "    with Parallel(n_jobs=5) as parallel:\n",
        "        parallel(delayed(create_sdf_obj)\n",
        "        (sdfcommand, marching_cube_command, norm_mesh_dir, sdf_dir, obj, res, iso_val, expand_rate, indx, ish5, norm, num_sample, bandwidth, max_verts, g, reduce)\n",
        "        for sdfcommand, marching_cube_command, norm_mesh_dir, sdf_dir, obj, res, iso_val,\n",
        "            expand_rate, indx, ish5, norm, num_sample, bandwidth, max_verts, g, reduce\n",
        "            in zip(sdfcommand_lst,\n",
        "                    marching_cube_command_lst,\n",
        "                    norm_mesh_dir_lst,\n",
        "                    sdf_dir_lst,\n",
        "                    list_obj,\n",
        "                    res_lst,\n",
        "                    iso_val_lst,\n",
        "                    expand_rate_lst,\n",
        "                    indx_lst, ish5_lst, normalize_lst, num_sample_lst,\n",
        "                    bandwidth_lst, max_verts_lst, g_lst, reduce_lst))\n",
        "\n",
        "    # debug\n",
        "    # for (sdfcommand, marching_cube_command, norm_mesh_dir, sdf_dir, obj, res, iso_val,\n",
        "    #     expand_rate, indx, ish5, norm, num_sample, bandwidth, max_verts, g, reduce) in \\\n",
        "    #     zip(sdfcommand_lst, marching_cube_command_lst, norm_mesh_dir_lst, sdf_dir_lst, list_obj,\n",
        "    #         res_lst, iso_val_lst, expand_rate_lst, indx_lst, ish5_lst, normalize_lst, num_sample_lst,\n",
        "    #         bandwidth_lst, max_verts_lst, g_lst, reduce_lst):\n",
        "    #         create_sdf_obj(\n",
        "    #             sdfcommand, marching_cube_command, norm_mesh_dir, sdf_dir, obj, res,\n",
        "    #             iso_val, expand_rate, indx, ish5, norm, num_sample, bandwidth, max_verts, g, reduce)\n",
        "    #         import pdb; pdb.set_trace()\n",
        "    start+=repeat\n",
        "    print(\"finish all\")\n",
        "\n",
        "\n",
        "\n",
        "def create_sdf_shapenet(sdfcommand, marching_cube_command, LIB_command,\n",
        "               num_sample, bandwidth, res, expand_rate,\n",
        "               lst_dir, all_cats, raw_dirs, iso_val,\n",
        "               max_verts, ish5=True, normalize=True, g=0.00, reduce=4):\n",
        "    '''\n",
        "    Usage: SDFGen <filename> <dx> <padding>\n",
        "    Where:\n",
        "        res is number of grids on xyz dimension\n",
        "        w is narrowband width\n",
        "        expand_rate is sdf range of max x,y,z\n",
        "    '''\n",
        "    os.system(LIB_command)\n",
        "    start=0\n",
        "\n",
        "    \"\"\" load input text. this is for chair. \"\"\"\n",
        "\n",
        "    print(\"command:\", LIB_command)\n",
        "    # import subprocess\n",
        "    # subprocess.run(LIB_command)\n",
        "    os.system(LIB_command)\n",
        "    # import pdb; pdb.set_trace()\n",
        "\n",
        "    resolution = int(res // reduce)\n",
        "    sdf_root = raw_dirs[\"sdf_dir\"]\n",
        "    sdf_dir = os.path.join(sdf_root, f'resolution_{resolution}') # .../ShapeNet/SDF_v1\n",
        "    if not os.path.exists(sdf_dir): os.makedirs(sdf_dir)\n",
        "\n",
        "    # map_input_to_info = {}\n",
        "    # for d in pix3d_info:\n",
        "    #     img_name = d['img']\n",
        "    #     img_name = os.path.splitext(img_name)[0]\n",
        "    #     map_input_to_info[img_name] = d\n",
        "\n",
        "    # sanity check: all files exists\n",
        "    for catnm in all_cats:\n",
        "\n",
        "        print(f'[*] checking obj files in {catnm} ({cats[catnm]})')\n",
        "\n",
        "        cat_id = cats[catnm]\n",
        "        cat_mesh_dir = os.path.join(raw_dirs[\"mesh_dir\"], cat_id)\n",
        "        with open(lst_dir+\"/\"+str(cat_id)+\"_test.lst\", \"r\") as f:\n",
        "            list_obj = f.readlines()\n",
        "\n",
        "        with open(lst_dir+\"/\"+str(cat_id)+\"_train.lst\", \"r\") as f:\n",
        "            list_obj += f.readlines()\n",
        "\n",
        "        list_obj = [f.rstrip() for f in list_obj]\n",
        "        list_obj = [f'{cat_mesh_dir}/{f}/model.obj' for f in list_obj]\n",
        "\n",
        "        for f in list_obj:\n",
        "            if not os.path.exists(f):\n",
        "                print(f)\n",
        "                import pdb; pdb.set_trace()\n",
        "            assert os.path.exists(f)\n",
        "\n",
        "        print(f'[*] all files exist for {catnm} ({cats[catnm]})!')\n",
        "\n",
        "\n",
        "    for catnm in all_cats:\n",
        "\n",
        "        cat_id = cats[catnm]\n",
        "        cat_sdf_dir = os.path.join(sdf_dir, cat_id)\n",
        "        if not os.path.exists(cat_sdf_dir): os.makedirs(cat_sdf_dir)\n",
        "        cat_mesh_dir = os.path.join(raw_dirs[\"mesh_dir\"], cat_id)\n",
        "        cat_norm_mesh_dir = os.path.join(raw_dirs[\"norm_mesh_dir\"], cat_id)\n",
        "\n",
        "        with open(lst_dir+\"/\"+str(cat_id)+\"_test.lst\", \"r\") as f:\n",
        "            list_obj = f.readlines()\n",
        "        with open(lst_dir+\"/\"+str(cat_id)+\"_train.lst\", \"r\") as f:\n",
        "            list_obj += f.readlines()\n",
        "\n",
        "        list_obj = [f.rstrip() for f in list_obj]\n",
        "        list_obj = [f'{cat_mesh_dir}/{f}/model.obj' for f in list_obj]\n",
        "\n",
        "        # model_dir = os.path.join(raw_dirs['mesh_dir'], cat)\n",
        "        # norm_mesh_dir = os.path.join(raw_dirs[\"norm_mesh_dir\"], cat)\n",
        "        # cat_sdf_dir = os.path.join(raw_dirs[\"sdf_dir\"], cat)\n",
        "        # if not os.path.exists(norm_mesh_dir): os.makedirs(norm_mesh_dir)\n",
        "        # if not os.path.exists(cat_sdf_dir): os.makedirs(cat_sdf_dir)\n",
        "\n",
        "        repeat = len(list_obj)\n",
        "        sdfcommand_lst=[sdfcommand for i in range(repeat)]\n",
        "        marching_cube_command_lst=[marching_cube_command for i in range(repeat)]\n",
        "        # norm_mesh_dir_lst=[norm_mesh_dir for i in range(repeat)] # by yc\n",
        "        # sdf_dir_lst=[sdf_dir for i in range(repeat)]\n",
        "        norm_mesh_dir_lst=[cat_norm_mesh_dir for i in range(repeat)] # by yc\n",
        "        sdf_dir_lst=[cat_sdf_dir for i in range(repeat)] # by yc\n",
        "        res_lst=[res for i in range(repeat)]\n",
        "        iso_val_lst=[iso_val for i in range(repeat)]\n",
        "        expand_rate_lst=[expand_rate for i in range(repeat)]\n",
        "        indx_lst = [i for i in range(start, start+repeat)]\n",
        "        ish5_lst=[ish5 for i in range(repeat)]\n",
        "        normalize_lst=[normalize for i in range(repeat)]\n",
        "        num_sample_lst=[num_sample for i in range(repeat)]\n",
        "        bandwidth_lst=[bandwidth for i in range(repeat)]\n",
        "        max_verts_lst=[max_verts for i in range(repeat)]\n",
        "        g_lst=[g for i in range(repeat)]\n",
        "        reduce_lst=[reduce for i in range(repeat)]\n",
        "\n",
        "        # not sure why, n_jobs=2 fails...\n",
        "        with Parallel(n_jobs=5) as parallel:\n",
        "            parallel(delayed(create_sdf_obj)\n",
        "            (sdfcommand, marching_cube_command, norm_mesh_dir, sdf_dir, obj, res, iso_val, expand_rate, indx, ish5, norm, num_sample, bandwidth, max_verts, g, reduce)\n",
        "            for sdfcommand, marching_cube_command, norm_mesh_dir, sdf_dir, obj, res, iso_val,\n",
        "                expand_rate, indx, ish5, norm, num_sample, bandwidth, max_verts, g, reduce\n",
        "                in zip(sdfcommand_lst,\n",
        "                       marching_cube_command_lst,\n",
        "                       norm_mesh_dir_lst,\n",
        "                       sdf_dir_lst,\n",
        "                       list_obj,\n",
        "                       res_lst,\n",
        "                       iso_val_lst,\n",
        "                       expand_rate_lst,\n",
        "                       indx_lst, ish5_lst, normalize_lst, num_sample_lst,\n",
        "                       bandwidth_lst, max_verts_lst, g_lst, reduce_lst))\n",
        "\n",
        "        # debug\n",
        "        # for (sdfcommand, marching_cube_command, norm_mesh_dir, sdf_dir, obj, res, iso_val,\n",
        "        #     expand_rate, indx, ish5, norm, num_sample, bandwidth, max_verts, g, reduce) in \\\n",
        "        #     zip(sdfcommand_lst, marching_cube_command_lst, norm_mesh_dir_lst, sdf_dir_lst, list_obj,\n",
        "        #         res_lst, iso_val_lst, expand_rate_lst, indx_lst, ish5_lst, normalize_lst, num_sample_lst,\n",
        "        #         bandwidth_lst, max_verts_lst, g_lst, reduce_lst):\n",
        "        #         create_sdf_obj(\n",
        "        #             sdfcommand, marching_cube_command, norm_mesh_dir, sdf_dir, obj, res,\n",
        "        #             iso_val, expand_rate, indx, ish5, norm, num_sample, bandwidth, max_verts, g, reduce)\n",
        "\n",
        "        start+=repeat\n",
        "    print(\"finish all\")\n",
        "\n",
        "\n",
        "# def test_sdf(sdf_h5_file):\n",
        "#     h5_f = h5py.File(sdf_h5_file, 'r')\n",
        "#     red = np.asarray([255.0, 0, 0]).astype(np.float32)\n",
        "#     blue = np.asarray([0, 0, 255.0]).astype(np.float32)\n",
        "#     try:\n",
        "#         if ('pc_sdf_original' in h5_f.keys() and 'pc_sdf_sample' in h5_f.keys()):\n",
        "#             ori_sdf = h5_f['pc_sdf_original'][:]\n",
        "#             sample_sdf = np.reshape(h5_f['pc_sdf_sample'][:], (-1, 4))\n",
        "#             ori_pt, ori_sdf_val = ori_sdf[:, :3], ori_sdf[:, 3]\n",
        "#             sample_pt, sample_sdf_val = sample_sdf[:, :3], sample_sdf[:, 3]\n",
        "#             minval, maxval = np.min(ori_sdf_val), np.max(ori_sdf_val)\n",
        "#             sdf_pt_color = np.zeros([ori_pt.shape[0], 6], dtype=np.float32)\n",
        "#             sdf_pt_color[:, :3] = ori_pt\n",
        "#             for i in range(sdf_pt_color.shape[0]):\n",
        "#                 sdf_pt_color[i, 3:] = red + (blue - red) * (\n",
        "#                             float(ori_sdf_val[i] - minval) / float(maxval - minval))\n",
        "#             np.savetxt(\"./ori.txt\", sdf_pt_color)\n",
        "#\n",
        "#             sample_pt_color = np.zeros([sample_pt.shape[0], 6], dtype=np.float32)\n",
        "#             sample_pt_color[:, :3] = sample_pt\n",
        "#             minval, maxval = np.min(sample_sdf_val), np.max(sample_sdf_val)\n",
        "#             for i in range(sample_pt_color.shape[0]):\n",
        "#                 sample_pt_color[i, 3:] = red + (blue - red) * (\n",
        "#                         float(sample_sdf_val[i] - minval) / float(maxval - minval))\n",
        "#             np.savetxt(\"./sample.txt\", sample_pt_color)\n",
        "#     finally:\n",
        "#         h5_f.close()\n",
        "\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    # nohup python -u create_point_sdf_fullgrid.py &> createfull.log &\n",
        "    # lst_dir, cats, all_cats, raw_dirs = create_file_lst_abc.get_all_info()\n",
        "\n",
        "    dset = FLAGS.dset\n",
        "    cat = FLAGS.category\n",
        "\n",
        "    # lst_dir, cats, all_cats, raw_dirs = get_sdf_file_lst.get_all_info(dset)\n",
        "\n",
        "    #info_file = '../dataset_info_files/info-shapenet.json'\n",
        "    info_file = '../dataset_info_files/info-building.json'\n",
        "    with open(info_file) as json_file:\n",
        "        info_data = json.load(json_file)\n",
        "        lst_dir, cats, all_cats, raw_dirs = info_data[\"lst_dir\"], info_data['cats'], info_data['all_cats'], info_data['raw_dirs_v1']\n",
        "\n",
        "    if dset == 'shapenet':\n",
        "        if cat != 'all':\n",
        "            cats = {cat: cats[cat]}\n",
        "    elif dset != 'abc':\n",
        "        if cat == 'all':\n",
        "            FLAGS.cats = all_cats\n",
        "        else:\n",
        "            FLAGS.cats = [cat]\n",
        "\n",
        "    isosurface_dir = './isosurface'\n",
        "    sdf_cmd = f'{isosurface_dir}/computeDistanceField'\n",
        "    mcube_cmd = f'{isosurface_dir}/computeMarchingCubes'\n",
        "    lib_cmd = f'{isosurface_dir}/LIB_PATH'\n",
        "\n",
        "    # set env variable\n",
        "    os.environ['LD_LIBRARY_PATH'] = f'$LD_LIBRARY_PATH:{isosurface_dir}:./isosurface/tbb/tbb2018_20180822oss/lib/intel64/gcc4.7/'\n",
        "\n",
        "    num_sample = 65 ** 3\n",
        "    bandwidth = 0.1\n",
        "    res = 256\n",
        "    reduce = FLAGS.reduce\n",
        "    expand_rate = 1.3\n",
        "    iso_val = 0.003\n",
        "    max_verts = 16384\n",
        "\n",
        "    #  full set\n",
        "    # create_sdf(sdfcommand,\n",
        "    #            mcube_cmd,\n",
        "    #            \"source %s\" % lib_cmd, 274625, 0.1,\n",
        "    #            256, 1.3, all_cats, cats, raw_dirs,\n",
        "    #            lst_dir, 0.003, 16384, ish5=True, normalize=True, g=0.00, reduce=4)\n",
        "    # create_sdf_obj(sdfcommand,\n",
        "    #            mcube_cmd,\n",
        "    #            \"source %s\" % lib_cmd, num_sample, bandwidth,\n",
        "    #            res, expand_rate, raw_dirs, iso_val, max_verts, ish5=True, normalize=True, g=0.00, reduce=4)\n",
        "\n",
        "    if dset == 'abc':\n",
        "        create_sdf_abc(sdf_cmd, mcube_cmd, \"source %s\" % lib_cmd,\n",
        "                   num_sample, bandwidth, res, expand_rate, raw_dirs, iso_val,\n",
        "                   max_verts, ish5=True, normalize=True, g=0.00, reduce=reduce)\n",
        "    elif dset == 'pix3d':\n",
        "        create_sdf_pix3d(sdf_cmd, mcube_cmd, \"source %s\" % lib_cmd,\n",
        "                   num_sample, bandwidth, res, expand_rate,\n",
        "                   lst_dir, FLAGS.cats, raw_dirs,\n",
        "                   iso_val, max_verts, ish5=True, normalize=True, g=0.00, reduce=reduce)\n",
        "    elif dset == 'building':\n",
        "        create_sdf_building(sdf_cmd, mcube_cmd, \"source %s\" % lib_cmd,\n",
        "                   num_sample, bandwidth, res, expand_rate, raw_dirs, iso_val,\n",
        "                   max_verts, ish5=True, normalize=True, g=0.00, reduce=reduce)\n",
        "    elif dset == 'shapenet':\n",
        "        # cats: synset_name: synset_id\n",
        "        create_sdf_shapenet(sdf_cmd, mcube_cmd, \"source %s\" % lib_cmd,\n",
        "                   num_sample, bandwidth, res, expand_rate,\n",
        "                   lst_dir, cats, raw_dirs,\n",
        "                   iso_val, max_verts, ish5=True, normalize=True, g=0.00, reduce=reduce)\n",
        "\n"
      ],
      "metadata": {
        "id": "bDl786vzap8f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2230d278-e696-45f3-9d59-619c44cf7989"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing create_sdf.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# we need to be in preprocess\n",
        "!ls"
      ],
      "metadata": {
        "id": "8odzQi-McLF4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "45bee24e-1f23-4d7b-efb2-2fec27dbe04a"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "create_buildingnet_split.py  create_snet_text_split.py\tlaunchers\n",
            "create_sdf.py\t\t     isosurface\t\t\tprocess_one_mesh.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!./launchers/launch_create_sdf_building.sh\n",
        "%cd ../\n",
        "!ls"
      ],
      "metadata": {
        "id": "GvVWGG9XcG6a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8cc72710-b44b-411e-9d0d-3d88fa857edd"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/content/SDFusion/preprocess/create_sdf.py\", line 868, in <module>\n",
            "    lst_dir, cats, all_cats, raw_dirs = info_data[\"lst_dir\"], info_data['cats'], info_data['all_cats'], info_data['raw_dirs_v1']\n",
            "                                        ~~~~~~~~~^^^^^^^^^^^\n",
            "KeyError: 'lst_dir'\n",
            "/content/SDFusion\n",
            "configs\t\t    demo_img2shape.ipynb\t  launchers   README.md\n",
            "data\t\t    demo_mm2shape.ipynb\t\t  LICENSE     saved_ckpt\n",
            "dataset_info_files  demo_txt2shape.ipynb\t  models      setup_env.sh\n",
            "datasets\t    demo_uncond_shape_comp.ipynb  options     train.py\n",
            "demo_data\t    external\t\t\t  preprocess  utils\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Train VQVAE\n",
        "# ShapeNet\n",
        "#./launchers/train_vqvae_snet.sh\n",
        "\n",
        "# BuildingNet\n",
        "!./launchers/train_vqvae-bnet.sh\n",
        "\n",
        "#After training, copy the trained VQVAE checkpoint to the ./saved_ckpt folder.\n",
        "#Let's say the name of the checkpoints are\n",
        "#vqvae-snet-all.ckpt or\n",
        "#vqvae-bnet-all.ckpt.\n",
        "#This is necessary for training the Diffusion model. For SDFusion on various tasks, please see 2.~5. below.\n"
      ],
      "metadata": {
        "id": "pc8K4Up-AhlM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "38a5c22f-1273-43c7-a4f5-86bc5b7aaacc"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/bin/bash: line 1: ./launchers/train_vqvae-bnet.sh: No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#@title Train SDFusion on ShapeNet and BuildingNet\n",
        "# ShapeNet\n",
        "#./launchers/train_sdfusion_snet.sh\n",
        "\n",
        "# BuildingNet\n",
        "!./launchers/train_sdfusion_bnet.sh\n"
      ],
      "metadata": {
        "id": "iWYQp2G8BLmJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "527d0af8-4e9b-493d-9310-2c255bd9170c"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/bin/bash: line 1: ./launchers/train_sdfusion_bnet.sh: No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title text2shape\n",
        "#./launchers/train_sdfusion_img2shape.sh\n",
        "#Train SDFusion for text-guided shape generation\n",
        "\n",
        "# text2shape\n",
        "!./launchers/train_sdfusion_txt2shape.sh\n"
      ],
      "metadata": {
        "id": "S-EJ9GX8BPRT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a385e31e-d1c9-434a-c7c7-daa44f162f89"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[0;31mDebugging!\u001b[0m\n",
            "setting dataroot to: data_data01\n",
            "[*] Training is starting on f3a1fc086c80, GPU#: 0, logs_dir: logs_home\n",
            "[*] Training with command: \n",
            "CUDA_VISIBLE_DEVICES=0 python train.py --name DEBUG-2025-05-11T03-22-11-sdfusion-txt2shape-text2shape-all-LR1e-5-clean-code --logs_dir logs_home --gpu_ids 0 --lr 1e-5 --batch_size 2     --model sdfusion-txt2shape --df_cfg configs/sdfusion-txt2shape.yaml     --vq_model vqvae --vq_cfg configs/vqvae_snet.yaml --vq_ckpt saved_ckpt/vqvae-snet-all.pth --vq_dset snet --vq_cat all     --dataset_mode text2shape --cat all --max_dataset_size 6 --trunc_thres 0.2     --display_freq 3 --print_freq 3     --total_iters 1000000 --save_steps_freq 3     --debug 1 --dataroot data_data01\n",
            "2025-05-11 03:22:15.957379: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1746933736.228431   12223 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1746933736.299679   12223 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-05-11 03:22:16.850696: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "python train.py --name DEBUG-2025-05-11T03-22-11-sdfusion-txt2shape-text2shape-all-LR1e-5-clean-code --logs_dir logs_home --gpu_ids 0 --lr 1e-5 --batch_size 2 --model sdfusion-txt2shape --df_cfg configs/sdfusion-txt2shape.yaml --vq_model vqvae --vq_cfg configs/vqvae_snet.yaml --vq_ckpt saved_ckpt/vqvae-snet-all.pth --vq_dset snet --vq_cat all --dataset_mode text2shape --cat all --max_dataset_size 6 --trunc_thres 0.2 --display_freq 3 --print_freq 3 --total_iters 1000000 --save_steps_freq 3 --debug 1 --dataroot data_data01\n",
            "------------ Options -------------\n",
            "backend: gloo\n",
            "batch_size: 2\n",
            "cat: all\n",
            "ckpt: None\n",
            "continue_train: False\n",
            "dataroot: data_data01\n",
            "dataset_mode: text2shape\n",
            "ddim_eta: 0.0\n",
            "ddim_steps: 100\n",
            "debug: 1\n",
            "device: cuda\n",
            "df_cfg: configs/sdfusion-txt2shape.yaml\n",
            "display_freq: 3\n",
            "distributed: False\n",
            "gpu_ids: 0\n",
            "gpu_ids_str: 0\n",
            "isTrain: True\n",
            "lambda_L1: 10.0\n",
            "local_rank: 0\n",
            "logs_dir: logs_home\n",
            "lr: 1e-05\n",
            "lr_decay_iters: 50\n",
            "lr_policy: lambda\n",
            "max_dataset_size: 6\n",
            "model: sdfusion-txt2shape\n",
            "nThreads: 9\n",
            "name: DEBUG-2025-05-11T03-22-11-sdfusion-txt2shape-text2shape-all-LR1e-5-clean-code\n",
            "phase: train\n",
            "print_freq: 3\n",
            "rank: 0\n",
            "ratio: 1.0\n",
            "res: 64\n",
            "save_epoch_freq: 5\n",
            "save_latest_freq: 500\n",
            "save_steps_freq: 3\n",
            "seed: 111\n",
            "serial_batches: False\n",
            "total_iters: 1000000\n",
            "trunc_thres: 0.2\n",
            "uc_scale: 1.0\n",
            "vq_cat: all\n",
            "vq_cfg: configs/vqvae_snet.yaml\n",
            "vq_ckpt: saved_ckpt/vqvae-snet-all.pth\n",
            "vq_dset: snet\n",
            "vq_model: vqvae\n",
            "which_epoch: latest\n",
            "-------------- End ----------------\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/SDFusion/train.py\", line 131, in <module>\n",
            "    train_dl, test_dl, test_dl_for_eval = CreateDataLoader(opt)\n",
            "                                          ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/SDFusion/datasets/dataloader.py\", line 12, in CreateDataLoader\n",
            "    train_dataset, test_dataset = CreateDataset(opt)\n",
            "                                  ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/SDFusion/datasets/base_dataset.py\", line 60, in CreateDataset\n",
            "    train_dataset.initialize(opt, 'train', cat=opt.cat, res=opt.res)\n",
            "  File \"/content/SDFusion/datasets/text2shape_dataset.py\", line 37, in initialize\n",
            "    with open(self.text_csv) as f:\n",
            "         ^^^^^^^^^^^^^^^^^^^\n",
            "FileNotFoundError: [Errno 2] No such file or directory: 'data_data01/ShapeNet/text2shape/captions.tablechair_train.csv'\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.11"
    },
    "vscode": {
      "interpreter": {
        "hash": "c9bb85e944c303a90ba1b7f3901817f7bc3ecb5f736863b2299a6fa67a7b3c89"
      }
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "c77ca4f16aa849fcbd51cb16bdb22cdc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_6fe1dd115b7e4adb832fdc7c4ded7366",
              "IPY_MODEL_d46ca147dc854b0da91404ed124eceba",
              "IPY_MODEL_68a7fcf81a0d4695b714759880ce5238"
            ],
            "layout": "IPY_MODEL_c4e4179930ec415da854864f18628c81"
          }
        },
        "6fe1dd115b7e4adb832fdc7c4ded7366": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e924f10f7ff64140a56973aa67726565",
            "placeholder": "​",
            "style": "IPY_MODEL_c9ff1818e20347fdbea65541bb5e1611",
            "value": "tokenizer_config.json: 100%"
          }
        },
        "d46ca147dc854b0da91404ed124eceba": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_03b67c457158420f91390fb3e6893d29",
            "max": 48,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_25e053d1684b4498853228d2b72a975f",
            "value": 48
          }
        },
        "68a7fcf81a0d4695b714759880ce5238": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9a8bebb433034f47813a97d101f10cc0",
            "placeholder": "​",
            "style": "IPY_MODEL_8e5bee842e1447aeb48f981aeadf894c",
            "value": " 48.0/48.0 [00:00&lt;00:00, 4.54kB/s]"
          }
        },
        "c4e4179930ec415da854864f18628c81": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e924f10f7ff64140a56973aa67726565": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c9ff1818e20347fdbea65541bb5e1611": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "03b67c457158420f91390fb3e6893d29": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "25e053d1684b4498853228d2b72a975f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "9a8bebb433034f47813a97d101f10cc0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8e5bee842e1447aeb48f981aeadf894c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e4520810fe8a44ec8bd91be918751f43": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_1b19a32e33f74fa686a5eb18767b1761",
              "IPY_MODEL_81add2e3a3f24ab2ac84d84c250c9de0",
              "IPY_MODEL_14460c3f95624fb5808d1bb73fb6207c"
            ],
            "layout": "IPY_MODEL_744e1c4f60ff4a579ff566b74eb3ef46"
          }
        },
        "1b19a32e33f74fa686a5eb18767b1761": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b0a0c53350c24e729f74f0fdea9aafeb",
            "placeholder": "​",
            "style": "IPY_MODEL_ed58b87b65874c41a590eebac46fa8f7",
            "value": "vocab.txt: 100%"
          }
        },
        "81add2e3a3f24ab2ac84d84c250c9de0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a98ec91c561c40f9b59ebace49579397",
            "max": 231508,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_cef908833e434399bd8df323bdb137d0",
            "value": 231508
          }
        },
        "14460c3f95624fb5808d1bb73fb6207c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_96f804c959df485cb52f3e461dbb58d2",
            "placeholder": "​",
            "style": "IPY_MODEL_1a5c09bd882b4fce850280ee457fafcd",
            "value": " 232k/232k [00:00&lt;00:00, 2.80MB/s]"
          }
        },
        "744e1c4f60ff4a579ff566b74eb3ef46": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b0a0c53350c24e729f74f0fdea9aafeb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ed58b87b65874c41a590eebac46fa8f7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a98ec91c561c40f9b59ebace49579397": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cef908833e434399bd8df323bdb137d0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "96f804c959df485cb52f3e461dbb58d2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1a5c09bd882b4fce850280ee457fafcd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a0b69cbceda644d8b6a10d5e38de5c0d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_5913ab7923134cad8dc81781b10e6005",
              "IPY_MODEL_4042a235a4774c2f8098d6f032bf533c",
              "IPY_MODEL_4eab10aa9fa6413b871a7bfcd0393059"
            ],
            "layout": "IPY_MODEL_3464c9848be540e582822a99019b3c41"
          }
        },
        "5913ab7923134cad8dc81781b10e6005": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_39f15ccba796449e92126dbb4e93d8be",
            "placeholder": "​",
            "style": "IPY_MODEL_8f2409654fc4442e86de723d059b6a03",
            "value": "tokenizer.json: 100%"
          }
        },
        "4042a235a4774c2f8098d6f032bf533c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f868c4f7b80e4bd58b23901bee9e4f48",
            "max": 466062,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_e50a7d504ea346648c617a4d5779a872",
            "value": 466062
          }
        },
        "4eab10aa9fa6413b871a7bfcd0393059": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a5abf288fcb148acb682fe94620ed839",
            "placeholder": "​",
            "style": "IPY_MODEL_a6e81d376bda4c989da7c54c84764919",
            "value": " 466k/466k [00:00&lt;00:00, 3.38MB/s]"
          }
        },
        "3464c9848be540e582822a99019b3c41": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "39f15ccba796449e92126dbb4e93d8be": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8f2409654fc4442e86de723d059b6a03": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f868c4f7b80e4bd58b23901bee9e4f48": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e50a7d504ea346648c617a4d5779a872": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a5abf288fcb148acb682fe94620ed839": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a6e81d376bda4c989da7c54c84764919": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "efe6ac01a4c04bb9a0c3e68221b23728": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_8cc36c44ea7c4a27b76b8e678f903c7e",
              "IPY_MODEL_f4c84c148975496493c6012cdc8a6d87",
              "IPY_MODEL_c1717fb5d549414fba48009ecf2d7449"
            ],
            "layout": "IPY_MODEL_52328a2b12d34f00a6b8542b0e8da026"
          }
        },
        "8cc36c44ea7c4a27b76b8e678f903c7e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3b29f79c2b154f37b0d34bf36619237a",
            "placeholder": "​",
            "style": "IPY_MODEL_3d9be72e92074c058f8a45db025c6aa4",
            "value": "config.json: 100%"
          }
        },
        "f4c84c148975496493c6012cdc8a6d87": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_94aced33138c4b6b991279e688610002",
            "max": 570,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_39434dd0ae1b4646a1f1709661d42bcf",
            "value": 570
          }
        },
        "c1717fb5d549414fba48009ecf2d7449": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ec9fd65c5a5b44e99e94929256d9df0f",
            "placeholder": "​",
            "style": "IPY_MODEL_9cf99614c73b40dd9453b73515354276",
            "value": " 570/570 [00:00&lt;00:00, 41.6kB/s]"
          }
        },
        "52328a2b12d34f00a6b8542b0e8da026": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3b29f79c2b154f37b0d34bf36619237a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3d9be72e92074c058f8a45db025c6aa4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "94aced33138c4b6b991279e688610002": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "39434dd0ae1b4646a1f1709661d42bcf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "ec9fd65c5a5b44e99e94929256d9df0f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9cf99614c73b40dd9453b73515354276": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}